{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "938f8410",
   "metadata": {},
   "source": [
    "## **IMPORTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93e5ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(f\"# --- VERIFICATION DU REPERTOIRE COURANT : {os.getcwd()}\")\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from preprocessing import text_cleaning\n",
    "from tqdm.notebook import tqdm\n",
    "from preprocessing.text_cleaning import load_tech_terms\n",
    "print(load_tech_terms())\n",
    "from eda.eda_analysis import plot_distribution\n",
    "from collections import Counter\n",
    "from scipy.stats import entropy\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from api.client.embedding_client import get_embedding\n",
    "from api.client.embedding_client import encode_with_sbert_batchwise\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import joblib\n",
    "import ast\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4586150",
   "metadata": {},
   "source": [
    "## **1. RECUPERATION DES DONNEES**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaefdd0",
   "metadata": {},
   "source": [
    "### **1.1. EXTRACTION DES DONNEES VIA STACK EXCHANGE DATA EXPLORER**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31a2f55",
   "metadata": {},
   "source": [
    "### **1.2. CHARGEMENT DES DONNEES DANS UN DATAFRAME**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3d3001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# --- CHARGER LE FICHIER CSV\n",
    "# ---------------------------\n",
    "full_df = pd.read_csv(\"../data/raw/stackoverflow_questions_sede.csv\")\n",
    "# -----------------------------------------------\n",
    "# --- VERIFIER LE NOMBRE DE LIGNES ET UN APERCU\n",
    "# -----------------------------------------------\n",
    "print(f\"Nombre de questions : {len(full_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a73a9a5",
   "metadata": {},
   "source": [
    "## **2. NETTOYAGE DES DONNEES**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7ee747",
   "metadata": {},
   "source": [
    "**Objectif : transformer du texte brut en texte propre et exploitable**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7963c02d",
   "metadata": {},
   "source": [
    "### 2.0. PREALABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec95ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES DE NETTOYAGES\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import preprocessing.text_cleaning as tc\n",
    "importlib.reload(tc)\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# --- ETAPE 0. Préparation des sets de mots à exclure\n",
    "# -----------------------------------------------------\n",
    "# --- OXFORD TERMS\n",
    "oxford_terms = tc.extract_oxford_terms(\n",
    "    path=\"../src/config/oxford3000.txt\",\n",
    "    export_cleaned_path=\"../src/config/oxford_cleaned.txt\",\n",
    "    verbose=False\n",
    ")\n",
    "print(f\"Nombre de mots dans oxford_cleaned.txt : {len(oxford_terms)}\")\n",
    "# --- CUSTOM TERMS\n",
    "stop_terms_custom = tc.load_stop_terms(\"../src/config/stop_terms.txt\")\n",
    "# stop_terms_custom = tc.custom_stop_terms\n",
    "print(f\"Nombre de mots dans stop_terms.txt : {len(stop_terms_custom)}\")\n",
    "# --- VAGUE TERMS\n",
    "vague_path = tc.generate_vague_terms(path=\"../src/config/vague_terms.txt\", verbose=True)\n",
    "vague_terms = tc.load_stop_terms(vague_path)\n",
    "print(f\"Nombre de mots dans vague_terms.txt : {len(vague_terms)}\")\n",
    "# --- COMBINAISON DE CE QU'ON VEUT EXCLURE\n",
    "stopwords_set = tc.combined_stopwords.union(stop_terms_custom).union(oxford_terms).union(vague_terms)\n",
    "print(f\"Nombre de mots exclus : {len(stopwords_set)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd6189f",
   "metadata": {},
   "source": [
    "### **2.1. APPLICATION SUR UN ECHANTILLON DE 100 QUESTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a298ac2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES DE NETTOYAGES\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import preprocessing.text_cleaning as tc\n",
    "importlib.reload(tc)\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# --- ON CONSTRUIT UN ECHANTILLON DE 100 QUESTIONS\n",
    "# --------------------------------------------------\n",
    "sample_df = full_df.sample(100, random_state=42).copy()\n",
    "\n",
    "# ----------------------------------------\n",
    "# --- NETTOYAGE DES CORPUS TITLE ET BODY\n",
    "# ----------------------------------------\n",
    "sample_clean = sample_df.copy()\n",
    "tqdm.pandas()\n",
    "\n",
    "sample_clean[\"clean_title_body\"] = (\n",
    "    sample_df[\"Title\"].fillna(\"\") + \" \" + sample_df[\"Body\"].fillna(\"\")\n",
    ").apply(lambda x: tc.clean_text_spacy_custom_2(x, stopwords_set=stopwords_set))\n",
    "# -------------------------------------------\n",
    "# --- APERCU DU CORPUS NETTOYÉ\n",
    "# -------------------------------------------\n",
    "sample_clean[[\"PostId\", \"clean_title_body\"]].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0472218",
   "metadata": {},
   "source": [
    "### 2.2. APPLICATION SUR 10 K QUESTIONS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29590ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import importlib\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import preprocessing.text_cleaning as tc\n",
    "importlib.reload(tc)\n",
    "\n",
    "brut_df = full_df.sample(10000, random_state=42).copy()\n",
    "\n",
    "\n",
    "# 🔸 PARAMÈTRES\n",
    "chunk_size = 5000\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "output_dir = f\"../data/processed/brut_chunks_{timestamp}\"\n",
    "os.makedirs(output_dir, exist_ok=True)  # 📁 création du dossier si absent\n",
    "\n",
    "# 🔸 CALCUL DU NOMBRE DE CHUNKS\n",
    "n_chunks = len(brut_df) // chunk_size + int(len(brut_df) % chunk_size > 0)\n",
    "\n",
    "# 🔸 TRAITEMENT PAR CHUNK\n",
    "for chunk_index in tqdm(range(n_chunks), desc=\"Nettoyage & sauvegarde des chunks\"):\n",
    "    start = chunk_index * chunk_size\n",
    "    end = start + chunk_size\n",
    "    batch = brut_df.iloc[start:end].copy()\n",
    "\n",
    "    texts = [\n",
    "    f\"{title} {body}\" \n",
    "    for title, body in zip(batch[\"Title\"].fillna(\"\"), batch[\"Body\"].fillna(\"\"))\n",
    "    ]\n",
    "\n",
    "    # texts = (\n",
    "    #     title + \" \" + body\n",
    "    #     for title, body in zip(batch[\"Title\"].fillna(\"\"), batch[\"Body\"].fillna(\"\"))\n",
    "    # )\n",
    "    \n",
    "\n",
    "    docs = tc.nlp.pipe(texts, batch_size=250, disable=[\"parser\", \"ner\"])\n",
    "    batch[\"clean_title_body\"] = [\n",
    "        tc.clean_doc_spacy_custom(doc, stopwords_set=stopwords_set) for doc in docs\n",
    "    ]\n",
    "\n",
    "    # 🔸 SAUVEGARDE DU CHUNK EN .parquet\n",
    "    chunk_path = os.path.join(output_dir, f\"chunk_{chunk_index+1:02}.parquet\")\n",
    "    batch.to_parquet(chunk_path, index=False)\n",
    "    print(f\"💾 Chunk {chunk_index+1}/{n_chunks} sauvegardé → {chunk_path}\")\n",
    "\n",
    "    # 🔸 NETTOYAGE DE MÉMOIRE\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "import glob\n",
    "\n",
    "files = sorted(glob.glob(os.path.join(output_dir, \"chunk_*.parquet\")))\n",
    "print(f\"\"\"Les chunks sont assemblés à partir des fichiers du répertoire horodaté :\n",
    "             {output_dir}\"\"\")\n",
    "brut_df_clean = pd.concat([pd.read_parquet(f) for f in files], ignore_index=True)\n",
    "\n",
    "display(brut_df_clean[\"clean_title_body\"].head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07ba749",
   "metadata": {},
   "source": [
    "## **3. EXPLORATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e852c6",
   "metadata": {},
   "source": [
    "### **3.1. APPLICATION SUR UN ECHANTILLON DE 100 QUESTIONS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445c67fc",
   "metadata": {},
   "source": [
    "#### **3.1.1. FREQUENCE DES MOTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830f2c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "# Copie de travail pour toute l’exploration\n",
    "sample_explo = sample_clean.copy()\n",
    "\n",
    "# -------------------------------\n",
    "# --- ANALYSE FREQUENCE DES MOTS\n",
    "# -------------------------------\n",
    "freq_title_body = eda.compute_word_frequencies(sample_explo, \"clean_title_body\", top_n=100)\n",
    "display(freq_title_body.head())\n",
    "\n",
    "eda.plot_word_frequencies(\n",
    "    freq_title_body,\n",
    "    max_words_display=25,\n",
    "    palette=\"viridis\",\n",
    "    title=\"Top mots dans les questions (titre + corps fusionnés)\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f767383d",
   "metadata": {},
   "source": [
    "#### **3.1.2. NUAGE DE MOTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607ded88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "# -------------------\n",
    "# --- NUAGE DE MOTS\n",
    "# -------------------\n",
    "eda.generate_wordcloud(\n",
    "    freq_title_body,\n",
    "     max_words=100,\n",
    "      colormap=\"mako\",\n",
    "      title=\"Nuage de mots - Corpus clean_title_body des questions\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fcb9bc",
   "metadata": {},
   "source": [
    "#### **3.1.3. ANALYSE DE LA LONGUEUR DES DOCUMENTS ET DE LA FORME DE SA DISTRIBUTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da23e805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "# -------------------------------\n",
    "# --- ANALYSE LONGUEUR DU CORPUS FUSIONNÉ\n",
    "# -------------------------------\n",
    "sample_explo[\"title_body_length_char\"] = sample_explo[\"clean_title_body\"].str.len()\n",
    "sample_explo[\"title_body_length_words\"] = sample_explo[\"clean_title_body\"].str.split().apply(len)\n",
    "\n",
    "# 🧪 Statistiques descriptives\n",
    "display(sample_explo[[\"title_body_length_char\", \"title_body_length_words\"]].describe())\n",
    "\n",
    "# 📊 Visualisation\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "eda.plot_distribution(ax[0], sample_explo[\"title_body_length_char\"], \"Longueur (caractères)\", \"mediumorchid\")\n",
    "eda.plot_distribution(ax[1], sample_explo[\"title_body_length_words\"], \"Longueur (mots utiles)\", \"goldenrod\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b521bf23",
   "metadata": {},
   "source": [
    "#### **3.1.4. DETECTION DES OUTLIERS ET DOUBLONS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa9c2ac",
   "metadata": {},
   "source": [
    "##### ***3.1.4.1. DÉTECTION DES DOUBLONS EXACTS***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88c580a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "# -------------------------------------\n",
    "# --- DÉTECTION DES DOUBLONS EXACTS\n",
    "# -------------------------------------\n",
    "\n",
    "# Nombre total de questions\n",
    "total_questions = sample_explo.shape[0]\n",
    "\n",
    "# 🔍 Masque des doublons exacts sur le corpus fusionné nettoyé\n",
    "duplicate_mask = sample_explo.duplicated(subset=[\"clean_title_body\"], keep=False)\n",
    "duplicates_df = sample_explo[duplicate_mask]\n",
    "\n",
    "# 📊 Statistiques sur les doublons\n",
    "nb_duplicates = duplicates_df.shape[0]\n",
    "nb_unique_duplicates = duplicates_df.duplicated(subset=[\"clean_title_body\"]).sum()\n",
    "\n",
    "# 📢 Affichage des résultats\n",
    "print(f\"Nombre total de questions : {total_questions}\")\n",
    "print(f\"Nombre de doublons exacts sur 'clean_title_body' : {nb_duplicates}\")\n",
    "print(f\"Nombre de doublons à supprimer (copies identiques) : {nb_unique_duplicates}\")\n",
    "print(f\"Proportion de doublons dans l’échantillon : {nb_duplicates / total_questions:.2%}\")\n",
    "\n",
    "# 👀 Aperçu des doublons\n",
    "duplicates_df.sort_values(by=\"clean_title_body\").head(6)[[\"PostId\", \"clean_title_body\"]]\n",
    "\n",
    "# 🧹 Suppression des doublons exacts (on garde la première occurrence)\n",
    "sample_explo = sample_explo.drop_duplicates(subset=[\"clean_title_body\"], keep=\"first\").reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829629de",
   "metadata": {},
   "source": [
    "##### ***3.1.4.2. DÉTECTION DES DOUBLONS FLOUS***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e96260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# --- DÉTECTION DE DOUBLONS FLOUS SUR LE CORPUS NETTOYÉ\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "# Cosine TF-IDF\n",
    "fuzzy_cosine = eda.detect_fuzzy_duplicates(sample_explo[\"clean_title_body\"], threshold=0.85)\n",
    "print(f\"# --- Cosine TF-IDF : {len(fuzzy_cosine)} paires similaires détectées (threshold ≥ 0.85)\")\n",
    "\n",
    "# Levenshtein\n",
    "fuzzy_lev = eda.detect_levenshtein_duplicates(sample_explo[\"clean_title_body\"], threshold=90)\n",
    "print(f\"# --- Levenshtein : {len(fuzzy_lev)} paires similaires détectées (threshold ≥ 90)\")\n",
    "\n",
    "# Jaccard\n",
    "fuzzy_jaccard = eda.detect_jaccard_duplicates(sample_explo[\"clean_title_body\"], threshold=0.7)\n",
    "print(f\"# --- Jaccard : {len(fuzzy_jaccard)} paires similaires détectées (threshold ≥ 0.7)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04acff6d",
   "metadata": {},
   "source": [
    "##### ***3.1.4.4. GESTION DES OUTLIERS***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6075b787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "# 📏 Création des métriques de longueur\n",
    "sample_explo[\"title_body_length_char\"] = sample_explo[\"clean_title_body\"].str.len()\n",
    "sample_explo[\"title_body_length_words\"] = sample_explo[\"clean_title_body\"].str.split().apply(len)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# --- DETECTION DES OUTLIERS PAR LA METHODE DES SEUILS IQR\n",
    "# ---------------------------------------------------------\n",
    "sample_explo, sample_outliers = eda.mark_outliers(\n",
    "    df=sample_explo,\n",
    "    columns=[\"title_body_length_words\"],  # ← colonne ciblée\n",
    "    verbose=True,\n",
    "    return_outliers=True\n",
    ")\n",
    "print(f\"# --- NOMBRE OUTLIERS : {sample_outliers.shape[0]}\")\n",
    "# -------------------------------------------\n",
    "# --- VISUALISATION BOXPLOT DE LA DISPERSION\n",
    "# -------------------------------------------\n",
    "eda.plot_boxplots_grid(\n",
    "    df=sample_explo,\n",
    "    columns=[\"title_body_length_words\"],\n",
    "    titles=[\"Longueur des questions nettoyées (mots)\"],\n",
    "    colors=[\"goldenrod\"]\n",
    ")\n",
    "\n",
    "# 🚿 Suppression des outliers\n",
    "sample_explo_wo = eda.remove_outliers(sample_explo)\n",
    "print(f\"# --- NOMBRE DE QUESTIONS RESTANTES APRES SUPPRESSION DES OUTLIERS : {sample_explo_wo.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b5d3a8",
   "metadata": {},
   "source": [
    "#### **3.1.5. ANALYSE DES TAGS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3631ac96",
   "metadata": {},
   "source": [
    "##### 3.1.5.1. Structure des tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37108ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "# -----------------------------\n",
    "# --- APERCU DE LA COLONNE TAG\n",
    "# -----------------------------\n",
    "print(sample_explo_wo[\"Tags\"].head(10))\n",
    "print(sample_explo_wo[\"Tags\"].apply(type).value_counts())\n",
    "# ------------------------------------------\n",
    "# --- CONVERSION DES CHAINES STRING EN LIST\n",
    "# ------------------------------------------\n",
    "sample_explo_wo[\"Tags\"] = sample_explo_wo[\"Tags\"].apply(lambda x: x.split(\";\"))\n",
    "sample_explo_wo[\"Tags\"].apply(type).value_counts()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7443da",
   "metadata": {},
   "source": [
    "##### 3.1.5.2. Fréquence des tags  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c46801c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "\n",
    "\n",
    "# --- APLATIR TOUTES LES LISTES DE TAGS\n",
    "all_tags = [tag for tags in sample_explo_wo[\"Tags\"] for tag in tags]\n",
    "\n",
    "# --- COMPTER LES OCCURRENCES\n",
    "tag_counts = Counter(all_tags)\n",
    "\n",
    "# --- CONVERSION PANDAS POUR VISUALISER\n",
    "tag_freq_df = pd.DataFrame(tag_counts.items(), columns=[\"Tag\", \"Count\"]).sort_values(by=\"Count\", ascending=False)\n",
    "tag_freq_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# --- LES 10 TAGS LES PLUS FREQUENTS\n",
    "tag_freq_df.head(10)\n",
    "\n",
    "# --- VISUSALISATION DES TACGS LES PLUS FREQUENTS\n",
    "eda.plot_tag_occurrences(tag_freq_df, top_n=10)\n",
    "eda.plot_tag_distribution(tag_freq_df, top_n=10)\n",
    "\n",
    "# --- TAGS QUI SONT RARES\n",
    "rare_tags = tag_freq_df[tag_freq_df[\"Count\"] == 1]\n",
    "print(f\"Nombre de tags apparaissant une seule fois : {len(rare_tags)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897e5848",
   "metadata": {},
   "source": [
    "##### 3.1.5.3. Diversité des tags  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d272e1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "# --- NOMBRE DE TAGS DISTINCTS\n",
    "nb_tags_uniques = tag_freq_df[\"Tag\"].nunique()\n",
    "print(f\"Nombre de tags distincts dans l’échantillon : {nb_tags_uniques}\")\n",
    "\n",
    "# --- CALCUL DE LA PROPORTION CUMULEE\n",
    "tag_freq_df[\"Proportion\"] = tag_freq_df[\"Count\"] / tag_freq_df[\"Count\"].sum()\n",
    "tag_freq_df[\"Cumulative\"] = tag_freq_df[\"Proportion\"].cumsum()\n",
    "\n",
    "# --- VISUALISATION  : COURBE DE COUVERTURE CUMULATIVE\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.lineplot(data=tag_freq_df, x=range(1, len(tag_freq_df)+1), y=\"Cumulative\", marker=\"o\")\n",
    "plt.axhline(0.8, color=\"red\", linestyle=\"--\", label=\"80% de couverture\")\n",
    "plt.title(\"Courbe de couverture cumulative des tags\")\n",
    "plt.xlabel(\"Nombre de tags (classés par fréquence décroissante)\")\n",
    "plt.ylabel(\"Proportion cumulée\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- NOMBRE DE TAGS NECESSAIRE POUR COUVRIR 80% DU CORPUS\n",
    "nb_tags_80 = (tag_freq_df[\"Cumulative\"] <= 0.8).sum()\n",
    "print(f\"Nombre de tags nécessaires pour couvrir 80% des occurrences : {nb_tags_80}\")\n",
    "\n",
    "# ----------------------------------\n",
    "# --- CALCUL DE L'ENTROPIE DES TAGS\n",
    "# ----------------------------------\n",
    "# --- DISTRIBUTION DES PROPORTIONS\n",
    "proportions = tag_freq_df[\"Proportion\"].values\n",
    "\n",
    "# --- ENTROPIE DE SHANNON (BASE 2)\n",
    "tag_entropy = entropy(proportions, base=2)\n",
    "print(f\"Entropie de la distribution des tags : {tag_entropy:.4f} bits\")\n",
    "\n",
    "# --------------------------------\n",
    "# --- ENTROPIE MAXIMALE THEORIQUE\n",
    "# --------------------------------\n",
    "max_entropy = np.log2(len(proportions))\n",
    "print(f\"Entropie maximale possible : {max_entropy:.4f} bits\")\n",
    "print(f\"Taux de diversité relative : {tag_entropy / max_entropy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c493355a",
   "metadata": {},
   "source": [
    "##### 3.1.5.4. Marquage des tags dominants  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39ffadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "# --------------------------------\n",
    "# --- MARQUAGE DES TAGS DOMINANTS\n",
    "# --------------------------------\n",
    "tag_freq_df_marked, dominant_tags = eda.mark_dominant_tags_by_frequency_and_coverage(\n",
    "    tag_freq_df,\n",
    "    nb_questions=100,  # ou 50000 plus tard\n",
    "    min_coverage=0.8,\n",
    "    min_count_floor=3\n",
    ")\n",
    "\n",
    "display(dominant_tags)\n",
    "display(tag_freq_df_marked)\n",
    "\n",
    "eda.plot_tag_coverage(tag_freq_df_marked, coverage_target=0.8, title_suffix=\"échantillon de 100 questions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6980406e",
   "metadata": {},
   "source": [
    "#### **3.1.6. CO-OCCURRENCE DES TAGS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7272556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "cooc_matrix, tag_matrix = eda.build_tag_cooccurrence_matrix(\n",
    "    df_questions=sample_explo_wo,  # ou le nom de ton DataFrame\n",
    "    dominant_tags=dominant_tags,\n",
    "    tag_col=\"Tags\"\n",
    ")\n",
    "\n",
    "cooc_matrix.style.background_gradient(cmap=\"Blues\")\n",
    "\n",
    "eda.plot_tag_cooccurrence_heatmap(cooc_matrix)\n",
    "\n",
    "eda.plot_tag_cooccurrence_graph(cooc_matrix, min_edge_weight=2, layout=\"spring\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2852b0a9",
   "metadata": {},
   "source": [
    "### 3.2. APPLICATION SUR 10 K QUESTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa7945e",
   "metadata": {},
   "source": [
    "#### **3.2.1. FREQUENCE DES MOTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929e8c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "# ✅ Standardisation du nom du corpus complet\n",
    "full_explo = brut_df_clean.copy()\n",
    "\n",
    "\n",
    "# 🔍 Calcul des fréquences (top 100 lemmes sans les stopwords personnalisés)\n",
    "freq_clean_title_body = eda.compute_word_frequencies(\n",
    "    df=full_explo,\n",
    "    column=\"clean_title_body\",\n",
    "    top_n=100  # tu peux augmenter selon besoin\n",
    ")\n",
    "display(freq_clean_title_body)\n",
    "# 👁️ Visualisation des top tokens\n",
    "eda.plot_word_frequencies(\n",
    "    df_freq=freq_clean_title_body,\n",
    "    max_words_display=30,\n",
    "    palette=\"viridis\",\n",
    "    title=\"Top mots dans clean_title_body (10 000 questions)\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41f41e0",
   "metadata": {},
   "source": [
    "#### **3.2.2. NUAGE DE MOTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324f182a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "eda.generate_wordcloud(\n",
    "    df_freq=freq_clean_title_body,\n",
    "    max_words=100,\n",
    "    colormap=\"plasma\",\n",
    "    title=\"Nuage de mots - clean_title_body (Corpus complet)\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceaaa86",
   "metadata": {},
   "source": [
    "#### **3.2.3. ANALYSE DE LA LONGUEUR DES DOCUMENTS ET DE LA FORME DE SA DISTRIBUTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db37e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "display(full_explo.columns)\n",
    "# 🔹 Reconstruction du texte brut fusionné\n",
    "full_explo[\"title_body_raw\"] = full_explo[\"Title\"].fillna(\"\") + \" \" + full_explo[\"Body\"].fillna(\"\")\n",
    "\n",
    "# 🔹 Calcul des longueurs en mots\n",
    "full_explo[\"length_words_raw\"] = full_explo[\"title_body_raw\"].str.split().apply(len)\n",
    "full_explo[\"length_words_clean\"] = full_explo[\"clean_title_body\"].str.split().apply(len)\n",
    "\n",
    "# 🔹 Ratio de réduction lexical\n",
    "full_explo[\"reduction_ratio_global\"] = full_explo[\"length_words_clean\"] / full_explo[\"length_words_raw\"]\n",
    "\n",
    "import eda.eda_analysis as eda\n",
    "\n",
    "eda.plot_distribution(\n",
    "    ax=plt.gca(),\n",
    "    data=full_explo[\"reduction_ratio_global\"],\n",
    "    title=\"Ratio de réduction lexical global (nettoyé vs brut)\",\n",
    "    color=\"mediumseagreen\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac2a5b7",
   "metadata": {},
   "source": [
    "#### **3.2.4. DETECTION DES OUTLIERS ET DOUBLONS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d78d43",
   "metadata": {},
   "source": [
    "##### ***3.2.4.1. DÉTECTION DES DOUBLONS EXACTS***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6585ffca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "# 🔍 Détection des doublons exacts sur le champ nettoyé\n",
    "exact_dup_mask = full_explo.duplicated(subset=[\"clean_title_body\"], keep=False)\n",
    "exact_duplicates_df = full_explo[exact_dup_mask]\n",
    "\n",
    "# 📊 Statistiques\n",
    "nb_duplicates = exact_duplicates_df.shape[0]\n",
    "nb_unique = exact_duplicates_df.duplicated(subset=[\"clean_title_body\"]).sum()\n",
    "total = full_explo.shape[0]\n",
    "\n",
    "print(f\"Nombre total de questions : {total}\")\n",
    "print(f\"Nombre de doublons exacts (clean_title_body) : {nb_duplicates}\")\n",
    "print(f\"Nombre de doublons à supprimer (copies identiques) : {nb_unique}\")\n",
    "print(f\"Proportion de doublons dans le corpus : {nb_duplicates / total:.2%}\")\n",
    "\n",
    "# 💬 Affichage de quelques doublons\n",
    "exact_duplicates_df.sort_values(\"clean_title_body\").head(6)[[\"PostId\", \"clean_title_body\"]]\n",
    "\n",
    "# 🧹 Suppression des doublons exacts (on garde la première occurrence)\n",
    "full_explo = full_explo.drop_duplicates(subset=[\"clean_title_body\"], keep=\"first\").reset_index(drop=True)\n",
    "display(exact_duplicates_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee05caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bc26db",
   "metadata": {},
   "source": [
    "##### ***3.2.4.3. GESTION DES OUTLIERS***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bfd38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "\n",
    "full_explo, full_outliers = eda.mark_outliers(\n",
    "    df=full_explo,\n",
    "    columns=[\n",
    "        \"length_words_clean\",\n",
    "        \"reduction_ratio_global\"\n",
    "    ],\n",
    "    verbose=True,\n",
    "    return_outliers=True\n",
    ")\n",
    "\n",
    "# eda.plot_boxplots_grid(\n",
    "#     df=full_explo,\n",
    "#     columns=[\n",
    "#         \"length_words_clean\",\n",
    "#         \"reduction_ratio_global\"\n",
    "#     ],\n",
    "#     titles=[\n",
    "#         \"Longueur nettoyée - titre + body\",\n",
    "#         \"Ratio de réduction lexical\"\n",
    "#     ],\n",
    "#     colors=[\"steelblue\", \"mediumseagreen\"]\n",
    "# )\n",
    "full_explo_wo = eda.remove_outliers(full_explo)\n",
    "print(f\"# ✅ Questions restantes après suppression des outliers : {full_explo_wo.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ea8b60",
   "metadata": {},
   "source": [
    "#### **3.2.5. ANALYSE DES TAGS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0f5ed2",
   "metadata": {},
   "source": [
    "##### 3.2.5.1. Structure des tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe7bd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "# --- APERCU DE LA COLONNE TAGS\n",
    "print(full_explo_wo[\"Tags\"].head(10))\n",
    "print(full_explo_wo[\"Tags\"].apply(type).value_counts())\n",
    "# --- CONVERSION DES CHAINES EN LISTES\n",
    "full_explo_wo[\"Tags\"] = full_explo_wo[\"Tags\"].apply(lambda x: x.split(\";\") if isinstance(x, str) else [])\n",
    "# --- VERIFICATION POST CONVERSION\n",
    "print(full_explo_wo[\"Tags\"].apply(type).value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03993b6",
   "metadata": {},
   "source": [
    "##### 3.2.5.2. Fréquence des tags  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48d5a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "# --- ON APLATIT LA LISTE DES TAGS\n",
    "from collections import Counter\n",
    "\n",
    "all_tags = [tag for tags in full_explo_wo[\"Tags\"] for tag in tags]\n",
    "\n",
    "# --- ON COMPTE LES OCCURRENCES\n",
    "tag_counts = Counter(all_tags)\n",
    "# --- CONVERSION EN PANDAS DATAFRAME POUR VISUALISATION\n",
    "tag_freq_df = pd.DataFrame(tag_counts.items(), columns=[\"Tag\", \"Count\"]).sort_values(by=\"Count\", ascending=False)\n",
    "tag_freq_df.reset_index(drop=True, inplace=True)\n",
    "# ---AFFICHAGE DES TAGS LES PLUS FREQUENTS\n",
    "tag_freq_df.head(10)\n",
    "# --- VISUALISATION\n",
    "eda.plot_tag_occurrences(tag_freq_df, top_n=10)\n",
    "eda.plot_tag_distribution(tag_freq_df, top_n=10)\n",
    "# --- EXTRACTION DES TAGS RARES\n",
    "rare_tags = tag_freq_df[tag_freq_df[\"Count\"] == 1]\n",
    "print(f\"Nombre de tags apparaissant une seule fois : {len(rare_tags)}\")\n",
    "# --- DISTRIBUTION DES TAGS PAR FREQUENCE\n",
    "tag_freq_df[\"log_count\"] = np.log1p(tag_freq_df[\"Count\"])\n",
    "sns.histplot(tag_freq_df[\"log_count\"], bins=30, color=\"indigo\")\n",
    "plt.title(\"Distribution log-transformée des fréquences de tags\")\n",
    "plt.xlabel(\"log(1 + Count)\")\n",
    "plt.ylabel(\"Nombre de tags\")\n",
    "plt.show()\n",
    "\n",
    "# --- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c9748d",
   "metadata": {},
   "source": [
    "##### 3.2.5.3. Diversité des tags  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba199ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "# --- NOMBRE DE TAGS UNIQUES\n",
    "nb_tags_uniques = tag_freq_df[\"Tag\"].nunique()\n",
    "print(f\"Nombre de tags distincts dans le corpus : {nb_tags_uniques}\")\n",
    "# --- CALCUL DE LA PROPORTION CUMULEE\n",
    "tag_freq_df[\"Proportion\"] = tag_freq_df[\"Count\"] / tag_freq_df[\"Count\"].sum()\n",
    "tag_freq_df[\"Cumulative\"] = tag_freq_df[\"Proportion\"].cumsum()\n",
    "# --- VISUALISATION DE COUVERTURE CUMULATIVE\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.lineplot(\n",
    "    data=tag_freq_df,\n",
    "    x=range(1, len(tag_freq_df) + 1),\n",
    "    y=\"Cumulative\",\n",
    "    marker=\"o\"\n",
    ")\n",
    "plt.axhline(0.8, color=\"red\", linestyle=\"--\", label=\"80% de couverture\")\n",
    "plt.title(\"Courbe de couverture cumulative des tags\")\n",
    "plt.xlabel(\"Nombre de tags (classés par fréquence décroissante)\")\n",
    "plt.ylabel(\"Proportion cumulée\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- NOMBRE DE TAGS NECESSAIRES POUR COUVRIR 80% DES OCCURRENCES\n",
    "nb_tags_80 = (tag_freq_df[\"Cumulative\"] <= 0.8).sum()\n",
    "print(f\"Nombre de tags nécessaires pour couvrir 80 % des occurrences : {nb_tags_80}\")\n",
    "# --- ENTROPIE DE LA DISTRIBUTION DES TAGS (SHANNON BASE 2)\n",
    "from scipy.stats import entropy\n",
    "\n",
    "proportions = tag_freq_df[\"Proportion\"].values\n",
    "tag_entropy = entropy(proportions, base=2)\n",
    "print(f\"Entropie de la distribution des tags : {tag_entropy:.4f} bits\")\n",
    "\n",
    "max_entropy = np.log2(len(proportions))\n",
    "print(f\"Entropie maximale possible : {max_entropy:.4f} bits\")\n",
    "print(f\"Taux de diversité relative : {tag_entropy / max_entropy:.2%}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4632e831",
   "metadata": {},
   "source": [
    "##### 3.2.5.4. Marquage des tags dominants  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1e8afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "\n",
    "# ✅ Marquage des tags dominants sur le corpus complet\n",
    "tag_freq_df_marked, dominant_tags = eda.mark_dominant_tags_by_frequency_and_coverage(\n",
    "    tag_freq_df=tag_freq_df,\n",
    "    nb_questions=full_explo_wo.shape[0],  # 50 000 ou autre taille réelle\n",
    "    min_coverage=0.8,\n",
    "    min_count_floor=3\n",
    ")\n",
    "\n",
    "# 👁️ Affichage des tags dominants\n",
    "display(dominant_tags)\n",
    "display(tag_freq_df_marked)\n",
    "\n",
    "# 📊 Visualisation de la couverture cumulative\n",
    "eda.plot_tag_coverage(\n",
    "    tag_freq_df_marked,\n",
    "    coverage_target=0.8,\n",
    "    title_suffix=\"corpus de 50 000 questions\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506bd157",
   "metadata": {},
   "source": [
    "#### **3.2.6. CO-OCCURRENCE DES TAGS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539c9def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "# # --- CONSTRUCTION DES MATRICES\n",
    "# cooc_matrix, tag_matrix = eda.build_tag_cooccurrence_matrix(\n",
    "#     df_questions=full_explo_wo,\n",
    "#     dominant_tags=dominant_tags,\n",
    "#     tag_col=\"Tags\"\n",
    "# )\n",
    "# # --- HEATMAP DES CO OCCURRENCES\n",
    "# cooc_matrix.style.background_gradient(cmap=\"Blues\")\n",
    "\n",
    "# eda.plot_tag_cooccurrence_heatmap(cooc_matrix)\n",
    "# # --- GRAPHE DE CO OCCURRENCE VISUEL\n",
    "# eda.plot_tag_cooccurrence_graph(\n",
    "#     cooc_matrix,\n",
    "#     min_edge_weight=2,           # pour éviter les arêtes anecdotiques\n",
    "#     layout=\"spring\"              # layout fluide et lisible\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f144e91",
   "metadata": {},
   "source": [
    "## **4. FEATURE ENGINEERING**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e73ba5",
   "metadata": {},
   "source": [
    "### **4.1. APPLICATION SUR UN ECHANTILLON DE 100 QUESTIONS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ad831e",
   "metadata": {},
   "source": [
    "#### **4.1.1. VECTORISATION TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1f0715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "# --------------------------------------\n",
    "# --- VECTORISATION DU CORPUS FUSIONNÉ\n",
    "# --------------------------------------\n",
    "X_titlebody_tfidf, titlebody_vocab, top_titlebody_words = eda.vectorize_tfidf(\n",
    "    sample_explo_wo[\"clean_title_body\"],\n",
    "    label=\"title + body\"\n",
    ")\n",
    "\n",
    "# 📊 Aperçu des termes les plus fréquents\n",
    "top_titlebody_words_df = pd.DataFrame(top_titlebody_words, columns=[\"word\", \"score\"])\n",
    "display(top_titlebody_words_df.head(10))\n",
    "\n",
    "# 🔍 Shape du matrice TF-IDF\n",
    "print(f\"Dimensions du X_titlebody_tfidf : {X_titlebody_tfidf.shape}\")\n",
    "from scipy.sparse import save_npz\n",
    "save_npz(\"models/tfidf/X_tfidf_sample.npz\", X_titlebody_tfidf)\n",
    "import pickle\n",
    "with open(\"models/tfidf/vocab_tfidf_sample.pkl\", \"wb\") as f:\n",
    "    pickle.dump(titlebody_vocab, f)\n",
    "with open(\"models/tfidf/corpus_sample.pkl\", \"wb\") as f:\n",
    "    pickle.dump(sample_explo_wo[\"clean_title_body\"].tolist(), f)\n",
    "\n",
    "with open(\"models/tfidf/top_titlebody_words.pkl\", \"wb\") as f:\n",
    "    pickle.dump(top_titlebody_words, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa56f268",
   "metadata": {},
   "source": [
    "#### **4.1.2. REDUCTION DE DIMENSION (SVD, PCA)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e236974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "# ----------------------------------------------\n",
    "# --- REDUCTION DIMENSIONS SVD DU CORPUS FUSIONNÉ\n",
    "# ----------------------------------------------\n",
    "# --- Reduction en 2D pour visualisation\n",
    "X_titlebody_svd, svd_titlebody = eda.apply_svd_and_plot(X_titlebody_tfidf, label=\"title + body\")\n",
    "\n",
    "\n",
    "# --- Réduction à n dimensions avec suivi de la variance cumulée\n",
    "X_titlebody_svd100, svd_titlebody100 = eda.apply_svd_variance(X_titlebody_tfidf, label=\"title + body\", n_components=50)\n",
    "np.save(\"models/svd/X_titlebody_svd_100.npy\", X_titlebody_svd100)\n",
    "import pickle\n",
    "with open(\"models/svd/svd_model_100.pkl\", \"wb\") as f:\n",
    "    pickle.dump(svd_titlebody100, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8079efd1",
   "metadata": {},
   "source": [
    "#### **4.1.3. EMBEDDINGS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bb177e",
   "metadata": {},
   "source": [
    "##### 4.1.3.1. METHODE SBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efe5e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "corpus_titlebody = sample_explo_wo[\"clean_title_body\"].fillna(\"\").tolist()\n",
    "embeddings_sbert = eda.encode_sbert_corpus(corpus_titlebody, batch_size=32)\n",
    "print(f\"✅ SBERT Embeddings shape : {embeddings_sbert.shape}\")\n",
    "np.save(\"models/sbert/embeddings_sbert.npy\", embeddings_sbert)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cc806e",
   "metadata": {},
   "source": [
    "##### 4.1.3.2. METHODE WORD2VEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa10821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "corpus_titlebody = sample_explo_wo[\"clean_title_body\"].fillna(\"\").tolist()\n",
    "\n",
    "model_w2v = eda.train_word2vec(\n",
    "    corpus=corpus_titlebody,\n",
    "    save_path=\"models/word2vec/word2vec_titlebody.bin\",  # dossier à créer si nécessaire\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=3,\n",
    "    epochs=10,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 🔁 Vectorisation du corpus complet\n",
    "w2v_model_sample = Word2Vec.load(\"models/word2vec/word2vec_titlebody.bin\")\n",
    "corpus_sample = sample_explo_wo[\"clean_title_body\"].fillna(\"\").tolist()\n",
    "X_w2v_sample = eda.vectorize_texts(corpus_sample, w2v_model_sample)\n",
    "\n",
    "# 💾 Sauvegarde\n",
    "np.save(\"models/word2vec/X_w2v_sample.npy\", X_w2v_sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683e4908",
   "metadata": {},
   "source": [
    "##### 4.1.3.3. METHODE UNIVERSAL SENTENCE ENCODER (USE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a44e6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "\n",
    "corpus_titlebody = sample_explo_wo[\"clean_title_body\"].fillna(\"\").tolist()\n",
    "\n",
    "embeddings_use = eda.encode_use_corpus(corpus_titlebody, batch_size=100)\n",
    "print(f\"✅ USE Embeddings shape : {embeddings_use.shape}\")\n",
    "np.save(\"models/use_model/embeddings_use.npy\", embeddings_use)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f914661",
   "metadata": {},
   "source": [
    "#### **4.1.4. VECTORISATION BoW (CountVectorizer)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2801ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "corpus_bow, X_bow, vocab_bow = eda.build_bow_matrix(\n",
    "    df=sample_explo_wo,\n",
    "    col_title=\"clean_title_body\",  # ✅ corpus fusionné\n",
    "    col_body=\"clean_title_body\",  # ✅ encore le même\n",
    "    max_df=0.9,\n",
    "    min_df=5,\n",
    "    stop_words=\"english\"\n",
    ")\n",
    "\n",
    "print(\"# --- Matrice BoW:\", X_bow.shape)\n",
    "print(\"# --- Taille du vocabulaire:\", len(vocab_bow))\n",
    "import scipy.sparse\n",
    "scipy.sparse.save_npz(\"models/bow/X_bow.npz\", X_bow)\n",
    "import pickle\n",
    "with open(\"models/bow/corpus_bow.pkl\", \"wb\") as f:\n",
    "    pickle.dump(corpus_bow, f)\n",
    "with open(\"models/bow/vocab_bow.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vocab_bow, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adef1b24",
   "metadata": {},
   "source": [
    "#### **4.1.5. ENCODAGE DES TAGS (MULTI-LABEL BINARIZER)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c04f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# --- VERIFICATION ET CONVERSION DES TAGS SI NECESSAIRE\n",
    "# ------------------------------------------------------\n",
    "if isinstance(sample_explo_wo[\"Tags\"].iloc[0], str):\n",
    "    sample_explo_wo[\"Tags\"] = sample_explo_wo[\"Tags\"].apply(ast.literal_eval)\n",
    "# -------------------------\n",
    "# --- ENCODAGE MULTI-LABEL\n",
    "# -------------------------\n",
    "mlb = MultiLabelBinarizer()\n",
    "tags_encoded = mlb.fit_transform(sample_explo_wo[\"Tags\"])\n",
    "# ---------------------------------\n",
    "# --- CREATION DU DATAFRAME ENCODE\n",
    "# ----------------------------------\n",
    "tags_df = pd.DataFrame(tags_encoded, columns=mlb.classes_)\n",
    "print(\"# --- ON AFFICHE LES TAGS ENCODES\")\n",
    "display(tags_df)\n",
    "# --------------------------------------------\n",
    "# --- FUSION AVEC LE DATAFRAME sample_explo_wo\n",
    "# ---------------------------------------------\n",
    "sample_explo_wo = pd.concat([sample_explo_wo, tags_df], axis=1)\n",
    "print(\"# --- ON AFFICHE LA FUSION DES TAGS ENCODES AVEC LE DATAFRAME sample_explo_wo : \")\n",
    "display(sample_explo_wo)\n",
    "# ----------------------------\n",
    "# --- SAUVEGARDE DU BINARIZER\n",
    "# -----------------------------\n",
    "import os\n",
    "import joblib\n",
    "# 🔐 Création du dossier standardisé\n",
    "os.makedirs(\"models/tags\", exist_ok=True)\n",
    "# 💾 Sauvegarde du binarizer\n",
    "joblib.dump(mlb, \"models/tags/multilabel_binarizer_sample.pkl\")\n",
    "np.save(\"models/tags/y_tags.npy\", tags_encoded)\n",
    "\n",
    "\n",
    "\n",
    "# --- VISUALISATION \n",
    "tag_counts = tags_df.sum().sort_values(ascending=False)\n",
    "tag_freq_df = tag_counts.reset_index()\n",
    "tag_freq_df.columns = [\"Tag\", \"Count\"]\n",
    "eda.plot_tag_occurrences(tag_freq_df, top_n=15, palette=\"mako\")\n",
    "eda.plot_tag_distribution(tag_freq_df, top_n=15, palette=\"mako\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13deee3e",
   "metadata": {},
   "source": [
    "### 4.2. APPLICATION SUR 50 K QUESTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e100aa67",
   "metadata": {},
   "source": [
    "#### **4.2.1. VECTORISATION TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac36b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "X_titlebody_tfidf_full, titlebody_vocab_full, top_titlebody_words_full = eda.vectorize_tfidf(\n",
    "    full_explo_wo[\"clean_title_body\"],\n",
    "    label=\"title + body\",\n",
    "    max_features=10000,\n",
    "    min_df=5,\n",
    "    top_n=20,\n",
    "    show_wordcloud=False  # Désactivé si tu veux préserver la RAM\n",
    ")\n",
    "from scipy.sparse import save_npz\n",
    "import pickle\n",
    "\n",
    "# 🔒 Sauvegarde de la matrice TF-IDF\n",
    "save_npz(\"models/tfidf/X_tfidf_full.npz\", X_titlebody_tfidf_full)\n",
    "\n",
    "# 📦 Sauvegarde du vocabulaire\n",
    "with open(\"models/tfidf/vocab_tfidf_full.pkl\", \"wb\") as f:\n",
    "    pickle.dump(titlebody_vocab_full, f)\n",
    "\n",
    "with open(\"models/tfidf/top_titlebody_words_full.pkl\", \"wb\") as f:\n",
    "    pickle.dump(top_titlebody_words_full, f)\n",
    "\n",
    "with open(\"models/tfidf/corpus_full.pkl\", \"wb\") as f:\n",
    "    pickle.dump(full_explo_wo[\"clean_title_body\"].tolist(), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d21cd8",
   "metadata": {},
   "source": [
    "#### **4.2.2. REDUCTION DE DIMENSION (SVD, PCA)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb11439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "from scipy.sparse import load_npz\n",
    "\n",
    "X_titlebody_tfidf_full = load_npz(\"models/tfidf/X_tfidf_full.npz\")\n",
    "\n",
    "# --- REDUCTION SVD POUR VISUALISATION 2D\n",
    "X_titlebody_svd_full, svd_titlebody_full = eda.apply_svd_and_plot(\n",
    "    X_titlebody_tfidf_full,\n",
    "    label=\"title + body\",\n",
    "    n_components=2\n",
    ")\n",
    "# --- REDUCTION SVD POUR EXTRACTION DES FEATURES ET SAUVEGARDE EN .npy\n",
    "X_titlebody_svd10k, svd_titlebody10k = eda.apply_svd_variance(\n",
    "    X_titlebody_tfidf_full,\n",
    "    label=\"title + body\",\n",
    "    n_components=50\n",
    ")\n",
    "import numpy as np\n",
    "np.save(\"models/svd/X_titlebody_svd10k.npy\", X_titlebody_svd10k)\n",
    "\n",
    "import pickle\n",
    "with open(\"models/svd/svd_model_10k.pkl\", \"wb\") as f:\n",
    "    pickle.dump(svd_titlebody10k, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fb944c",
   "metadata": {},
   "source": [
    "#### **4.2.3. EMBEDDINGS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2e28a7",
   "metadata": {},
   "source": [
    "##### 4.2.3.1. METHODE SBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d8f698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "corpus_titlebody_full = full_explo_wo[\"clean_title_body\"].fillna(\"\").tolist()\n",
    "embeddings_sbert_full = eda.encode_sbert_corpus(corpus_titlebody_full, batch_size=32)\n",
    "print(f\"✅ SBERT Embeddings shape : {embeddings_sbert_full.shape}\")\n",
    "np.save(\"models/sbert/embeddings_sbert_full.npy\", embeddings_sbert_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe22d10",
   "metadata": {},
   "source": [
    "##### 4.2.3.2. METHODE WORD2VEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28fa312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "corpus_titlebody_full = full_explo_wo[\"clean_title_body\"].fillna(\"\").tolist()\n",
    "\n",
    "model_w2v_full = eda.train_word2vec(\n",
    "    corpus=corpus_titlebody_full,\n",
    "    save_path=\"models/word2vec/word2vec_titlebody_full.bin\",  # dossier à créer si nécessaire\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=3,\n",
    "    epochs=10,\n",
    "    verbose=True\n",
    ")\n",
    "# 🔁 Vectorisation du corpus complet\n",
    "w2v_model_full = Word2Vec.load(\"models/word2vec/word2vec_titlebody_full.bin\")\n",
    "# corpus_full = full_df[\"clean_title_body\"].fillna(\"\").tolist()\n",
    "X_w2v_full = eda.vectorize_texts(corpus_titlebody_full, w2v_model_full)\n",
    "\n",
    "# 💾 Sauvegarde\n",
    "np.save(\"models/word2vec/X_w2v_full.npy\", X_w2v_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2d466c",
   "metadata": {},
   "source": [
    "##### 4.2.3.3. METHODE UNIVERSAL SENTENCE ENCODER (USE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ff8cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "\n",
    "corpus_titlebody_full = full_explo_wo[\"clean_title_body\"].fillna(\"\").tolist()\n",
    "\n",
    "embeddings_use_full = eda.encode_use_corpus(corpus_titlebody_full, batch_size=100)\n",
    "print(f\"✅ USE Embeddings shape : {embeddings_use_full.shape}\")\n",
    "np.save(\"models/use_model/embeddings_use_full.npy\", embeddings_use_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678674ce",
   "metadata": {},
   "source": [
    "#### **4.2.4. VECTORISATION BoW (CountVectorizer)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caee444c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "corpus_bow_full, X_bow_full, vocab_bow_full = eda.build_bow_matrix(\n",
    "    df=full_explo_wo,\n",
    "    col_title=\"clean_title_body\",  # ✅ corpus fusionné\n",
    "    col_body=\"clean_title_body\",  # ✅ encore le même\n",
    "    max_df=0.9,\n",
    "    min_df=5,\n",
    "    stop_words=\"english\"\n",
    ")\n",
    "\n",
    "print(\"# --- Matrice BoW:\", X_bow_full.shape)\n",
    "print(\"# --- Taille du vocabulaire:\", len(vocab_bow_full))\n",
    "import scipy.sparse\n",
    "scipy.sparse.save_npz(\"models/bow/X_bow_full.npz\", X_bow_full)\n",
    "import pickle\n",
    "with open(\"models/bow/corpus_bow_full.pkl\", \"wb\") as f:\n",
    "    pickle.dump(corpus_bow_full, f)\n",
    "with open(\"models/bow/vocab_bow_full.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vocab_bow_full, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c45be3d",
   "metadata": {},
   "source": [
    "#### **4.2.5. ENCODAGE DES TAGS (MULTI-LABEL BINARIZER)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed51926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import joblib\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --------------------------\n",
    "# 🔍 Vérification des tags\n",
    "# --------------------------\n",
    "if isinstance(full_explo_wo[\"Tags\"].iloc[0], str):\n",
    "    full_explo_wo[\"Tags\"] = full_explo_wo[\"Tags\"].apply(ast.literal_eval)\n",
    "\n",
    "# ---------------------------\n",
    "# 🧠 Encodage multilabel\n",
    "# ---------------------------\n",
    "mlb_full = MultiLabelBinarizer()\n",
    "tags_encoded_full = mlb_full.fit_transform(full_explo_wo[\"Tags\"])\n",
    "\n",
    "# ✅ DataFrame des tags encodés\n",
    "tags_df_full = pd.DataFrame(tags_encoded_full, columns=mlb_full.classes_)\n",
    "print(f\"Shape du matrix multi-label : {tags_encoded_full.shape}\")\n",
    "display(tags_df_full.head())\n",
    "\n",
    "# -----------------------------------\n",
    "# 🔗 Fusion avec full_explo_wo\n",
    "# -----------------------------------\n",
    "full_explo_wo = pd.concat([full_explo_wo, tags_df_full], axis=1)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 💾 Sauvegarde du binarizer + vecteur encodé\n",
    "# ---------------------------------------------\n",
    "os.makedirs(\"models/tags\", exist_ok=True)\n",
    "np.save(\"models/tags/y_tags_full.npy\", tags_encoded_full)\n",
    "joblib.dump(mlb_full, \"models/tags/multilabel_binarizer_full.pkl\")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 📊 Visualisation des tags les plus fréquents\n",
    "# ---------------------------------------------\n",
    "tag_counts = tags_df_full.sum().sort_values(ascending=False)\n",
    "tag_freq_df = tag_counts.reset_index()\n",
    "tag_freq_df.columns = [\"Tag\", \"Count\"]\n",
    "\n",
    "# --- Barplot (Top 15 Tags)\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(data=tag_freq_df.head(15), x=\"Count\", y=\"Tag\", palette=\"mako\")\n",
    "plt.title(\"Top 15 des tags les plus fréquents\")\n",
    "plt.xlabel(\"Nombre d’occurrences\")\n",
    "plt.ylabel(\"Tag\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d47dd0",
   "metadata": {},
   "source": [
    "## **5. EXPORT**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de382bbd",
   "metadata": {},
   "source": [
    "### 5.1. APPLICATION SUR UN ECHANTILLON DE 100 QUESTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d48c85",
   "metadata": {},
   "source": [
    "Les artefacts `corpus`, `X_bow` et `vocab` ayant été créés dans la section précédente dédiée à la vectorisation Bag-of-Words, ils sont ici uniquement exportés pour être utilisés dans le notebook `3_modele_non_supervise_lda.ipynb` (modélisation LDA).\n",
    "\n",
    "Aucun recalcul n’est effectué ici afin de garantir la modularité du pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fdac7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Dimensions du dataframe sample_df : {sample_explo_wo.shape}\")\n",
    "print(f\" Colonnes du dataframe sample_df : {sample_explo_wo.columns.tolist()}\")\n",
    "print(f\"Dimensions du dataframe full_df : {full_explo_wo.shape}\")\n",
    "print(f\" Colonnes du dataframe full_df : {full_explo_wo.columns.tolist()}\")\n",
    "\n",
    "# ---------------------------------\n",
    "# --- CREATION DU DOSSIER D'EXPORT\n",
    "# ---------------------------------\n",
    "os.makedirs(\"../data/processed\", exist_ok=True)\n",
    "# ------------------------------------------------------------\n",
    "# --- EXPORT DES DATAFRAMES NETTOYES SANS OUTLIERS ET EXPLORES\n",
    "# -------------------------------------------------------------\n",
    "# 💾 sample_explo_wo (100 questions sans outliers)\n",
    "sample_explo_wo.to_parquet(\"data/processed/sample_explo_wo.parquet\", index=False)\n",
    "\n",
    "# 💾 full_explo_wo (10k questions sans outliers)\n",
    "full_explo_wo.to_parquet(\"data/processed/full_explo_wo.parquet\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b66acbf",
   "metadata": {},
   "source": [
    "### 5.2. EXPORT SUR 50 K QUESTIONS\n",
    "\n",
    "Cette sous-section permet de sauvegarder tous les artefacts NLP construits sur le corpus complet (`brut_df`) afin de les rendre accessibles aux notebooks suivants :  \n",
    "`3_modele_non_supervise_lda.ipynb`, `4_modele_supervise_mlflow.ipynb`, etc.\n",
    "\n",
    "\n",
    "\n",
    "***Corpus exporté :***\n",
    "\n",
    "| Nom | Format | Description |\n",
    "|-----|--------|-------------|\n",
    "| `corpus_for_lda.csv` | CSV | Texte nettoyé (titre + body concaténé) avec `PostId` |\n",
    "| `corpus_for_lda_bow.pkl` | PKL | Matrice BoW pour modélisation LDA |\n",
    "| `corpus_for_lda_vocab.pkl` | PKL | Vocabulaire associé à la BoW |\n",
    "| `tfidf_matrix_concat.pkl` | PKL | Matrice TF-IDF des 50k questions |\n",
    "| `tfidf_vocab_concat.pkl` | PKL | Vocabulaire TF-IDF (concaténé) |\n",
    "| `dominant_tags_brut.pkl` | PKL | Liste des tags dominants repérés sur `brut_df` |\n",
    "| `multilabel_binarizer.pkl` | PKL | Encodeur des tags multi-label (sklearn) |\n",
    "| `tags_encoded_brut.pkl` | PKL | Matrice Y cible (multi-label) pour modèle supervisé |\n",
    "\n",
    "\n",
    "\n",
    "Ces artefacts sont réutilisables sans recalcul dans tous les notebooks de modélisation, et peuvent être versionnés pour suivre l’évolution du pipeline.\n",
    "\n",
    "Cette étape garantit la modularité du projet et son passage en production.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11acb946",
   "metadata": {},
   "source": [
    "## ANNEXE "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e5fcab",
   "metadata": {},
   "source": [
    "### A.1. Nomenclature des DataFrames utilisés\n",
    "\n",
    "| Nom du DataFrame       | Description |\n",
    "|------------------------|-------------|\n",
    "| `brut_df`              | Données brutes chargées depuis le fichier CSV |\n",
    "| `sample_df`            | Échantillon de 100 questions tiré de `brut_df` |\n",
    "| `sample_clean`         | Version nettoyée de `sample_df` (après prétraitement) |\n",
    "| `sample_explo`         | Copie de travail pour l’exploration (à partir de `sample_clean`) |\n",
    "| `sample_explo_wo`      | Version de `sample_explo` sans outliers (`wo = without outliers`) |\n",
    "| `sample_outliers`      | Sous-ensemble de `sample_explo` contenant uniquement les outliers |\n",
    "| `full_clean`           | Données complètes nettoyées (lors de la mise à l’échelle) |\n",
    "| `full_explo`           | Copie de travail pour l’exploration complète |\n",
    "| `full_explo_wo`        | Version sans outliers du corpus complet |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813bff81",
   "metadata": {},
   "source": [
    "### A.2. RAPPELS DES NOTIONS IMPLIQUEES DANS LE FEATURE ENGINEERING EN NLP\n",
    "\n",
    "Cette annexe vise à expliciter les concepts mathématiques et vectoriels sous-jacents aux principales étapes de transformation des textes dans le cadre du NLP. Elle s’adresse à un lecteur ayant une formation scientifique avancée.\n",
    "\n",
    "\n",
    "\n",
    "#### A.2.1. Vecteurs creux vs. vecteurs denses\n",
    "\n",
    "- Un **vecteur creux** (sparse vector) est un vecteur dont la majorité des composantes sont nulles.\n",
    "  - Exemple : `[0, 0, 0.5, 0, 0, 0.3, 0, 0, 0, 0.2]`\n",
    "  - Typique des représentations TF-IDF : chaque document n’utilise qu’un petit sous-ensemble du vocabulaire.\n",
    "\n",
    "- Un **vecteur dense** est un vecteur dont la plupart des composantes sont non nulles.\n",
    "  - Exemple : `[0.12, -0.03, 0.45, 0.08, -0.22, 0.19]`\n",
    "  - Typique des embeddings ou des vecteurs après réduction de dimension.\n",
    "\n",
    "\n",
    "\n",
    "#### A.2.2. Variables latentes et espace latent\n",
    "\n",
    "- Une **variable latente** est une variable non observée directement, mais déduite à partir des données.\n",
    "- En NLP, les **axes latents** correspondent à des thématiques ou structures cachées dans les textes.\n",
    "- L’**espace latent** est un espace vectoriel réduit, où chaque dimension représente une composante thématique implicite.\n",
    "\n",
    "\n",
    "\n",
    "#### A.2.3. Base canonique vs. base sémantique\n",
    "\n",
    "- La **base canonique** est formée par les mots du vocabulaire : chaque mot est une dimension orthogonale.\n",
    "- Elle est arbitraire, très grande, et souvent redondante.\n",
    "- Une **base sémantique** est une base apprise ou extraite (via SVD ou embeddings) où chaque dimension reflète un thème ou une proximité de sens.\n",
    "- Elle est plus compacte, plus informative, mais non orthogonale.\n",
    "\n",
    "\n",
    "\n",
    "#### A.2.4. Décomposition SVD (Singular Value Decomposition)  \n",
    "\n",
    "Soit une matrice TF-IDF X de taille (m × n), où :\n",
    "\n",
    "- m est le nombre de documents\n",
    "- n est la taille du vocabulaire (nombre de mots uniques)\n",
    "- Chaque ligne de X est un vecteur TF-IDF représentant un document\n",
    "\n",
    "La décomposition en valeurs singulières (SVD) permet d’écrire :\n",
    "\n",
    "X = U × Σ × Vᵗ\n",
    "\n",
    "avec :\n",
    "\n",
    "- U ∈ ℝ^(m × r) : matrice des vecteurs propres des documents (orthogonaux)\n",
    "- Σ ∈ ℝ^(r × r) : matrice diagonale contenant les valeurs singulières décroissantes\n",
    "- V ∈ ℝ^(n × r) : matrice des vecteurs propres des mots (orthogonaux)\n",
    "\n",
    "où r est le rang de X (r ≤ min(m, n))\n",
    "\n",
    "\n",
    "**Réduction de dimension**\n",
    "\n",
    "En pratique, on ne conserve que les k premières composantes (avec k ≪ r), ce qui donne une approximation de rang k :\n",
    "\n",
    "X ≈ U_k × Σ_k × V_kᵗ\n",
    "\n",
    "avec :\n",
    "\n",
    "- U_k ∈ ℝ^(m × k) : projection des documents dans un espace latent de dimension k\n",
    "- Σ_k ∈ ℝ^(k × k) : valeurs singulières principales\n",
    "- V_k ∈ ℝ^(n × k) : projection des mots dans le même espace latent\n",
    "\n",
    "\n",
    "**Interprétation**\n",
    "\n",
    "- Chaque document est représenté par un vecteur dense de dimension k, capturant ses composantes thématiques latentes\n",
    "- Chaque mot est également projeté dans cet espace, révélant ses affinités sémantiques\n",
    "- Les valeurs singulières indiquent l’importance relative de chaque axe latent\n",
    "\n",
    "**Utilité en NLP**\n",
    "\n",
    "- Réduction de dimension : compresser les vecteurs TF-IDF très grands et creux\n",
    "- Suppression du bruit lexical : éliminer les dimensions peu informatives\n",
    "- Découverte de thématiques : chaque axe latent peut être interprété comme un thème sémantique\n",
    "\n",
    "Cette technique est à la base de la méthode LSA (Latent Semantic Analysis).\n",
    "\n",
    "#### A.2.5. Embeddings (Word2Vec, FastText, BERT…)\n",
    "\n",
    "- Les **embeddings** sont des vecteurs denses appris par des modèles de langage.\n",
    "- Ils sont construits pour que les mots ayant des contextes similaires aient des vecteurs proches.\n",
    "- Contrairement à TF-IDF, ils capturent :\n",
    "  - La **sémantique** (proximité de sens)\n",
    "  - Le **contexte** (dans les modèles comme BERT)\n",
    "  - Les **relations syntaxiques** et **logiques**\n",
    "\n",
    "Exemple d’analogie vectorielle :  \n",
    "`king - man + woman ≈ queen`\n",
    "\n",
    "Mathématiquement, les embeddings sont issus d’une **factorisation implicite** de la matrice de co-occurrence des mots, ou d’un apprentissage supervisé/non supervisé sur des tâches de prédiction de contexte.\n",
    "\n",
    "\n",
    "\n",
    "#### A.2.6. Encodage des tags – MultiLabelBinarizer\n",
    "\n",
    "- Les tags sont des cibles multi-label (un document peut avoir plusieurs étiquettes).\n",
    "- Le **MultiLabelBinarizer** transforme chaque liste de tags en un vecteur binaire.\n",
    "\n",
    "Exemple :\n",
    "\n",
    "| Tags | Encodage |\n",
    "|------|----------|\n",
    "| ['python', 'django'] | [1, 0, 1, 0, …] |\n",
    "\n",
    "Chaque dimension correspond à un tag unique.  \n",
    "Ce format est requis pour entraîner un modèle de classification multi-label.\n",
    "\n",
    "\n",
    "\n",
    "#### A.2.7. Résumé global du pipeline vectoriel\n",
    "\n",
    "| Étape | Entrée | Sortie | Objectif |\n",
    "|-------|--------|--------|----------|\n",
    "| TF-IDF | Texte brut | Vecteurs creux | Pondérer les mots |\n",
    "| SVD / PCA | TF-IDF | Vecteurs denses réduits | Capturer les axes thématiques |\n",
    "| Embeddings | Texte brut | Vecteurs denses sémantiques | Capturer le sens |\n",
    "| MultiLabelBinarizer | Tags | Vecteurs binaires (cibles) | Préparer les cibles |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f855595",
   "metadata": {},
   "source": [
    "### A.3. FONCTIONNEMENT DE FASTAPI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab3bc84",
   "metadata": {},
   "source": [
    "```text\n",
    "+-------------------------------+\n",
    "|        Client (UI,           |\n",
    "|   script Python, Swagger)    |\n",
    "+---------------+---------------+\n",
    "                |\n",
    "                |  POST /embed/sbert\n",
    "                |  { \"text\": \"...\" }\n",
    "                v\n",
    "+---------------+---------------+\n",
    "|         FastAPI App           |\n",
    "|     (src/api/main.py)         |\n",
    "+---------------+---------------+\n",
    "                |\n",
    "                v\n",
    "+---------------+---------------+\n",
    "|  Route /embed/sbert           |\n",
    "|  (embed_routes.py)            |\n",
    "|  - Valide l'entrée (pydantic) |\n",
    "|  - Appelle encode_with_sbert  |\n",
    "+---------------+---------------+\n",
    "                |\n",
    "                v\n",
    "+---------------+---------------+\n",
    "|  SBERT Model (sbert_model.py) |\n",
    "+---------------+---------------+\n",
    "                |\n",
    "                v\n",
    "+---------------+---------------+\n",
    "|  Embedding vector (list)      |\n",
    "|  ex: [0.123, -0.456, ...]     |\n",
    "+---------------+---------------+\n",
    "                |\n",
    "                v\n",
    "+-------------------------------+\n",
    "|   JSON Response to Client     |\n",
    "|   { \"embedding\": [...] }      |\n",
    "+-------------------------------+\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c9119e",
   "metadata": {},
   "source": [
    "### A.4. MISE A L'ECHELLE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e920196",
   "metadata": {},
   "source": [
    "Ce notebook a été développé sur `sample_df` pour valider les fonctions de nettoyage, d’analyse exploratoire et de construction des features textuelles.\n",
    "\n",
    "***Objectif à l’échelle : prétraiter les 50 000 questions StackOverflow extraites via SQL.***\n",
    "\n",
    "| Étape | Action | Détails |\n",
    "|-------|--------|---------|\n",
    "|  Chargement des données brutes | Import du `brut_df` (50 000 questions) | Format : `.csv` ou SQL → colonnes `Title`, `Body`, `Tags`, etc. |\n",
    "|  Nettoyage des textes | Application des fonctions `clean_title`, `clean_body` | Nettoyage HTML, ponctuation, contractions, stopwords |\n",
    "|  Fusion des colonnes | Création de `clean_title_body` | Pour vectorisations futures (BoW, TF-IDF, embeddings) |\n",
    "|  Suppression des doublons exacts | `df.duplicated([\"Title\", \"Body\"])` | Élimination des doublons 1:1 |\n",
    "|  Filtrage d’outliers lexicaux | Longueur minimale des titres/corps | Exemple : `len(title) > 10` ou `nb_mots > 3` |\n",
    "|  Analyse optionnelle | Stats descriptives & distribution de longueur | Permet de valider la structure du corpus brut |\n",
    "|  Export des artefacts | Sauvegarde du corpus nettoyé dans `/data/processed/` | `brut_df_cleaned.csv` ou `.pkl` |\n",
    "\n",
    " Cette étape garantit que l’ensemble du corpus est prêt pour la modélisation à grande échelle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c7e091",
   "metadata": {},
   "source": [
    "Ce notebook a été conçu pour valider les étapes de prétraitement NLP sur un échantillon réduit (`sample_df`, 100 questions).  \n",
    "La mise à l’échelle sur le corpus complet (`brut_df`, 50 000 questions) est structurée de manière parallèle, chaque sous-section étant dupliquée pour assurer cohérence et industrialisation.\n",
    "\n",
    "---\n",
    "\n",
    "***Structure de Mise à l’Échelle par Bloc***\n",
    "\n",
    "| Section originale | Sous-section à créer pour `brut_df` | Objectif |\n",
    "|------------------|--------------------------------------|----------|\n",
    "| `2.1. Nettoyage - sample_df` | `2.2. Nettoyage - brut_df` | Nettoyer les 50k titres et corps via `clean_text_spacy_custom()` |\n",
    "| `3.1. Exploration - sample_explo` | `3.2. Exploration - full_explo` | Calculer longueurs, ratios, distributions sur tout le corpus |\n",
    "| `3.1.4 Détection des doublons/outliers - sample_df` | `3.2.4 Détection - brut_df` | Exclure les lignes bruitées avant vectorisation |\n",
    "| `3.1.5 Analyse des tags - sample_df` | `3.2.5 Analyse des tags - brut_df` | Étudier la diversité et marquer les tags dominants sur 50k |\n",
    "| `3.1.6 Co-occurrence des tags` | `3.2.6 Co-occurrence des tags - brut_df` | Identifier clusters thématiques globaux |\n",
    "| `4.1.1 Vectorisation TF-IDF - sample_df` | `4.2.1 Vectorisation TF-IDF - brut_df` | Générer les vecteurs `X_titlebody_tfidf_brut.pkl` |\n",
    "| `4.1.4 Vectorisation BoW - sample_df` | `4.2.4 Vectorisation BoW - brut_df` | Générer `X_bow_brut.pkl` pour LDA |\n",
    "| `4.1.5 Encodage des tags - sample_df` | `4.2.5 Encodage des tags - brut_df` | Générer la matrice cible multi-label `tags_encoded_brut.pkl` |\n",
    "| `5. Export - sample_df` | `5. Export - brut_df` | Sauvegarde des artefacts TF-IDF, BoW, corpus nettoyé, vocabulaire |\n",
    "\n",
    "---\n",
    "\n",
    "***Artefacts à générer pour mise à l’échelle***\n",
    "\n",
    "| Type | Nom suggéré | Format |\n",
    "|------|-------------|--------|\n",
    "| Corpus nettoyé complet | `corpus_cleaned_brut.csv` | CSV |\n",
    "| Matrice BoW | `corpus_for_lda_bow_brut.pkl` | PKL |\n",
    "| Vocabulaire BoW | `corpus_for_lda_vocab_brut.pkl` | PKL |\n",
    "| Matrice TF-IDF | `tfidf_matrix_concat_brut.pkl` | PKL |\n",
    "| Vocab TF-IDF | `tfidf_vocab_concat_brut.pkl` | PKL |\n",
    "| Binarizer tags | `multilabel_binarizer_brut.pkl` | PKL |\n",
    "| Matrice tags encodés | `tags_encoded_brut.pkl` | PKL |\n",
    "\n",
    "\n",
    "\n",
    "***Recommandations***\n",
    "\n",
    "- Utiliser le flag `use_sample=True/False` dans les fonctions du module `eda_analysis.py` pour switcher d’un jeu à l’autre.\n",
    "- Surveiller la mémoire lors de l’utilisation d’embeddings ou de réductions de dimension : lancer par batch si nécessaire.\n",
    "- Documenter chaque sous-section `brut_df` avec la mention :  \n",
    "  *\"Cette sous-section applique la logique validée sur `sample_df` au corpus complet de 50k questions (`brut_df`).\"*\n",
    "\n",
    "\n",
    "\n",
    "Ce plan permet une industrialisation progressive et structurée du pipeline, tout en conservant la rigueur d’exploration testée sur l’échantillon.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21132ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import kagglehub\n",
    "# path = kagglehub.model_download(\"google/universal-sentence-encoder/tensorFlow2/universal-sentence-encoder\")\n",
    "# print(\"✅ Modèle téléchargé dans :\", path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "StackOverFlowTags (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
