{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "938f8410",
   "metadata": {},
   "source": [
    "## **IMPORTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93e5ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(f\"# --- VERIFICATION DU REPERTOIRE COURANT : {os.getcwd()}\")\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from preprocessing import text_cleaning\n",
    "from tqdm.notebook import tqdm\n",
    "from preprocessing.text_cleaning import load_tech_terms\n",
    "print(load_tech_terms())\n",
    "from eda.eda_analysis import plot_distribution\n",
    "from collections import Counter\n",
    "from scipy.stats import entropy\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from api.client.embedding_client import get_embedding\n",
    "from api.client.embedding_client import encode_with_sbert_batchwise\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import joblib\n",
    "import ast\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4586150",
   "metadata": {},
   "source": [
    "## **1. RECUPERATION DES DONNEES**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaefdd0",
   "metadata": {},
   "source": [
    "### **1.1. EXTRACTION DES DONNEES VIA STACK EXCHANGE DATA EXPLORER**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31a2f55",
   "metadata": {},
   "source": [
    "### **1.2. CHARGEMENT DES DONNEES DANS UN DATAFRAME**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3d3001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# --- CHARGER LE FICHIER CSV\n",
    "# ---------------------------\n",
    "full_df = pd.read_csv(\"../data/raw/stackoverflow_questions_sede.csv\")\n",
    "# -----------------------------------------------\n",
    "# --- VERIFIER LE NOMBRE DE LIGNES ET UN APERCU\n",
    "# -----------------------------------------------\n",
    "print(f\"Nombre de questions : {len(full_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a73a9a5",
   "metadata": {},
   "source": [
    "## **2. NETTOYAGE DES DONNEES**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7ee747",
   "metadata": {},
   "source": [
    "**Objectif : transformer du texte brut en texte propre et exploitable**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7963c02d",
   "metadata": {},
   "source": [
    "### 2.0. PREALABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec95ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES DE NETTOYAGES\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import preprocessing.text_cleaning as tc\n",
    "importlib.reload(tc)\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# --- ETAPE 0. Pr√©paration des sets de mots √† exclure\n",
    "# -----------------------------------------------------\n",
    "# --- OXFORD TERMS\n",
    "oxford_terms = tc.extract_oxford_terms(\n",
    "    path=\"../src/config/oxford3000.txt\",\n",
    "    export_cleaned_path=\"../src/config/oxford_cleaned.txt\",\n",
    "    verbose=False\n",
    ")\n",
    "print(f\"Nombre de mots dans oxford_cleaned.txt : {len(oxford_terms)}\")\n",
    "# --- CUSTOM TERMS\n",
    "stop_terms_custom = tc.load_stop_terms(\"../src/config/stop_terms.txt\")\n",
    "# stop_terms_custom = tc.custom_stop_terms\n",
    "print(f\"Nombre de mots dans stop_terms.txt : {len(stop_terms_custom)}\")\n",
    "# --- VAGUE TERMS\n",
    "vague_path = tc.generate_vague_terms(path=\"../src/config/vague_terms.txt\", verbose=True)\n",
    "vague_terms = tc.load_stop_terms(vague_path)\n",
    "print(f\"Nombre de mots dans vague_terms.txt : {len(vague_terms)}\")\n",
    "# --- COMBINAISON DE CE QU'ON VEUT EXCLURE\n",
    "stopwords_set = tc.combined_stopwords.union(stop_terms_custom).union(oxford_terms).union(vague_terms)\n",
    "print(f\"Nombre de mots exclus : {len(stopwords_set)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd6189f",
   "metadata": {},
   "source": [
    "### **2.1. APPLICATION SUR UN ECHANTILLON DE 100 QUESTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a298ac2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES DE NETTOYAGES\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import preprocessing.text_cleaning as tc\n",
    "importlib.reload(tc)\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# --- ON CONSTRUIT UN ECHANTILLON DE 100 QUESTIONS\n",
    "# --------------------------------------------------\n",
    "sample_df = full_df.sample(100, random_state=42).copy()\n",
    "\n",
    "# ----------------------------------------\n",
    "# --- NETTOYAGE DES CORPUS TITLE ET BODY\n",
    "# ----------------------------------------\n",
    "sample_clean = sample_df.copy()\n",
    "tqdm.pandas()\n",
    "\n",
    "sample_clean[\"clean_title_body\"] = (\n",
    "    sample_df[\"Title\"].fillna(\"\") + \" \" + sample_df[\"Body\"].fillna(\"\")\n",
    ").apply(lambda x: tc.clean_text_spacy_custom_2(x, stopwords_set=stopwords_set))\n",
    "# -------------------------------------------\n",
    "# --- APERCU DU CORPUS NETTOY√â\n",
    "# -------------------------------------------\n",
    "sample_clean[[\"PostId\", \"clean_title_body\"]].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0472218",
   "metadata": {},
   "source": [
    "### 2.2. APPLICATION SUR 10 K QUESTIONS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29590ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import importlib\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import preprocessing.text_cleaning as tc\n",
    "importlib.reload(tc)\n",
    "\n",
    "brut_df = full_df.sample(10000, random_state=42).copy()\n",
    "\n",
    "\n",
    "# üî∏ PARAM√àTRES\n",
    "chunk_size = 5000\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "output_dir = f\"../data/processed/brut_chunks_{timestamp}\"\n",
    "os.makedirs(output_dir, exist_ok=True)  # üìÅ cr√©ation du dossier si absent\n",
    "\n",
    "# üî∏ CALCUL DU NOMBRE DE CHUNKS\n",
    "n_chunks = len(brut_df) // chunk_size + int(len(brut_df) % chunk_size > 0)\n",
    "\n",
    "# üî∏ TRAITEMENT PAR CHUNK\n",
    "for chunk_index in tqdm(range(n_chunks), desc=\"Nettoyage & sauvegarde des chunks\"):\n",
    "    start = chunk_index * chunk_size\n",
    "    end = start + chunk_size\n",
    "    batch = brut_df.iloc[start:end].copy()\n",
    "\n",
    "    texts = [\n",
    "    f\"{title} {body}\" \n",
    "    for title, body in zip(batch[\"Title\"].fillna(\"\"), batch[\"Body\"].fillna(\"\"))\n",
    "    ]\n",
    "\n",
    "    # texts = (\n",
    "    #     title + \" \" + body\n",
    "    #     for title, body in zip(batch[\"Title\"].fillna(\"\"), batch[\"Body\"].fillna(\"\"))\n",
    "    # )\n",
    "    \n",
    "\n",
    "    docs = tc.nlp.pipe(texts, batch_size=250, disable=[\"parser\", \"ner\"])\n",
    "    batch[\"clean_title_body\"] = [\n",
    "        tc.clean_doc_spacy_custom(doc, stopwords_set=stopwords_set) for doc in docs\n",
    "    ]\n",
    "\n",
    "    # üî∏ SAUVEGARDE DU CHUNK EN .parquet\n",
    "    chunk_path = os.path.join(output_dir, f\"chunk_{chunk_index+1:02}.parquet\")\n",
    "    batch.to_parquet(chunk_path, index=False)\n",
    "    print(f\"üíæ Chunk {chunk_index+1}/{n_chunks} sauvegard√© ‚Üí {chunk_path}\")\n",
    "\n",
    "    # üî∏ NETTOYAGE DE M√âMOIRE\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "import glob\n",
    "\n",
    "files = sorted(glob.glob(os.path.join(output_dir, \"chunk_*.parquet\")))\n",
    "print(f\"\"\"Les chunks sont assembl√©s √† partir des fichiers du r√©pertoire horodat√© :\n",
    "             {output_dir}\"\"\")\n",
    "brut_df_clean = pd.concat([pd.read_parquet(f) for f in files], ignore_index=True)\n",
    "\n",
    "display(brut_df_clean[\"clean_title_body\"].head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07ba749",
   "metadata": {},
   "source": [
    "## **3. EXPLORATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e852c6",
   "metadata": {},
   "source": [
    "### **3.1. APPLICATION SUR UN ECHANTILLON DE 100 QUESTIONS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445c67fc",
   "metadata": {},
   "source": [
    "#### **3.1.1. FREQUENCE DES MOTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830f2c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "# Copie de travail pour toute l‚Äôexploration\n",
    "sample_explo = sample_clean.copy()\n",
    "\n",
    "# -------------------------------\n",
    "# --- ANALYSE FREQUENCE DES MOTS\n",
    "# -------------------------------\n",
    "freq_title_body = eda.compute_word_frequencies(sample_explo, \"clean_title_body\", top_n=100)\n",
    "display(freq_title_body.head())\n",
    "\n",
    "eda.plot_word_frequencies(\n",
    "    freq_title_body,\n",
    "    max_words_display=25,\n",
    "    palette=\"viridis\",\n",
    "    title=\"Top mots dans les questions (titre + corps fusionn√©s)\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f767383d",
   "metadata": {},
   "source": [
    "#### **3.1.2. NUAGE DE MOTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607ded88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "# -------------------\n",
    "# --- NUAGE DE MOTS\n",
    "# -------------------\n",
    "eda.generate_wordcloud(\n",
    "    freq_title_body,\n",
    "     max_words=100,\n",
    "      colormap=\"mako\",\n",
    "      title=\"Nuage de mots - Corpus clean_title_body des questions\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fcb9bc",
   "metadata": {},
   "source": [
    "#### **3.1.3. ANALYSE DE LA LONGUEUR DES DOCUMENTS ET DE LA FORME DE SA DISTRIBUTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da23e805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "# -------------------------------\n",
    "# --- ANALYSE LONGUEUR DU CORPUS FUSIONN√â\n",
    "# -------------------------------\n",
    "sample_explo[\"title_body_length_char\"] = sample_explo[\"clean_title_body\"].str.len()\n",
    "sample_explo[\"title_body_length_words\"] = sample_explo[\"clean_title_body\"].str.split().apply(len)\n",
    "\n",
    "# üß™ Statistiques descriptives\n",
    "display(sample_explo[[\"title_body_length_char\", \"title_body_length_words\"]].describe())\n",
    "\n",
    "# üìä Visualisation\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "eda.plot_distribution(ax[0], sample_explo[\"title_body_length_char\"], \"Longueur (caract√®res)\", \"mediumorchid\")\n",
    "eda.plot_distribution(ax[1], sample_explo[\"title_body_length_words\"], \"Longueur (mots utiles)\", \"goldenrod\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b521bf23",
   "metadata": {},
   "source": [
    "#### **3.1.4. DETECTION DES OUTLIERS ET DOUBLONS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa9c2ac",
   "metadata": {},
   "source": [
    "##### ***3.1.4.1. D√âTECTION DES DOUBLONS EXACTS***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88c580a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "# -------------------------------------\n",
    "# --- D√âTECTION DES DOUBLONS EXACTS\n",
    "# -------------------------------------\n",
    "\n",
    "# Nombre total de questions\n",
    "total_questions = sample_explo.shape[0]\n",
    "\n",
    "# üîç Masque des doublons exacts sur le corpus fusionn√© nettoy√©\n",
    "duplicate_mask = sample_explo.duplicated(subset=[\"clean_title_body\"], keep=False)\n",
    "duplicates_df = sample_explo[duplicate_mask]\n",
    "\n",
    "# üìä Statistiques sur les doublons\n",
    "nb_duplicates = duplicates_df.shape[0]\n",
    "nb_unique_duplicates = duplicates_df.duplicated(subset=[\"clean_title_body\"]).sum()\n",
    "\n",
    "# üì¢ Affichage des r√©sultats\n",
    "print(f\"Nombre total de questions : {total_questions}\")\n",
    "print(f\"Nombre de doublons exacts sur 'clean_title_body' : {nb_duplicates}\")\n",
    "print(f\"Nombre de doublons √† supprimer (copies identiques) : {nb_unique_duplicates}\")\n",
    "print(f\"Proportion de doublons dans l‚Äô√©chantillon : {nb_duplicates / total_questions:.2%}\")\n",
    "\n",
    "# üëÄ Aper√ßu des doublons\n",
    "duplicates_df.sort_values(by=\"clean_title_body\").head(6)[[\"PostId\", \"clean_title_body\"]]\n",
    "\n",
    "# üßπ Suppression des doublons exacts (on garde la premi√®re occurrence)\n",
    "sample_explo = sample_explo.drop_duplicates(subset=[\"clean_title_body\"], keep=\"first\").reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829629de",
   "metadata": {},
   "source": [
    "##### ***3.1.4.2. D√âTECTION DES DOUBLONS FLOUS***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e96260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# --- D√âTECTION DE DOUBLONS FLOUS SUR LE CORPUS NETTOY√â\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "# Cosine TF-IDF\n",
    "fuzzy_cosine = eda.detect_fuzzy_duplicates(sample_explo[\"clean_title_body\"], threshold=0.85)\n",
    "print(f\"# --- Cosine TF-IDF : {len(fuzzy_cosine)} paires similaires d√©tect√©es (threshold ‚â• 0.85)\")\n",
    "\n",
    "# Levenshtein\n",
    "fuzzy_lev = eda.detect_levenshtein_duplicates(sample_explo[\"clean_title_body\"], threshold=90)\n",
    "print(f\"# --- Levenshtein : {len(fuzzy_lev)} paires similaires d√©tect√©es (threshold ‚â• 90)\")\n",
    "\n",
    "# Jaccard\n",
    "fuzzy_jaccard = eda.detect_jaccard_duplicates(sample_explo[\"clean_title_body\"], threshold=0.7)\n",
    "print(f\"# --- Jaccard : {len(fuzzy_jaccard)} paires similaires d√©tect√©es (threshold ‚â• 0.7)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04acff6d",
   "metadata": {},
   "source": [
    "##### ***3.1.4.4. GESTION DES OUTLIERS***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6075b787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "# üìè Cr√©ation des m√©triques de longueur\n",
    "sample_explo[\"title_body_length_char\"] = sample_explo[\"clean_title_body\"].str.len()\n",
    "sample_explo[\"title_body_length_words\"] = sample_explo[\"clean_title_body\"].str.split().apply(len)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# --- DETECTION DES OUTLIERS PAR LA METHODE DES SEUILS IQR\n",
    "# ---------------------------------------------------------\n",
    "sample_explo, sample_outliers = eda.mark_outliers(\n",
    "    df=sample_explo,\n",
    "    columns=[\"title_body_length_words\"],  # ‚Üê colonne cibl√©e\n",
    "    verbose=True,\n",
    "    return_outliers=True\n",
    ")\n",
    "print(f\"# --- NOMBRE OUTLIERS : {sample_outliers.shape[0]}\")\n",
    "# -------------------------------------------\n",
    "# --- VISUALISATION BOXPLOT DE LA DISPERSION\n",
    "# -------------------------------------------\n",
    "eda.plot_boxplots_grid(\n",
    "    df=sample_explo,\n",
    "    columns=[\"title_body_length_words\"],\n",
    "    titles=[\"Longueur des questions nettoy√©es (mots)\"],\n",
    "    colors=[\"goldenrod\"]\n",
    ")\n",
    "\n",
    "# üöø Suppression des outliers\n",
    "sample_explo_wo = eda.remove_outliers(sample_explo)\n",
    "print(f\"# --- NOMBRE DE QUESTIONS RESTANTES APRES SUPPRESSION DES OUTLIERS : {sample_explo_wo.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b5d3a8",
   "metadata": {},
   "source": [
    "#### **3.1.5. ANALYSE DES TAGS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3631ac96",
   "metadata": {},
   "source": [
    "##### 3.1.5.1. Structure des tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37108ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "# -----------------------------\n",
    "# --- APERCU DE LA COLONNE TAG\n",
    "# -----------------------------\n",
    "print(sample_explo_wo[\"Tags\"].head(10))\n",
    "print(sample_explo_wo[\"Tags\"].apply(type).value_counts())\n",
    "# ------------------------------------------\n",
    "# --- CONVERSION DES CHAINES STRING EN LIST\n",
    "# ------------------------------------------\n",
    "sample_explo_wo[\"Tags\"] = sample_explo_wo[\"Tags\"].apply(lambda x: x.split(\";\"))\n",
    "sample_explo_wo[\"Tags\"].apply(type).value_counts()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7443da",
   "metadata": {},
   "source": [
    "##### 3.1.5.2. Fr√©quence des tags  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c46801c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "\n",
    "\n",
    "# --- APLATIR TOUTES LES LISTES DE TAGS\n",
    "all_tags = [tag for tags in sample_explo_wo[\"Tags\"] for tag in tags]\n",
    "\n",
    "# --- COMPTER LES OCCURRENCES\n",
    "tag_counts = Counter(all_tags)\n",
    "\n",
    "# --- CONVERSION PANDAS POUR VISUALISER\n",
    "tag_freq_df = pd.DataFrame(tag_counts.items(), columns=[\"Tag\", \"Count\"]).sort_values(by=\"Count\", ascending=False)\n",
    "tag_freq_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# --- LES 10 TAGS LES PLUS FREQUENTS\n",
    "tag_freq_df.head(10)\n",
    "\n",
    "# --- VISUSALISATION DES TACGS LES PLUS FREQUENTS\n",
    "eda.plot_tag_occurrences(tag_freq_df, top_n=10)\n",
    "eda.plot_tag_distribution(tag_freq_df, top_n=10)\n",
    "\n",
    "# --- TAGS QUI SONT RARES\n",
    "rare_tags = tag_freq_df[tag_freq_df[\"Count\"] == 1]\n",
    "print(f\"Nombre de tags apparaissant une seule fois : {len(rare_tags)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897e5848",
   "metadata": {},
   "source": [
    "##### 3.1.5.3. Diversit√© des tags  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d272e1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "# --- NOMBRE DE TAGS DISTINCTS\n",
    "nb_tags_uniques = tag_freq_df[\"Tag\"].nunique()\n",
    "print(f\"Nombre de tags distincts dans l‚Äô√©chantillon : {nb_tags_uniques}\")\n",
    "\n",
    "# --- CALCUL DE LA PROPORTION CUMULEE\n",
    "tag_freq_df[\"Proportion\"] = tag_freq_df[\"Count\"] / tag_freq_df[\"Count\"].sum()\n",
    "tag_freq_df[\"Cumulative\"] = tag_freq_df[\"Proportion\"].cumsum()\n",
    "\n",
    "# --- VISUALISATION  : COURBE DE COUVERTURE CUMULATIVE\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.lineplot(data=tag_freq_df, x=range(1, len(tag_freq_df)+1), y=\"Cumulative\", marker=\"o\")\n",
    "plt.axhline(0.8, color=\"red\", linestyle=\"--\", label=\"80% de couverture\")\n",
    "plt.title(\"Courbe de couverture cumulative des tags\")\n",
    "plt.xlabel(\"Nombre de tags (class√©s par fr√©quence d√©croissante)\")\n",
    "plt.ylabel(\"Proportion cumul√©e\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- NOMBRE DE TAGS NECESSAIRE POUR COUVRIR 80% DU CORPUS\n",
    "nb_tags_80 = (tag_freq_df[\"Cumulative\"] <= 0.8).sum()\n",
    "print(f\"Nombre de tags n√©cessaires pour couvrir 80% des occurrences : {nb_tags_80}\")\n",
    "\n",
    "# ----------------------------------\n",
    "# --- CALCUL DE L'ENTROPIE DES TAGS\n",
    "# ----------------------------------\n",
    "# --- DISTRIBUTION DES PROPORTIONS\n",
    "proportions = tag_freq_df[\"Proportion\"].values\n",
    "\n",
    "# --- ENTROPIE DE SHANNON (BASE 2)\n",
    "tag_entropy = entropy(proportions, base=2)\n",
    "print(f\"Entropie de la distribution des tags : {tag_entropy:.4f} bits\")\n",
    "\n",
    "# --------------------------------\n",
    "# --- ENTROPIE MAXIMALE THEORIQUE\n",
    "# --------------------------------\n",
    "max_entropy = np.log2(len(proportions))\n",
    "print(f\"Entropie maximale possible : {max_entropy:.4f} bits\")\n",
    "print(f\"Taux de diversit√© relative : {tag_entropy / max_entropy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c493355a",
   "metadata": {},
   "source": [
    "##### 3.1.5.4. Marquage des tags dominants  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39ffadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "# --------------------------------\n",
    "# --- MARQUAGE DES TAGS DOMINANTS\n",
    "# --------------------------------\n",
    "tag_freq_df_marked, dominant_tags = eda.mark_dominant_tags_by_frequency_and_coverage(\n",
    "    tag_freq_df,\n",
    "    nb_questions=100,  # ou 50000 plus tard\n",
    "    min_coverage=0.8,\n",
    "    min_count_floor=3\n",
    ")\n",
    "\n",
    "display(dominant_tags)\n",
    "display(tag_freq_df_marked)\n",
    "\n",
    "eda.plot_tag_coverage(tag_freq_df_marked, coverage_target=0.8, title_suffix=\"√©chantillon de 100 questions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6980406e",
   "metadata": {},
   "source": [
    "#### **3.1.6. CO-OCCURRENCE DES TAGS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7272556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "cooc_matrix, tag_matrix = eda.build_tag_cooccurrence_matrix(\n",
    "    df_questions=sample_explo_wo,  # ou le nom de ton DataFrame\n",
    "    dominant_tags=dominant_tags,\n",
    "    tag_col=\"Tags\"\n",
    ")\n",
    "\n",
    "cooc_matrix.style.background_gradient(cmap=\"Blues\")\n",
    "\n",
    "eda.plot_tag_cooccurrence_heatmap(cooc_matrix)\n",
    "\n",
    "eda.plot_tag_cooccurrence_graph(cooc_matrix, min_edge_weight=2, layout=\"spring\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2852b0a9",
   "metadata": {},
   "source": [
    "### 3.2. APPLICATION SUR 10 K QUESTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa7945e",
   "metadata": {},
   "source": [
    "#### **3.2.1. FREQUENCE DES MOTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929e8c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "# ‚úÖ Standardisation du nom du corpus complet\n",
    "full_explo = brut_df_clean.copy()\n",
    "\n",
    "\n",
    "# üîç Calcul des fr√©quences (top 100 lemmes sans les stopwords personnalis√©s)\n",
    "freq_clean_title_body = eda.compute_word_frequencies(\n",
    "    df=full_explo,\n",
    "    column=\"clean_title_body\",\n",
    "    top_n=100  # tu peux augmenter selon besoin\n",
    ")\n",
    "display(freq_clean_title_body)\n",
    "# üëÅÔ∏è Visualisation des top tokens\n",
    "eda.plot_word_frequencies(\n",
    "    df_freq=freq_clean_title_body,\n",
    "    max_words_display=30,\n",
    "    palette=\"viridis\",\n",
    "    title=\"Top mots dans clean_title_body (10 000 questions)\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41f41e0",
   "metadata": {},
   "source": [
    "#### **3.2.2. NUAGE DE MOTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324f182a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "eda.generate_wordcloud(\n",
    "    df_freq=freq_clean_title_body,\n",
    "    max_words=100,\n",
    "    colormap=\"plasma\",\n",
    "    title=\"Nuage de mots - clean_title_body (Corpus complet)\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceaaa86",
   "metadata": {},
   "source": [
    "#### **3.2.3. ANALYSE DE LA LONGUEUR DES DOCUMENTS ET DE LA FORME DE SA DISTRIBUTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db37e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "display(full_explo.columns)\n",
    "# üîπ Reconstruction du texte brut fusionn√©\n",
    "full_explo[\"title_body_raw\"] = full_explo[\"Title\"].fillna(\"\") + \" \" + full_explo[\"Body\"].fillna(\"\")\n",
    "\n",
    "# üîπ Calcul des longueurs en mots\n",
    "full_explo[\"length_words_raw\"] = full_explo[\"title_body_raw\"].str.split().apply(len)\n",
    "full_explo[\"length_words_clean\"] = full_explo[\"clean_title_body\"].str.split().apply(len)\n",
    "\n",
    "# üîπ Ratio de r√©duction lexical\n",
    "full_explo[\"reduction_ratio_global\"] = full_explo[\"length_words_clean\"] / full_explo[\"length_words_raw\"]\n",
    "\n",
    "import eda.eda_analysis as eda\n",
    "\n",
    "eda.plot_distribution(\n",
    "    ax=plt.gca(),\n",
    "    data=full_explo[\"reduction_ratio_global\"],\n",
    "    title=\"Ratio de r√©duction lexical global (nettoy√© vs brut)\",\n",
    "    color=\"mediumseagreen\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac2a5b7",
   "metadata": {},
   "source": [
    "#### **3.2.4. DETECTION DES OUTLIERS ET DOUBLONS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d78d43",
   "metadata": {},
   "source": [
    "##### ***3.2.4.1. D√âTECTION DES DOUBLONS EXACTS***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6585ffca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "# üîç D√©tection des doublons exacts sur le champ nettoy√©\n",
    "exact_dup_mask = full_explo.duplicated(subset=[\"clean_title_body\"], keep=False)\n",
    "exact_duplicates_df = full_explo[exact_dup_mask]\n",
    "\n",
    "# üìä Statistiques\n",
    "nb_duplicates = exact_duplicates_df.shape[0]\n",
    "nb_unique = exact_duplicates_df.duplicated(subset=[\"clean_title_body\"]).sum()\n",
    "total = full_explo.shape[0]\n",
    "\n",
    "print(f\"Nombre total de questions : {total}\")\n",
    "print(f\"Nombre de doublons exacts (clean_title_body) : {nb_duplicates}\")\n",
    "print(f\"Nombre de doublons √† supprimer (copies identiques) : {nb_unique}\")\n",
    "print(f\"Proportion de doublons dans le corpus : {nb_duplicates / total:.2%}\")\n",
    "\n",
    "# üí¨ Affichage de quelques doublons\n",
    "exact_duplicates_df.sort_values(\"clean_title_body\").head(6)[[\"PostId\", \"clean_title_body\"]]\n",
    "\n",
    "# üßπ Suppression des doublons exacts (on garde la premi√®re occurrence)\n",
    "full_explo = full_explo.drop_duplicates(subset=[\"clean_title_body\"], keep=\"first\").reset_index(drop=True)\n",
    "display(exact_duplicates_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee05caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bc26db",
   "metadata": {},
   "source": [
    "##### ***3.2.4.3. GESTION DES OUTLIERS***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bfd38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "\n",
    "full_explo, full_outliers = eda.mark_outliers(\n",
    "    df=full_explo,\n",
    "    columns=[\n",
    "        \"length_words_clean\",\n",
    "        \"reduction_ratio_global\"\n",
    "    ],\n",
    "    verbose=True,\n",
    "    return_outliers=True\n",
    ")\n",
    "\n",
    "# eda.plot_boxplots_grid(\n",
    "#     df=full_explo,\n",
    "#     columns=[\n",
    "#         \"length_words_clean\",\n",
    "#         \"reduction_ratio_global\"\n",
    "#     ],\n",
    "#     titles=[\n",
    "#         \"Longueur nettoy√©e - titre + body\",\n",
    "#         \"Ratio de r√©duction lexical\"\n",
    "#     ],\n",
    "#     colors=[\"steelblue\", \"mediumseagreen\"]\n",
    "# )\n",
    "full_explo_wo = eda.remove_outliers(full_explo)\n",
    "print(f\"# ‚úÖ Questions restantes apr√®s suppression des outliers : {full_explo_wo.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ea8b60",
   "metadata": {},
   "source": [
    "#### **3.2.5. ANALYSE DES TAGS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0f5ed2",
   "metadata": {},
   "source": [
    "##### 3.2.5.1. Structure des tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe7bd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "# --- APERCU DE LA COLONNE TAGS\n",
    "print(full_explo_wo[\"Tags\"].head(10))\n",
    "print(full_explo_wo[\"Tags\"].apply(type).value_counts())\n",
    "# --- CONVERSION DES CHAINES EN LISTES\n",
    "full_explo_wo[\"Tags\"] = full_explo_wo[\"Tags\"].apply(lambda x: x.split(\";\") if isinstance(x, str) else [])\n",
    "# --- VERIFICATION POST CONVERSION\n",
    "print(full_explo_wo[\"Tags\"].apply(type).value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03993b6",
   "metadata": {},
   "source": [
    "##### 3.2.5.2. Fr√©quence des tags  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48d5a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "# --- ON APLATIT LA LISTE DES TAGS\n",
    "from collections import Counter\n",
    "\n",
    "all_tags = [tag for tags in full_explo_wo[\"Tags\"] for tag in tags]\n",
    "\n",
    "# --- ON COMPTE LES OCCURRENCES\n",
    "tag_counts = Counter(all_tags)\n",
    "# --- CONVERSION EN PANDAS DATAFRAME POUR VISUALISATION\n",
    "tag_freq_df = pd.DataFrame(tag_counts.items(), columns=[\"Tag\", \"Count\"]).sort_values(by=\"Count\", ascending=False)\n",
    "tag_freq_df.reset_index(drop=True, inplace=True)\n",
    "# ---AFFICHAGE DES TAGS LES PLUS FREQUENTS\n",
    "tag_freq_df.head(10)\n",
    "# --- VISUALISATION\n",
    "eda.plot_tag_occurrences(tag_freq_df, top_n=10)\n",
    "eda.plot_tag_distribution(tag_freq_df, top_n=10)\n",
    "# --- EXTRACTION DES TAGS RARES\n",
    "rare_tags = tag_freq_df[tag_freq_df[\"Count\"] == 1]\n",
    "print(f\"Nombre de tags apparaissant une seule fois : {len(rare_tags)}\")\n",
    "# --- DISTRIBUTION DES TAGS PAR FREQUENCE\n",
    "tag_freq_df[\"log_count\"] = np.log1p(tag_freq_df[\"Count\"])\n",
    "sns.histplot(tag_freq_df[\"log_count\"], bins=30, color=\"indigo\")\n",
    "plt.title(\"Distribution log-transform√©e des fr√©quences de tags\")\n",
    "plt.xlabel(\"log(1 + Count)\")\n",
    "plt.ylabel(\"Nombre de tags\")\n",
    "plt.show()\n",
    "\n",
    "# --- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c9748d",
   "metadata": {},
   "source": [
    "##### 3.2.5.3. Diversit√© des tags  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba199ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "# --- NOMBRE DE TAGS UNIQUES\n",
    "nb_tags_uniques = tag_freq_df[\"Tag\"].nunique()\n",
    "print(f\"Nombre de tags distincts dans le corpus : {nb_tags_uniques}\")\n",
    "# --- CALCUL DE LA PROPORTION CUMULEE\n",
    "tag_freq_df[\"Proportion\"] = tag_freq_df[\"Count\"] / tag_freq_df[\"Count\"].sum()\n",
    "tag_freq_df[\"Cumulative\"] = tag_freq_df[\"Proportion\"].cumsum()\n",
    "# --- VISUALISATION DE COUVERTURE CUMULATIVE\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.lineplot(\n",
    "    data=tag_freq_df,\n",
    "    x=range(1, len(tag_freq_df) + 1),\n",
    "    y=\"Cumulative\",\n",
    "    marker=\"o\"\n",
    ")\n",
    "plt.axhline(0.8, color=\"red\", linestyle=\"--\", label=\"80% de couverture\")\n",
    "plt.title(\"Courbe de couverture cumulative des tags\")\n",
    "plt.xlabel(\"Nombre de tags (class√©s par fr√©quence d√©croissante)\")\n",
    "plt.ylabel(\"Proportion cumul√©e\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- NOMBRE DE TAGS NECESSAIRES POUR COUVRIR 80% DES OCCURRENCES\n",
    "nb_tags_80 = (tag_freq_df[\"Cumulative\"] <= 0.8).sum()\n",
    "print(f\"Nombre de tags n√©cessaires pour couvrir 80‚ÄØ% des occurrences : {nb_tags_80}\")\n",
    "# --- ENTROPIE DE LA DISTRIBUTION DES TAGS (SHANNON BASE 2)\n",
    "from scipy.stats import entropy\n",
    "\n",
    "proportions = tag_freq_df[\"Proportion\"].values\n",
    "tag_entropy = entropy(proportions, base=2)\n",
    "print(f\"Entropie de la distribution des tags : {tag_entropy:.4f} bits\")\n",
    "\n",
    "max_entropy = np.log2(len(proportions))\n",
    "print(f\"Entropie maximale possible : {max_entropy:.4f} bits\")\n",
    "print(f\"Taux de diversit√© relative : {tag_entropy / max_entropy:.2%}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4632e831",
   "metadata": {},
   "source": [
    "##### 3.2.5.4. Marquage des tags dominants  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1e8afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "\n",
    "# ‚úÖ Marquage des tags dominants sur le corpus complet\n",
    "tag_freq_df_marked, dominant_tags = eda.mark_dominant_tags_by_frequency_and_coverage(\n",
    "    tag_freq_df=tag_freq_df,\n",
    "    nb_questions=full_explo_wo.shape[0],  # 50‚ÄØ000 ou autre taille r√©elle\n",
    "    min_coverage=0.8,\n",
    "    min_count_floor=3\n",
    ")\n",
    "\n",
    "# üëÅÔ∏è Affichage des tags dominants\n",
    "display(dominant_tags)\n",
    "display(tag_freq_df_marked)\n",
    "\n",
    "# üìä Visualisation de la couverture cumulative\n",
    "eda.plot_tag_coverage(\n",
    "    tag_freq_df_marked,\n",
    "    coverage_target=0.8,\n",
    "    title_suffix=\"corpus de 50‚ÄØ000 questions\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506bd157",
   "metadata": {},
   "source": [
    "#### **3.2.6. CO-OCCURRENCE DES TAGS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539c9def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "# # --- CONSTRUCTION DES MATRICES\n",
    "# cooc_matrix, tag_matrix = eda.build_tag_cooccurrence_matrix(\n",
    "#     df_questions=full_explo_wo,\n",
    "#     dominant_tags=dominant_tags,\n",
    "#     tag_col=\"Tags\"\n",
    "# )\n",
    "# # --- HEATMAP DES CO OCCURRENCES\n",
    "# cooc_matrix.style.background_gradient(cmap=\"Blues\")\n",
    "\n",
    "# eda.plot_tag_cooccurrence_heatmap(cooc_matrix)\n",
    "# # --- GRAPHE DE CO OCCURRENCE VISUEL\n",
    "# eda.plot_tag_cooccurrence_graph(\n",
    "#     cooc_matrix,\n",
    "#     min_edge_weight=2,           # pour √©viter les ar√™tes anecdotiques\n",
    "#     layout=\"spring\"              # layout fluide et lisible\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f144e91",
   "metadata": {},
   "source": [
    "## **4. FEATURE ENGINEERING**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e73ba5",
   "metadata": {},
   "source": [
    "### **4.1. APPLICATION SUR UN ECHANTILLON DE 100 QUESTIONS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ad831e",
   "metadata": {},
   "source": [
    "#### **4.1.1. VECTORISATION TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1f0715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "# --------------------------------------\n",
    "# --- VECTORISATION DU CORPUS FUSIONN√â\n",
    "# --------------------------------------\n",
    "X_titlebody_tfidf, titlebody_vocab, top_titlebody_words = eda.vectorize_tfidf(\n",
    "    sample_explo_wo[\"clean_title_body\"],\n",
    "    label=\"title + body\"\n",
    ")\n",
    "\n",
    "# üìä Aper√ßu des termes les plus fr√©quents\n",
    "top_titlebody_words_df = pd.DataFrame(top_titlebody_words, columns=[\"word\", \"score\"])\n",
    "display(top_titlebody_words_df.head(10))\n",
    "\n",
    "# üîç Shape du matrice TF-IDF\n",
    "print(f\"Dimensions du X_titlebody_tfidf : {X_titlebody_tfidf.shape}\")\n",
    "from scipy.sparse import save_npz\n",
    "save_npz(\"models/tfidf/X_tfidf_sample.npz\", X_titlebody_tfidf)\n",
    "import pickle\n",
    "with open(\"models/tfidf/vocab_tfidf_sample.pkl\", \"wb\") as f:\n",
    "    pickle.dump(titlebody_vocab, f)\n",
    "with open(\"models/tfidf/corpus_sample.pkl\", \"wb\") as f:\n",
    "    pickle.dump(sample_explo_wo[\"clean_title_body\"].tolist(), f)\n",
    "\n",
    "with open(\"models/tfidf/top_titlebody_words.pkl\", \"wb\") as f:\n",
    "    pickle.dump(top_titlebody_words, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa56f268",
   "metadata": {},
   "source": [
    "#### **4.1.2. REDUCTION DE DIMENSION (SVD, PCA)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e236974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "# ----------------------------------------------\n",
    "# --- REDUCTION DIMENSIONS SVD DU CORPUS FUSIONN√â\n",
    "# ----------------------------------------------\n",
    "# --- Reduction en 2D pour visualisation\n",
    "X_titlebody_svd, svd_titlebody = eda.apply_svd_and_plot(X_titlebody_tfidf, label=\"title + body\")\n",
    "\n",
    "\n",
    "# --- R√©duction √† n dimensions avec suivi de la variance cumul√©e\n",
    "X_titlebody_svd100, svd_titlebody100 = eda.apply_svd_variance(X_titlebody_tfidf, label=\"title + body\", n_components=50)\n",
    "np.save(\"models/svd/X_titlebody_svd_100.npy\", X_titlebody_svd100)\n",
    "import pickle\n",
    "with open(\"models/svd/svd_model_100.pkl\", \"wb\") as f:\n",
    "    pickle.dump(svd_titlebody100, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8079efd1",
   "metadata": {},
   "source": [
    "#### **4.1.3. EMBEDDINGS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bb177e",
   "metadata": {},
   "source": [
    "##### 4.1.3.1. METHODE SBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efe5e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "corpus_titlebody = sample_explo_wo[\"clean_title_body\"].fillna(\"\").tolist()\n",
    "embeddings_sbert = eda.encode_sbert_corpus(corpus_titlebody, batch_size=32)\n",
    "print(f\"‚úÖ SBERT Embeddings shape : {embeddings_sbert.shape}\")\n",
    "np.save(\"models/sbert/embeddings_sbert.npy\", embeddings_sbert)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cc806e",
   "metadata": {},
   "source": [
    "##### 4.1.3.2. METHODE WORD2VEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa10821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "corpus_titlebody = sample_explo_wo[\"clean_title_body\"].fillna(\"\").tolist()\n",
    "\n",
    "model_w2v = eda.train_word2vec(\n",
    "    corpus=corpus_titlebody,\n",
    "    save_path=\"models/word2vec/word2vec_titlebody.bin\",  # dossier √† cr√©er si n√©cessaire\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=3,\n",
    "    epochs=10,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# üîÅ Vectorisation du corpus complet\n",
    "w2v_model_sample = Word2Vec.load(\"models/word2vec/word2vec_titlebody.bin\")\n",
    "corpus_sample = sample_explo_wo[\"clean_title_body\"].fillna(\"\").tolist()\n",
    "X_w2v_sample = eda.vectorize_texts(corpus_sample, w2v_model_sample)\n",
    "\n",
    "# üíæ Sauvegarde\n",
    "np.save(\"models/word2vec/X_w2v_sample.npy\", X_w2v_sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683e4908",
   "metadata": {},
   "source": [
    "##### 4.1.3.3. METHODE UNIVERSAL SENTENCE ENCODER (USE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a44e6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "\n",
    "corpus_titlebody = sample_explo_wo[\"clean_title_body\"].fillna(\"\").tolist()\n",
    "\n",
    "embeddings_use = eda.encode_use_corpus(corpus_titlebody, batch_size=100)\n",
    "print(f\"‚úÖ USE Embeddings shape : {embeddings_use.shape}\")\n",
    "np.save(\"models/use_model/embeddings_use.npy\", embeddings_use)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f914661",
   "metadata": {},
   "source": [
    "#### **4.1.4. VECTORISATION BoW (CountVectorizer)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2801ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "corpus_bow, X_bow, vocab_bow = eda.build_bow_matrix(\n",
    "    df=sample_explo_wo,\n",
    "    col_title=\"clean_title_body\",  # ‚úÖ corpus fusionn√©\n",
    "    col_body=\"clean_title_body\",  # ‚úÖ encore le m√™me\n",
    "    max_df=0.9,\n",
    "    min_df=5,\n",
    "    stop_words=\"english\"\n",
    ")\n",
    "\n",
    "print(\"# --- Matrice BoW:\", X_bow.shape)\n",
    "print(\"# --- Taille du vocabulaire:\", len(vocab_bow))\n",
    "import scipy.sparse\n",
    "scipy.sparse.save_npz(\"models/bow/X_bow.npz\", X_bow)\n",
    "import pickle\n",
    "with open(\"models/bow/corpus_bow.pkl\", \"wb\") as f:\n",
    "    pickle.dump(corpus_bow, f)\n",
    "with open(\"models/bow/vocab_bow.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vocab_bow, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adef1b24",
   "metadata": {},
   "source": [
    "#### **4.1.5. ENCODAGE DES TAGS (MULTI-LABEL BINARIZER)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c04f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# --- VERIFICATION ET CONVERSION DES TAGS SI NECESSAIRE\n",
    "# ------------------------------------------------------\n",
    "if isinstance(sample_explo_wo[\"Tags\"].iloc[0], str):\n",
    "    sample_explo_wo[\"Tags\"] = sample_explo_wo[\"Tags\"].apply(ast.literal_eval)\n",
    "# -------------------------\n",
    "# --- ENCODAGE MULTI-LABEL\n",
    "# -------------------------\n",
    "mlb = MultiLabelBinarizer()\n",
    "tags_encoded = mlb.fit_transform(sample_explo_wo[\"Tags\"])\n",
    "# ---------------------------------\n",
    "# --- CREATION DU DATAFRAME ENCODE\n",
    "# ----------------------------------\n",
    "tags_df = pd.DataFrame(tags_encoded, columns=mlb.classes_)\n",
    "print(\"# --- ON AFFICHE LES TAGS ENCODES\")\n",
    "display(tags_df)\n",
    "# --------------------------------------------\n",
    "# --- FUSION AVEC LE DATAFRAME sample_explo_wo\n",
    "# ---------------------------------------------\n",
    "sample_explo_wo = pd.concat([sample_explo_wo, tags_df], axis=1)\n",
    "print(\"# --- ON AFFICHE LA FUSION DES TAGS ENCODES AVEC LE DATAFRAME sample_explo_wo : \")\n",
    "display(sample_explo_wo)\n",
    "# ----------------------------\n",
    "# --- SAUVEGARDE DU BINARIZER\n",
    "# -----------------------------\n",
    "import os\n",
    "import joblib\n",
    "# üîê Cr√©ation du dossier standardis√©\n",
    "os.makedirs(\"models/tags\", exist_ok=True)\n",
    "# üíæ Sauvegarde du binarizer\n",
    "joblib.dump(mlb, \"models/tags/multilabel_binarizer_sample.pkl\")\n",
    "np.save(\"models/tags/y_tags.npy\", tags_encoded)\n",
    "\n",
    "\n",
    "\n",
    "# --- VISUALISATION \n",
    "tag_counts = tags_df.sum().sort_values(ascending=False)\n",
    "tag_freq_df = tag_counts.reset_index()\n",
    "tag_freq_df.columns = [\"Tag\", \"Count\"]\n",
    "eda.plot_tag_occurrences(tag_freq_df, top_n=15, palette=\"mako\")\n",
    "eda.plot_tag_distribution(tag_freq_df, top_n=15, palette=\"mako\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13deee3e",
   "metadata": {},
   "source": [
    "### 4.2. APPLICATION SUR 50 K QUESTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e100aa67",
   "metadata": {},
   "source": [
    "#### **4.2.1. VECTORISATION TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac36b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "X_titlebody_tfidf_full, titlebody_vocab_full, top_titlebody_words_full = eda.vectorize_tfidf(\n",
    "    full_explo_wo[\"clean_title_body\"],\n",
    "    label=\"title + body\",\n",
    "    max_features=10000,\n",
    "    min_df=5,\n",
    "    top_n=20,\n",
    "    show_wordcloud=False  # D√©sactiv√© si tu veux pr√©server la RAM\n",
    ")\n",
    "from scipy.sparse import save_npz\n",
    "import pickle\n",
    "\n",
    "# üîí Sauvegarde de la matrice TF-IDF\n",
    "save_npz(\"models/tfidf/X_tfidf_full.npz\", X_titlebody_tfidf_full)\n",
    "\n",
    "# üì¶ Sauvegarde du vocabulaire\n",
    "with open(\"models/tfidf/vocab_tfidf_full.pkl\", \"wb\") as f:\n",
    "    pickle.dump(titlebody_vocab_full, f)\n",
    "\n",
    "with open(\"models/tfidf/top_titlebody_words_full.pkl\", \"wb\") as f:\n",
    "    pickle.dump(top_titlebody_words_full, f)\n",
    "\n",
    "with open(\"models/tfidf/corpus_full.pkl\", \"wb\") as f:\n",
    "    pickle.dump(full_explo_wo[\"clean_title_body\"].tolist(), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d21cd8",
   "metadata": {},
   "source": [
    "#### **4.2.2. REDUCTION DE DIMENSION (SVD, PCA)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb11439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "from scipy.sparse import load_npz\n",
    "\n",
    "X_titlebody_tfidf_full = load_npz(\"models/tfidf/X_tfidf_full.npz\")\n",
    "\n",
    "# --- REDUCTION SVD POUR VISUALISATION 2D\n",
    "X_titlebody_svd_full, svd_titlebody_full = eda.apply_svd_and_plot(\n",
    "    X_titlebody_tfidf_full,\n",
    "    label=\"title + body\",\n",
    "    n_components=2\n",
    ")\n",
    "# --- REDUCTION SVD POUR EXTRACTION DES FEATURES ET SAUVEGARDE EN .npy\n",
    "X_titlebody_svd10k, svd_titlebody10k = eda.apply_svd_variance(\n",
    "    X_titlebody_tfidf_full,\n",
    "    label=\"title + body\",\n",
    "    n_components=50\n",
    ")\n",
    "import numpy as np\n",
    "np.save(\"models/svd/X_titlebody_svd10k.npy\", X_titlebody_svd10k)\n",
    "\n",
    "import pickle\n",
    "with open(\"models/svd/svd_model_10k.pkl\", \"wb\") as f:\n",
    "    pickle.dump(svd_titlebody10k, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fb944c",
   "metadata": {},
   "source": [
    "#### **4.2.3. EMBEDDINGS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2e28a7",
   "metadata": {},
   "source": [
    "##### 4.2.3.1. METHODE SBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d8f698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "corpus_titlebody_full = full_explo_wo[\"clean_title_body\"].fillna(\"\").tolist()\n",
    "embeddings_sbert_full = eda.encode_sbert_corpus(corpus_titlebody_full, batch_size=32)\n",
    "print(f\"‚úÖ SBERT Embeddings shape : {embeddings_sbert_full.shape}\")\n",
    "np.save(\"models/sbert/embeddings_sbert_full.npy\", embeddings_sbert_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe22d10",
   "metadata": {},
   "source": [
    "##### 4.2.3.2. METHODE WORD2VEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28fa312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "corpus_titlebody_full = full_explo_wo[\"clean_title_body\"].fillna(\"\").tolist()\n",
    "\n",
    "model_w2v_full = eda.train_word2vec(\n",
    "    corpus=corpus_titlebody_full,\n",
    "    save_path=\"models/word2vec/word2vec_titlebody_full.bin\",  # dossier √† cr√©er si n√©cessaire\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=3,\n",
    "    epochs=10,\n",
    "    verbose=True\n",
    ")\n",
    "# üîÅ Vectorisation du corpus complet\n",
    "w2v_model_full = Word2Vec.load(\"models/word2vec/word2vec_titlebody_full.bin\")\n",
    "# corpus_full = full_df[\"clean_title_body\"].fillna(\"\").tolist()\n",
    "X_w2v_full = eda.vectorize_texts(corpus_titlebody_full, w2v_model_full)\n",
    "\n",
    "# üíæ Sauvegarde\n",
    "np.save(\"models/word2vec/X_w2v_full.npy\", X_w2v_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2d466c",
   "metadata": {},
   "source": [
    "##### 4.2.3.3. METHODE UNIVERSAL SENTENCE ENCODER (USE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ff8cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "\n",
    "corpus_titlebody_full = full_explo_wo[\"clean_title_body\"].fillna(\"\").tolist()\n",
    "\n",
    "embeddings_use_full = eda.encode_use_corpus(corpus_titlebody_full, batch_size=100)\n",
    "print(f\"‚úÖ USE Embeddings shape : {embeddings_use_full.shape}\")\n",
    "np.save(\"models/use_model/embeddings_use_full.npy\", embeddings_use_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678674ce",
   "metadata": {},
   "source": [
    "#### **4.2.4. VECTORISATION BoW (CountVectorizer)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caee444c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "corpus_bow_full, X_bow_full, vocab_bow_full = eda.build_bow_matrix(\n",
    "    df=full_explo_wo,\n",
    "    col_title=\"clean_title_body\",  # ‚úÖ corpus fusionn√©\n",
    "    col_body=\"clean_title_body\",  # ‚úÖ encore le m√™me\n",
    "    max_df=0.9,\n",
    "    min_df=5,\n",
    "    stop_words=\"english\"\n",
    ")\n",
    "\n",
    "print(\"# --- Matrice BoW:\", X_bow_full.shape)\n",
    "print(\"# --- Taille du vocabulaire:\", len(vocab_bow_full))\n",
    "import scipy.sparse\n",
    "scipy.sparse.save_npz(\"models/bow/X_bow_full.npz\", X_bow_full)\n",
    "import pickle\n",
    "with open(\"models/bow/corpus_bow_full.pkl\", \"wb\") as f:\n",
    "    pickle.dump(corpus_bow_full, f)\n",
    "with open(\"models/bow/vocab_bow_full.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vocab_bow_full, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c45be3d",
   "metadata": {},
   "source": [
    "#### **4.2.5. ENCODAGE DES TAGS (MULTI-LABEL BINARIZER)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed51926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import joblib\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --------------------------\n",
    "# üîç V√©rification des tags\n",
    "# --------------------------\n",
    "if isinstance(full_explo_wo[\"Tags\"].iloc[0], str):\n",
    "    full_explo_wo[\"Tags\"] = full_explo_wo[\"Tags\"].apply(ast.literal_eval)\n",
    "\n",
    "# ---------------------------\n",
    "# üß† Encodage multilabel\n",
    "# ---------------------------\n",
    "mlb_full = MultiLabelBinarizer()\n",
    "tags_encoded_full = mlb_full.fit_transform(full_explo_wo[\"Tags\"])\n",
    "\n",
    "# ‚úÖ DataFrame des tags encod√©s\n",
    "tags_df_full = pd.DataFrame(tags_encoded_full, columns=mlb_full.classes_)\n",
    "print(f\"Shape du matrix multi-label : {tags_encoded_full.shape}\")\n",
    "display(tags_df_full.head())\n",
    "\n",
    "# -----------------------------------\n",
    "# üîó Fusion avec full_explo_wo\n",
    "# -----------------------------------\n",
    "full_explo_wo = pd.concat([full_explo_wo, tags_df_full], axis=1)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# üíæ Sauvegarde du binarizer + vecteur encod√©\n",
    "# ---------------------------------------------\n",
    "os.makedirs(\"models/tags\", exist_ok=True)\n",
    "np.save(\"models/tags/y_tags_full.npy\", tags_encoded_full)\n",
    "joblib.dump(mlb_full, \"models/tags/multilabel_binarizer_full.pkl\")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# üìä Visualisation des tags les plus fr√©quents\n",
    "# ---------------------------------------------\n",
    "tag_counts = tags_df_full.sum().sort_values(ascending=False)\n",
    "tag_freq_df = tag_counts.reset_index()\n",
    "tag_freq_df.columns = [\"Tag\", \"Count\"]\n",
    "\n",
    "# --- Barplot (Top 15 Tags)\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(data=tag_freq_df.head(15), x=\"Count\", y=\"Tag\", palette=\"mako\")\n",
    "plt.title(\"Top 15 des tags les plus fr√©quents\")\n",
    "plt.xlabel(\"Nombre d‚Äôoccurrences\")\n",
    "plt.ylabel(\"Tag\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d47dd0",
   "metadata": {},
   "source": [
    "## **5. EXPORT**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de382bbd",
   "metadata": {},
   "source": [
    "### 5.1. APPLICATION SUR UN ECHANTILLON DE 100 QUESTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d48c85",
   "metadata": {},
   "source": [
    "Les artefacts `corpus`, `X_bow` et `vocab` ayant √©t√© cr√©√©s dans la section pr√©c√©dente d√©di√©e √† la vectorisation Bag-of-Words, ils sont ici uniquement export√©s pour √™tre utilis√©s dans le notebook `3_modele_non_supervise_lda.ipynb` (mod√©lisation LDA).\n",
    "\n",
    "Aucun recalcul n‚Äôest effectu√© ici afin de garantir la modularit√© du pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fdac7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES D' EXPLORATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import eda.eda_analysis as eda\n",
    "importlib.reload(eda)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Dimensions du dataframe sample_df : {sample_explo_wo.shape}\")\n",
    "print(f\" Colonnes du dataframe sample_df : {sample_explo_wo.columns.tolist()}\")\n",
    "print(f\"Dimensions du dataframe full_df : {full_explo_wo.shape}\")\n",
    "print(f\" Colonnes du dataframe full_df : {full_explo_wo.columns.tolist()}\")\n",
    "\n",
    "# ---------------------------------\n",
    "# --- CREATION DU DOSSIER D'EXPORT\n",
    "# ---------------------------------\n",
    "os.makedirs(\"../data/processed\", exist_ok=True)\n",
    "# ------------------------------------------------------------\n",
    "# --- EXPORT DES DATAFRAMES NETTOYES SANS OUTLIERS ET EXPLORES\n",
    "# -------------------------------------------------------------\n",
    "# üíæ sample_explo_wo (100 questions sans outliers)\n",
    "sample_explo_wo.to_parquet(\"data/processed/sample_explo_wo.parquet\", index=False)\n",
    "\n",
    "# üíæ full_explo_wo (10k questions sans outliers)\n",
    "full_explo_wo.to_parquet(\"data/processed/full_explo_wo.parquet\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b66acbf",
   "metadata": {},
   "source": [
    "### 5.2. EXPORT SUR 50 K QUESTIONS\n",
    "\n",
    "Cette sous-section permet de sauvegarder tous les artefacts NLP construits sur le corpus complet (`brut_df`) afin de les rendre accessibles aux notebooks suivants :  \n",
    "`3_modele_non_supervise_lda.ipynb`, `4_modele_supervise_mlflow.ipynb`, etc.\n",
    "\n",
    "\n",
    "\n",
    "***Corpus export√© :***\n",
    "\n",
    "| Nom | Format | Description |\n",
    "|-----|--------|-------------|\n",
    "| `corpus_for_lda.csv` | CSV | Texte nettoy√© (titre + body concat√©n√©) avec `PostId` |\n",
    "| `corpus_for_lda_bow.pkl` | PKL | Matrice BoW pour mod√©lisation LDA |\n",
    "| `corpus_for_lda_vocab.pkl` | PKL | Vocabulaire associ√© √† la BoW |\n",
    "| `tfidf_matrix_concat.pkl` | PKL | Matrice TF-IDF des 50k questions |\n",
    "| `tfidf_vocab_concat.pkl` | PKL | Vocabulaire TF-IDF (concat√©n√©) |\n",
    "| `dominant_tags_brut.pkl` | PKL | Liste des tags dominants rep√©r√©s sur `brut_df` |\n",
    "| `multilabel_binarizer.pkl` | PKL | Encodeur des tags multi-label (sklearn) |\n",
    "| `tags_encoded_brut.pkl` | PKL | Matrice Y cible (multi-label) pour mod√®le supervis√© |\n",
    "\n",
    "\n",
    "\n",
    "Ces artefacts sont r√©utilisables sans recalcul dans tous les notebooks de mod√©lisation, et peuvent √™tre versionn√©s pour suivre l‚Äô√©volution du pipeline.\n",
    "\n",
    "Cette √©tape garantit la modularit√© du projet et son passage en production.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11acb946",
   "metadata": {},
   "source": [
    "## ANNEXE "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e5fcab",
   "metadata": {},
   "source": [
    "### A.1. Nomenclature des DataFrames utilis√©s\n",
    "\n",
    "| Nom du DataFrame       | Description |\n",
    "|------------------------|-------------|\n",
    "| `brut_df`              | Donn√©es brutes charg√©es depuis le fichier CSV |\n",
    "| `sample_df`            | √âchantillon de 100 questions tir√© de `brut_df` |\n",
    "| `sample_clean`         | Version nettoy√©e de `sample_df` (apr√®s pr√©traitement) |\n",
    "| `sample_explo`         | Copie de travail pour l‚Äôexploration (√† partir de `sample_clean`) |\n",
    "| `sample_explo_wo`      | Version de `sample_explo` sans outliers (`wo = without outliers`) |\n",
    "| `sample_outliers`      | Sous-ensemble de `sample_explo` contenant uniquement les outliers |\n",
    "| `full_clean`           | Donn√©es compl√®tes nettoy√©es (lors de la mise √† l‚Äô√©chelle) |\n",
    "| `full_explo`           | Copie de travail pour l‚Äôexploration compl√®te |\n",
    "| `full_explo_wo`        | Version sans outliers du corpus complet |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813bff81",
   "metadata": {},
   "source": [
    "### A.2. RAPPELS DES NOTIONS IMPLIQUEES DANS LE FEATURE ENGINEERING EN NLP\n",
    "\n",
    "Cette annexe vise √† expliciter les concepts math√©matiques et vectoriels sous-jacents aux principales √©tapes de transformation des textes dans le cadre du NLP. Elle s‚Äôadresse √† un lecteur ayant une formation scientifique avanc√©e.\n",
    "\n",
    "\n",
    "\n",
    "#### A.2.1. Vecteurs creux vs. vecteurs denses\n",
    "\n",
    "- Un **vecteur creux** (sparse vector) est un vecteur dont la majorit√© des composantes sont nulles.\n",
    "  - Exemple : `[0, 0, 0.5, 0, 0, 0.3, 0, 0, 0, 0.2]`\n",
    "  - Typique des repr√©sentations TF-IDF : chaque document n‚Äôutilise qu‚Äôun petit sous-ensemble du vocabulaire.\n",
    "\n",
    "- Un **vecteur dense** est un vecteur dont la plupart des composantes sont non nulles.\n",
    "  - Exemple : `[0.12, -0.03, 0.45, 0.08, -0.22, 0.19]`\n",
    "  - Typique des embeddings ou des vecteurs apr√®s r√©duction de dimension.\n",
    "\n",
    "\n",
    "\n",
    "#### A.2.2. Variables latentes et espace latent\n",
    "\n",
    "- Une **variable latente** est une variable non observ√©e directement, mais d√©duite √† partir des donn√©es.\n",
    "- En NLP, les **axes latents** correspondent √† des th√©matiques ou structures cach√©es dans les textes.\n",
    "- L‚Äô**espace latent** est un espace vectoriel r√©duit, o√π chaque dimension repr√©sente une composante th√©matique implicite.\n",
    "\n",
    "\n",
    "\n",
    "#### A.2.3. Base canonique vs. base s√©mantique\n",
    "\n",
    "- La **base canonique** est form√©e par les mots du vocabulaire : chaque mot est une dimension orthogonale.\n",
    "- Elle est arbitraire, tr√®s grande, et souvent redondante.\n",
    "- Une **base s√©mantique** est une base apprise ou extraite (via SVD ou embeddings) o√π chaque dimension refl√®te un th√®me ou une proximit√© de sens.\n",
    "- Elle est plus compacte, plus informative, mais non orthogonale.\n",
    "\n",
    "\n",
    "\n",
    "#### A.2.4. D√©composition SVD (Singular Value Decomposition)  \n",
    "\n",
    "Soit une matrice TF-IDF X de taille (m √ó n), o√π :\n",
    "\n",
    "- m est le nombre de documents\n",
    "- n est la taille du vocabulaire (nombre de mots uniques)\n",
    "- Chaque ligne de X est un vecteur TF-IDF repr√©sentant un document\n",
    "\n",
    "La d√©composition en valeurs singuli√®res (SVD) permet d‚Äô√©crire :\n",
    "\n",
    "X = U √ó Œ£ √ó V·µó\n",
    "\n",
    "avec :\n",
    "\n",
    "- U ‚àà ‚Ñù^(m √ó r) : matrice des vecteurs propres des documents (orthogonaux)\n",
    "- Œ£ ‚àà ‚Ñù^(r √ó r) : matrice diagonale contenant les valeurs singuli√®res d√©croissantes\n",
    "- V ‚àà ‚Ñù^(n √ó r) : matrice des vecteurs propres des mots (orthogonaux)\n",
    "\n",
    "o√π r est le rang de X (r ‚â§ min(m, n))\n",
    "\n",
    "\n",
    "**R√©duction de dimension**\n",
    "\n",
    "En pratique, on ne conserve que les k premi√®res composantes (avec k ‚â™ r), ce qui donne une approximation de rang k :\n",
    "\n",
    "X ‚âà U_k √ó Œ£_k √ó V_k·µó\n",
    "\n",
    "avec :\n",
    "\n",
    "- U_k ‚àà ‚Ñù^(m √ó k) : projection des documents dans un espace latent de dimension k\n",
    "- Œ£_k ‚àà ‚Ñù^(k √ó k) : valeurs singuli√®res principales\n",
    "- V_k ‚àà ‚Ñù^(n √ó k) : projection des mots dans le m√™me espace latent\n",
    "\n",
    "\n",
    "**Interpr√©tation**\n",
    "\n",
    "- Chaque document est repr√©sent√© par un vecteur dense de dimension k, capturant ses composantes th√©matiques latentes\n",
    "- Chaque mot est √©galement projet√© dans cet espace, r√©v√©lant ses affinit√©s s√©mantiques\n",
    "- Les valeurs singuli√®res indiquent l‚Äôimportance relative de chaque axe latent\n",
    "\n",
    "**Utilit√© en NLP**\n",
    "\n",
    "- R√©duction de dimension : compresser les vecteurs TF-IDF tr√®s grands et creux\n",
    "- Suppression du bruit lexical : √©liminer les dimensions peu informatives\n",
    "- D√©couverte de th√©matiques : chaque axe latent peut √™tre interpr√©t√© comme un th√®me s√©mantique\n",
    "\n",
    "Cette technique est √† la base de la m√©thode LSA (Latent Semantic Analysis).\n",
    "\n",
    "#### A.2.5. Embeddings (Word2Vec, FastText, BERT‚Ä¶)\n",
    "\n",
    "- Les **embeddings** sont des vecteurs denses appris par des mod√®les de langage.\n",
    "- Ils sont construits pour que les mots ayant des contextes similaires aient des vecteurs proches.\n",
    "- Contrairement √† TF-IDF, ils capturent :\n",
    "  - La **s√©mantique** (proximit√© de sens)\n",
    "  - Le **contexte** (dans les mod√®les comme BERT)\n",
    "  - Les **relations syntaxiques** et **logiques**\n",
    "\n",
    "Exemple d‚Äôanalogie vectorielle :  \n",
    "`king - man + woman ‚âà queen`\n",
    "\n",
    "Math√©matiquement, les embeddings sont issus d‚Äôune **factorisation implicite** de la matrice de co-occurrence des mots, ou d‚Äôun apprentissage supervis√©/non supervis√© sur des t√¢ches de pr√©diction de contexte.\n",
    "\n",
    "\n",
    "\n",
    "#### A.2.6. Encodage des tags ‚Äì MultiLabelBinarizer\n",
    "\n",
    "- Les tags sont des cibles multi-label (un document peut avoir plusieurs √©tiquettes).\n",
    "- Le **MultiLabelBinarizer** transforme chaque liste de tags en un vecteur binaire.\n",
    "\n",
    "Exemple :\n",
    "\n",
    "| Tags | Encodage |\n",
    "|------|----------|\n",
    "| ['python', 'django'] | [1, 0, 1, 0, ‚Ä¶] |\n",
    "\n",
    "Chaque dimension correspond √† un tag unique.  \n",
    "Ce format est requis pour entra√Æner un mod√®le de classification multi-label.\n",
    "\n",
    "\n",
    "\n",
    "#### A.2.7. R√©sum√© global du pipeline vectoriel\n",
    "\n",
    "| √âtape | Entr√©e | Sortie | Objectif |\n",
    "|-------|--------|--------|----------|\n",
    "| TF-IDF | Texte brut | Vecteurs creux | Pond√©rer les mots |\n",
    "| SVD / PCA | TF-IDF | Vecteurs denses r√©duits | Capturer les axes th√©matiques |\n",
    "| Embeddings | Texte brut | Vecteurs denses s√©mantiques | Capturer le sens |\n",
    "| MultiLabelBinarizer | Tags | Vecteurs binaires (cibles) | Pr√©parer les cibles |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f855595",
   "metadata": {},
   "source": [
    "### A.3. FONCTIONNEMENT DE FASTAPI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab3bc84",
   "metadata": {},
   "source": [
    "```text\n",
    "+-------------------------------+\n",
    "|        Client (UI,           |\n",
    "|   script Python, Swagger)    |\n",
    "+---------------+---------------+\n",
    "                |\n",
    "                |  POST /embed/sbert\n",
    "                |  { \"text\": \"...\" }\n",
    "                v\n",
    "+---------------+---------------+\n",
    "|         FastAPI App           |\n",
    "|     (src/api/main.py)         |\n",
    "+---------------+---------------+\n",
    "                |\n",
    "                v\n",
    "+---------------+---------------+\n",
    "|  Route /embed/sbert           |\n",
    "|  (embed_routes.py)            |\n",
    "|  - Valide l'entr√©e (pydantic) |\n",
    "|  - Appelle encode_with_sbert  |\n",
    "+---------------+---------------+\n",
    "                |\n",
    "                v\n",
    "+---------------+---------------+\n",
    "|  SBERT Model (sbert_model.py) |\n",
    "+---------------+---------------+\n",
    "                |\n",
    "                v\n",
    "+---------------+---------------+\n",
    "|  Embedding vector (list)      |\n",
    "|  ex: [0.123, -0.456, ...]     |\n",
    "+---------------+---------------+\n",
    "                |\n",
    "                v\n",
    "+-------------------------------+\n",
    "|   JSON Response to Client     |\n",
    "|   { \"embedding\": [...] }      |\n",
    "+-------------------------------+\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c9119e",
   "metadata": {},
   "source": [
    "### A.4. MISE A L'ECHELLE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e920196",
   "metadata": {},
   "source": [
    "Ce notebook a √©t√© d√©velopp√© sur `sample_df` pour valider les fonctions de nettoyage, d‚Äôanalyse exploratoire et de construction des features textuelles.\n",
    "\n",
    "***Objectif √† l‚Äô√©chelle : pr√©traiter les 50 000 questions StackOverflow extraites via SQL.***\n",
    "\n",
    "| √âtape | Action | D√©tails |\n",
    "|-------|--------|---------|\n",
    "|  Chargement des donn√©es brutes | Import du `brut_df` (50 000 questions) | Format : `.csv` ou SQL ‚Üí colonnes `Title`, `Body`, `Tags`, etc. |\n",
    "|  Nettoyage des textes | Application des fonctions `clean_title`, `clean_body` | Nettoyage HTML, ponctuation, contractions, stopwords |\n",
    "|  Fusion des colonnes | Cr√©ation de `clean_title_body` | Pour vectorisations futures (BoW, TF-IDF, embeddings) |\n",
    "|  Suppression des doublons exacts | `df.duplicated([\"Title\", \"Body\"])` | √âlimination des doublons 1:1 |\n",
    "|  Filtrage d‚Äôoutliers lexicaux | Longueur minimale des titres/corps | Exemple : `len(title) > 10` ou `nb_mots > 3` |\n",
    "|  Analyse optionnelle | Stats descriptives & distribution de longueur | Permet de valider la structure du corpus brut |\n",
    "|  Export des artefacts | Sauvegarde du corpus nettoy√© dans `/data/processed/` | `brut_df_cleaned.csv` ou `.pkl` |\n",
    "\n",
    " Cette √©tape garantit que l‚Äôensemble du corpus est pr√™t pour la mod√©lisation √† grande √©chelle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c7e091",
   "metadata": {},
   "source": [
    "Ce notebook a √©t√© con√ßu pour valider les √©tapes de pr√©traitement NLP sur un √©chantillon r√©duit (`sample_df`, 100 questions).  \n",
    "La mise √† l‚Äô√©chelle sur le corpus complet (`brut_df`, 50 000 questions) est structur√©e de mani√®re parall√®le, chaque sous-section √©tant dupliqu√©e pour assurer coh√©rence et industrialisation.\n",
    "\n",
    "---\n",
    "\n",
    "***Structure de Mise √† l‚Äô√âchelle par Bloc***\n",
    "\n",
    "| Section originale | Sous-section √† cr√©er pour `brut_df` | Objectif |\n",
    "|------------------|--------------------------------------|----------|\n",
    "| `2.1. Nettoyage - sample_df` | `2.2. Nettoyage - brut_df` | Nettoyer les 50k titres et corps via `clean_text_spacy_custom()` |\n",
    "| `3.1. Exploration - sample_explo` | `3.2. Exploration - full_explo` | Calculer longueurs, ratios, distributions sur tout le corpus |\n",
    "| `3.1.4 D√©tection des doublons/outliers - sample_df` | `3.2.4 D√©tection - brut_df` | Exclure les lignes bruit√©es avant vectorisation |\n",
    "| `3.1.5 Analyse des tags - sample_df` | `3.2.5 Analyse des tags - brut_df` | √âtudier la diversit√© et marquer les tags dominants sur 50k |\n",
    "| `3.1.6 Co-occurrence des tags` | `3.2.6 Co-occurrence des tags - brut_df` | Identifier clusters th√©matiques globaux |\n",
    "| `4.1.1 Vectorisation TF-IDF - sample_df` | `4.2.1 Vectorisation TF-IDF - brut_df` | G√©n√©rer les vecteurs `X_titlebody_tfidf_brut.pkl` |\n",
    "| `4.1.4 Vectorisation BoW - sample_df` | `4.2.4 Vectorisation BoW - brut_df` | G√©n√©rer `X_bow_brut.pkl` pour LDA |\n",
    "| `4.1.5 Encodage des tags - sample_df` | `4.2.5 Encodage des tags - brut_df` | G√©n√©rer la matrice cible multi-label `tags_encoded_brut.pkl` |\n",
    "| `5. Export - sample_df` | `5. Export - brut_df` | Sauvegarde des artefacts TF-IDF, BoW, corpus nettoy√©, vocabulaire |\n",
    "\n",
    "---\n",
    "\n",
    "***Artefacts √† g√©n√©rer pour mise √† l‚Äô√©chelle***\n",
    "\n",
    "| Type | Nom sugg√©r√© | Format |\n",
    "|------|-------------|--------|\n",
    "| Corpus nettoy√© complet | `corpus_cleaned_brut.csv` | CSV |\n",
    "| Matrice BoW | `corpus_for_lda_bow_brut.pkl` | PKL |\n",
    "| Vocabulaire BoW | `corpus_for_lda_vocab_brut.pkl` | PKL |\n",
    "| Matrice TF-IDF | `tfidf_matrix_concat_brut.pkl` | PKL |\n",
    "| Vocab TF-IDF | `tfidf_vocab_concat_brut.pkl` | PKL |\n",
    "| Binarizer tags | `multilabel_binarizer_brut.pkl` | PKL |\n",
    "| Matrice tags encod√©s | `tags_encoded_brut.pkl` | PKL |\n",
    "\n",
    "\n",
    "\n",
    "***Recommandations***\n",
    "\n",
    "- Utiliser le flag `use_sample=True/False` dans les fonctions du module `eda_analysis.py` pour switcher d‚Äôun jeu √† l‚Äôautre.\n",
    "- Surveiller la m√©moire lors de l‚Äôutilisation d‚Äôembeddings ou de r√©ductions de dimension : lancer par batch si n√©cessaire.\n",
    "- Documenter chaque sous-section `brut_df` avec la mention :  \n",
    "  *\"Cette sous-section applique la logique valid√©e sur `sample_df` au corpus complet de 50k questions (`brut_df`).\"*\n",
    "\n",
    "\n",
    "\n",
    "Ce plan permet une industrialisation progressive et structur√©e du pipeline, tout en conservant la rigueur d‚Äôexploration test√©e sur l‚Äô√©chantillon.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21132ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import kagglehub\n",
    "# path = kagglehub.model_download(\"google/universal-sentence-encoder/tensorFlow2/universal-sentence-encoder\")\n",
    "# print(\"‚úÖ Mod√®le t√©l√©charg√© dans :\", path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "StackOverFlowTags (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
