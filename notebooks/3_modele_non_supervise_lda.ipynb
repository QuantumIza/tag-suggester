{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea7aebbf",
   "metadata": {},
   "source": [
    "## INTRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c4657e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "L’objectif de cette section est d’explorer les **structures latentes** du corpus sans recours aux `Tags` comme variable cible.  \n",
    "Nous cherchons à comprendre comment les questions se regroupent naturellement selon leur contenu sémantique, afin d’éventuellement **enrichir ou automatiser les suggestions de tags**.\n",
    "\n",
    "Les techniques utilisées ici incluent :\n",
    "\n",
    "- **LDA (Latent Dirichlet Allocation)** : pour extraire des *topics* latents et comprendre les thématiques présentes dans les questions\n",
    "- **Méthodes de Clustering (KMeans, DBSCAN, etc.)** : pour segmenter les questions selon leur similarité vectorielle\n",
    "\n",
    "Ces approches permettent :\n",
    "- Une lecture qualitative des **sous-communautés thématiques**\n",
    "- Une aide à la navigation ou à la classification implicite du corpus\n",
    "- Une base de travail pour construire des outils de **suggestion de tags intelligents**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ffe6a4",
   "metadata": {},
   "source": [
    "## IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c14721",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from gensim.matutils import Sparse2Corpus\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896ac532",
   "metadata": {},
   "source": [
    "## 1. MODELE LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb13df78",
   "metadata": {},
   "source": [
    "### **1.1. CHARGEMENT DES DONNEES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94a18da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CHARGEMENT DES DONNEES\n",
    "import pandas as pd\n",
    "df_corpus = pd.read_parquet(\"data/processed/full_explo_wo.parquet\")\n",
    "corpus = df_corpus[\"clean_title_body\"].tolist()  # Texte fusionné nettoyé\n",
    "\n",
    "import scipy.sparse\n",
    "import pickle\n",
    "X_bow = scipy.sparse.load_npz(\"models/bow/X_bow_full.npz\")\n",
    "with open(\"models/bow/vocab_bow_full.pkl\", \"rb\") as f:\n",
    "    vocab = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ff42e7",
   "metadata": {},
   "source": [
    "### **1.2. PREPARATION DES DONNEES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcb9fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- CREATION DU DICTIONNAIRE GENSIM\n",
    "id2word = corpora.Dictionary()\n",
    "id2word.id2token = dict(enumerate(vocab))\n",
    "id2word.token2id = {v: k for k, v in id2word.id2token.items()}\n",
    "\n",
    "# --- CONVERSION SPARSE MATRIC → FORMAT GENSIM\n",
    "corpus_gensim = Sparse2Corpus(X_bow, documents_columns=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5f63a6",
   "metadata": {},
   "source": [
    "### **1.3. PARAMETRAGE ET ENTRAINEMENT DU MODELE LDA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9488099",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- PARAMETRES DU MODELE\n",
    "num_topics = 10\n",
    "random_state = 42\n",
    "\n",
    "# --- ENTRAINEMENT\n",
    "lda_model = LdaModel(\n",
    "    corpus=corpus_gensim,\n",
    "    id2word=id2word,\n",
    "    num_topics=num_topics,\n",
    "    random_state=random_state,\n",
    "    passes=10,\n",
    "    chunksize=100,\n",
    "    alpha='auto',\n",
    "    per_word_topics=True\n",
    ")\n",
    "\n",
    "print(\"# --- Modèle LDA entraîné avec\", num_topics, \"topics\")\n",
    "\n",
    "# --- AFFICHAGE DES 5 MOTS LES PLUS REPRESENTATIFS PAR TOPIC\n",
    "for i in range(num_topics):\n",
    "    print(f\"\\n# ---  Topic {i}:\")\n",
    "    print([word for word, prob in lda_model.show_topic(i, topn=5)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af29e0b",
   "metadata": {},
   "source": [
    "### **1.4. VISUALISATION DES TOPICS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f63bd3",
   "metadata": {},
   "source": [
    "#### **1.4.1. DISTRIBUTION DES TERMES DANS LES TOPICS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4b6fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# --- PREPARATION DES DONNEES POUR LA VISUALISATION\n",
    "vis_data = pyLDAvis.gensim_models.prepare(lda_model, corpus_gensim, id2word)\n",
    "\n",
    "# --- AFFICHAGE\n",
    "pyLDAvis.display(vis_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884a9b10",
   "metadata": {},
   "source": [
    "#### **1.4.2. ATTRIBUTION DU TOPIC DOMINANT A CHAQUE DOCUMENT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcb92da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_corpus.columns.tolist())\n",
    "\n",
    "\n",
    "# --- ATTRIBUTION DU TOPIC DOMINANT A CHAQUE DOCUMENT\n",
    "topic_assignments = []\n",
    "for doc_bow in corpus_gensim:\n",
    "    topic_probs = lda_model.get_document_topics(doc_bow)\n",
    "    top_topic = sorted(topic_probs, key=lambda x: x[1], reverse=True)[0][0]\n",
    "    topic_assignments.append(top_topic)\n",
    "\n",
    "# --- AJOUT AU DATAFRAME\n",
    "df_corpus[\"dominant_topic\"] = topic_assignments\n",
    "\n",
    "# --- APERCU\n",
    "df_corpus[[\"PostId\", \"dominant_topic\", \"clean_title_body\"]].head()\n",
    "\n",
    "df_corpus.to_csv(\"../data/processed/corpus_topic_assignments.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b922531d",
   "metadata": {},
   "source": [
    "#### **1.4.3. CLASSEMENT DES QUESTIONS PAR TOPIC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4951a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- STRUCTURE POUR STOCKER LES MEILLEURES QUESTIONS PAR TOPIC\n",
    "top_docs_by_topic = defaultdict(list)\n",
    "\n",
    "# --- BOUCLE SUR CHAQUE DOCUMENT ( document = ligne du corpus)\n",
    "for i, doc_bow in enumerate(corpus_gensim):\n",
    "    topic_probs = lda_model.get_document_topics(doc_bow, minimum_probability=0.0)\n",
    "    sorted_topics = sorted(topic_probs, key=lambda x: x[1], reverse=True)\n",
    "    top_topic, top_score = sorted_topics[0]\n",
    "    \n",
    "    # --- STOCKER L'INDEX DU DOCUMENT + SON SCORE SI LE QUOTA N'EST PAS ATTEINT\n",
    "    if len(top_docs_by_topic[top_topic]) < 3:\n",
    "        top_docs_by_topic[top_topic].append((i, top_score))\n",
    "\n",
    "# --- AFFICHAGE\n",
    "for topic_id in range(num_topics):\n",
    "    print(f\"\\n# --- Topic {topic_id} — mots clés :\", [word for word, _ in lda_model.show_topic(topic_id, topn=5)])\n",
    "    for i_doc, score in top_docs_by_topic[topic_id]:\n",
    "        print(f\"# --- Score {score:.3f} — Question:\")\n",
    "        print(df_corpus.loc[i_doc, \"clean_title_body\"][:250], \"...\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b319721",
   "metadata": {},
   "source": [
    "## 2. MODELE CLUSTERING : REGROUPEMENT NATUREL DES QUESTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a704bc5f",
   "metadata": {},
   "source": [
    "- Objectif : segmentation selon la similarité\n",
    "- Technique : KMeans ou DBSCAN sur TF-IDF / Word2Vec\n",
    "- Résultat : visualisation en 2D (TSNE ou PCA), interprétation des clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a579b86c",
   "metadata": {},
   "source": [
    "### 2.1. CHARGEMENT DES DONNEES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a5afc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.sparse import load_npz\n",
    "\n",
    "# X_tfidf = load_npz(\"models/tfidf/X_tfidf_full.npz\")\n",
    "# X_dense = X_tfidf.toarray()  # nécessaire pour GMM\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c42683",
   "metadata": {},
   "source": [
    "### 2.2. PREPARATION DES DONNEES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace95984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # Si les vecteurs sont en sparse matrix → conversion vers dense\n",
    "# X_dense = X_tfidf.toarray()\n",
    "# print(\"✅ Vecteurs convertis → shape :\", X_dense.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab8b798",
   "metadata": {},
   "source": [
    "### 2.3. ENTRAINEMENT GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9139966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# gmm = GaussianMixture(\n",
    "#     n_components=8,            # nombre de clusters à tester\n",
    "#     covariance_type='full',    # plus flexible qu’'diag'\n",
    "#     max_iter=200,\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "# gmm.fit(X_dense)\n",
    "# labels = gmm.predict(X_dense)\n",
    "# probs = gmm.predict_proba(X_dense)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1178c8b3",
   "metadata": {},
   "source": [
    "### 2.4. VISUALISATION DES CLUSTERS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81f1752",
   "metadata": {},
   "source": [
    "#### 2.4.1. PROJECTION ACP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e70586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Réduction à 2 dimensions pour l'affichage\n",
    "# pca = PCA(n_components=2, random_state=42)\n",
    "# X_2d = pca.fit_transform(X_dense)\n",
    "\n",
    "# # Affichage des clusters détectés\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# scatter = plt.scatter(X_2d[:, 0], X_2d[:, 1], c=labels, cmap='tab10', alpha=0.7)\n",
    "# plt.title(\"📊 Clusters GMM — Projection PCA\")\n",
    "# plt.xlabel(\"PC1\")\n",
    "# plt.ylabel(\"PC2\")\n",
    "# plt.colorbar(scatter, label=\"Cluster\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38736453",
   "metadata": {},
   "source": [
    "#### 2.4.2. PROJECTION TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b47cf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.manifold import TSNE\n",
    "\n",
    "# X_2d_tsne = TSNE(n_components=2, perplexity=30, random_state=42).fit_transform(X_dense)\n",
    "\n",
    "# plt.figure(figsize=(8,6))\n",
    "# plt.scatter(X_2d_tsne[:, 0], X_2d_tsne[:, 1], c=labels, cmap='tab10', alpha=0.7)\n",
    "# plt.title(\"📌 Clusters GMM — Projection TSNE\")\n",
    "# plt.xlabel(\"Dim 1\")\n",
    "# plt.ylabel(\"Dim 2\")\n",
    "# plt.colorbar(label=\"Cluster\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5295ecd",
   "metadata": {},
   "source": [
    "#### 2.4.3. NUAGE DE MOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31855b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from wordcloud import WordCloud\n",
    "\n",
    "# # Création d’un DataFrame pour regrouper les documents\n",
    "# import pandas as pd\n",
    "\n",
    "# df_corpus = pd.read_csv(\"../data/processed/corpus_for_lda.csv\")\n",
    "# corpus = df_corpus[\"text_combined\"].tolist()  # ⚠️ colonne contenant le texte fusionné\n",
    "\n",
    "\n",
    "# df_clusters = pd.DataFrame({\n",
    "#     \"document\": corpus,\n",
    "#     \"cluster\": labels,\n",
    "# })\n",
    "\n",
    "# # Génération des wordclouds\n",
    "# for c in sorted(df_clusters[\"cluster\"].unique()):\n",
    "#     cluster_text = \" \".join(df_clusters[df_clusters[\"cluster\"] == c][\"document\"])\n",
    "#     wc = WordCloud(width=800, height=400, background_color=\"white\").generate(cluster_text)\n",
    "    \n",
    "#     plt.figure(figsize=(10, 5))\n",
    "#     plt.imshow(wc, interpolation='bilinear')\n",
    "#     plt.axis(\"off\")\n",
    "#     plt.title(f\"☁️ Cluster {c} — Wordcloud\")\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365fef32",
   "metadata": {},
   "source": [
    "#### 2.4.4. SILHOUETTE SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3d83ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import silhouette_score\n",
    "\n",
    "# sil_score = silhouette_score(X_dense, labels)\n",
    "# print(f\"📐 Silhouette Score : {sil_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da6d730",
   "metadata": {},
   "source": [
    "#### 2.4.5. NUAGE DE MOTS EN FREQUENCE COULEUR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea24065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from wordcloud import WordCloud\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Regrouper les documents et générer un WordCloud par cluster\n",
    "# for c in sorted(df_clusters[\"cluster\"].unique()):\n",
    "#     corpus_cluster = df_clusters[df_clusters[\"cluster\"] == c][\"document\"]\n",
    "#     text = \" \".join(corpus_cluster.astype(str))  # 🔧 Définition de 'text'\n",
    "    \n",
    "#     wc = WordCloud(\n",
    "#         width=800,\n",
    "#         height=400,\n",
    "#         colormap=\"tab10\",\n",
    "#         background_color=\"white\"\n",
    "#     ).generate(text)\n",
    "    \n",
    "#     plt.figure(figsize=(10, 5))\n",
    "#     plt.imshow(wc, interpolation='bilinear')\n",
    "#     plt.axis(\"off\")\n",
    "#     plt.title(f\"☁️ Cluster {c} — Wordcloud\")\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96db5bb6",
   "metadata": {},
   "source": [
    "#### 2.4.6. HEATMAP DE SIMILARITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c668ce63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# import seaborn as sns\n",
    "\n",
    "# sim_matrix = cosine_similarity(X_dense[:100])  # sur échantillon\n",
    "# sns.heatmap(sim_matrix, cmap='viridis')\n",
    "# plt.title(\"🧭 Similarité entre documents\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128a7835",
   "metadata": {},
   "source": [
    "### 2.5. ANALYSE DES CLUSTERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dd6357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Pour visualiser un aperçu par cluster\n",
    "# df_clusters = pd.DataFrame({\n",
    "#     \"document\": corpus,\n",
    "#     \"cluster\": labels,\n",
    "#     \"proba_max\": probs.max(axis=1)\n",
    "# })\n",
    "\n",
    "# # 🧠 Pour chaque cluster, afficher les documents les plus \"clairs\"\n",
    "# df_top = df_clusters.sort_values(\"proba_max\", ascending=False).groupby(\"cluster\").head(3)\n",
    "# df_top[[\"cluster\", \"proba_max\", \"document\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4ab68c",
   "metadata": {},
   "source": [
    "## 3.MODELE MINIBATCHKMEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00614d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import load_npz\n",
    "\n",
    "X_tfidf = load_npz(\"models/tfidf/X_tfidf_full.npz\")\n",
    "X_dense = X_tfidf.toarray()  # nécessaire pour GMM\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Si les vecteurs sont en sparse matrix → conversion vers dense\n",
    "X_dense = X_tfidf.toarray()\n",
    "print(\"✅ Vecteurs convertis → shape :\", X_dense.shape)\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X_reduced = PCA(n_components=50).fit_transform(X_dense)\n",
    "kmeans = MiniBatchKMeans(n_clusters=8, batch_size=256, random_state=42)\n",
    "labels = kmeans.fit_predict(X_reduced)\n",
    "\n",
    "import pandas as pd\n",
    "df_clusters_kmeans = pd.DataFrame({\n",
    "    \"document\": df_corpus[\"clean_title_body\"],\n",
    "    \"cluster\": labels\n",
    "})\n",
    "\n",
    "cluster_counts = df_clusters_kmeans[\"cluster\"].value_counts().sort_index()\n",
    "print(\"📊 Répartition des documents par cluster :\")\n",
    "print(cluster_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab494e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for c in sorted(df_clusters_kmeans[\"cluster\"].unique()):\n",
    "    cluster_text = \" \".join(df_clusters_kmeans[df_clusters_kmeans[\"cluster\"] == c][\"document\"])\n",
    "    wc = WordCloud(width=800, height=400, background_color=\"white\", colormap=\"tab10\").generate(cluster_text)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"🌐 Cluster {c} — Wordcloud\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9728ada4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "X_2d = TSNE(n_components=2, perplexity=30, random_state=42).fit_transform(X_reduced)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_2d[:, 0], X_2d[:, 1], c=labels, cmap='tab10', alpha=0.7)\n",
    "plt.title(\"📍 Projection TSNE des clusters MiniBatchKMeans\")\n",
    "plt.xlabel(\"Dim 1\")\n",
    "plt.ylabel(\"Dim 2\")\n",
    "plt.colorbar(label=\"Cluster\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc77ba8",
   "metadata": {},
   "source": [
    "## ANNEXES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9000222",
   "metadata": {},
   "source": [
    "### A1. MISE A L'ECHELLE (`modèle non supervisé LDA`)\n",
    "\n",
    "Ce notebook a été développé sur `sample_df` pour tester la logique d’extraction thématique via LDA.\n",
    "\n",
    "***Objectif à l’échelle : identifier les thématiques latentes dans 50 000 questions, à partir de vecteurs BoW.***\n",
    "\n",
    "| Étape | Action | Détails |\n",
    "|-------|--------|---------|\n",
    "|  Chargement du corpus textuel | Import du corpus nettoyé (`clean_title_body`) | Fichier `corpus_for_lda.csv` |\n",
    "|  Vectorisation BoW | Application du `CountVectorizer` | Paramètres : `max_df=0.95`, `min_df=5`, stopwords anglais |\n",
    "|  Matrice BoW | Résultat : `X_bow` (shape : n_docs × n_vocab) | Stockée dans `corpus_for_lda_bow.pkl` |\n",
    "|  Extraction du vocabulaire | `get_feature_names_out()` | Stockée dans `corpus_for_lda_vocab.pkl` |\n",
    "|  Modélisation LDA | Application du modèle LDA via `sklearn` ou `gensim` | `n_topics` à tester selon cohérence visuelle |\n",
    "|  Visualisation des topics | Utilisation de `pyLDAvis`, `TSNE`, WordClouds | Permet d’interpréter les regroupements |\n",
    "|  Évaluation qualitative | Lecture des documents top-par-topic | Aide à la cartographie thématique du corpus |\n",
    "\n",
    " Cette modélisation peut servir de base à la suggestion de tags ou à la navigation intelligente dans le corpus.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "StackOverFlowTags (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
