{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b4f4de5",
   "metadata": {},
   "source": [
    "## 1. IMPORTS  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9921b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STANDARD\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- VISUALISATION\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- MACHINE LEARNING\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import f1_score, hamming_loss, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# --- TRAITEMENT SPECIFIQUE\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# --- PERSISTENCE / OUTILS EXTERNES\n",
    "import joblib\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# --- UTILITAIRES\n",
    "from scipy.sparse import load_npz\n",
    "from tqdm.notebook import tqdm  # ou simplement tqdm si console\n",
    "\n",
    "# --- TRACKING MLFLOW\n",
    "import mlflow\n",
    "import mlflow.sklearn  # Pour les mod√®les scikit-learn\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# --- AJOUT DOSSIER PARENT DU NOTEBOOK AU PATH PYTHON\n",
    "project_root = os.path.abspath(\"..\")  # ou \"../..\" selon ton niveau\n",
    "sys.path.append(project_root)\n",
    "# --- MODULES PROJET\n",
    "import importlib\n",
    "import src.tags_suggester.modeling.modeling as mdl\n",
    "importlib.reload(mdl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d084e2",
   "metadata": {},
   "source": [
    "## 2. MODELISATION : LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5366b0ab",
   "metadata": {},
   "source": [
    "### 2.1. CHARGEMENT DES FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021eed5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES DE MODELISATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import src.tags_suggester.modeling.modeling as mdl\n",
    "importlib.reload(mdl)\n",
    "\n",
    "# -----------------------------------------\n",
    "# --- 0. CONFIGURATION DU MOD√àLE √Ä TESTER\n",
    "# -----------------------------------------\n",
    "# ‚úÖ Tu peux changer ces lignes pour benchmarker un autre mod√®le\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "model_type = \"logreg\"\n",
    "model_class = LogisticRegression\n",
    "model_wrapper = OneVsRestClassifier  # ou ClassifierChain ou None\n",
    "\n",
    "# ------------------------------\n",
    "# --- 1. SELECTION DES FEATURES\n",
    "# ------------------------------\n",
    "# --- COMMENTE CAR CAS 50000 QUESTIONS REND LECTURE IMPOSSIBLE\n",
    "# full_df = pd.read_parquet(\"data/processed/full_explo_wo.parquet\")\n",
    "# --- DEBUT SOLUCE ADAPTEE 50 000 QUESTIONS\n",
    "# import pandas as pd\n",
    "# chunks = []\n",
    "# chunk_size = 5000  # ou 10000 selon ta RAM\n",
    "\n",
    "# for chunk in pd.read_csv(\"data/processed/full_explo_wo.csv.gz\", chunksize=chunk_size):\n",
    "#     chunks.append(chunk)\n",
    "\n",
    "# full_df = pd.concat(chunks, ignore_index=True)\n",
    "# --- FIN SOLUCE ADAPTEE 50 000 QUESTIONS\n",
    "\n",
    "# print(f\"Dimensions du dataframe full_df : {full_df.shape}\")\n",
    "# print(f\" Colonnes du dataframe full_df : {full_df.columns.tolist()}\")\n",
    "# print(full_df[[\"clean_title_body\"]].head(3))\n",
    "\n",
    "# --- A. CHARGEMENT DES VECTEURS\n",
    "X_bow   = load_npz(\"models/bow/X_bow_full.npz\")\n",
    "X_tfidf = load_npz(\"models/tfidf/X_tfidf_full.npz\")\n",
    "X_svd   = np.load(\"models/svd/X_titlebody_svd10k.npy\")\n",
    "X_w2v   = np.load(\"models/w2v/X_w2v_full.npy\")\n",
    "X_use   = np.load(\"models/use/embeddings_use_full.npy\")\n",
    "X_sbert = np.load(\"models/sbert/embeddings_sbert_full.npy\")\n",
    "\n",
    "print(\"# --- DIMENSIONS DES VECTEURS :\")\n",
    "for name, mat in [(\"BoW\", X_bow), (\"TF-IDF\", X_tfidf), (\"SVD\", X_svd),\n",
    "                  (\"Word2Vec\", X_w2v), (\"USE\", X_use), (\"SBERT\", X_sbert)]:\n",
    "    print(f\"# --- {name:<10}: {mat.shape}\")\n",
    "\n",
    "\n",
    "# --- B. CHARGEMENT DES LABELS MULTILABEL\n",
    "mlb = joblib.load(\"models/tags/multilabel_binarizer_full.pkl\")\n",
    "Y_full = np.load(\"models/tags/y_tags_full.npy\")\n",
    "\n",
    "print(f\"# --- Labels multilabel charg√©s : {Y_full.shape}\")\n",
    "print(f\"# --- Nombre de tags avant filtrage : {len(mlb.classes_)}\")\n",
    "\n",
    "tag_counts = Y_full.sum(axis=0)\n",
    "tag_mask = tag_counts >= 1  # seuil de raret√©\n",
    "Y_full_filtered = Y_full[:, tag_mask]\n",
    "\n",
    "mlb_filtered = mlb\n",
    "mlb_filtered.classes_ = np.array(mlb.classes_)[tag_mask]\n",
    "print(f\"# --- TAGS conserv√©s apr√®s filtrage : {len(mlb_filtered.classes_)}\")\n",
    "print(mlb_filtered.classes_.tolist())\n",
    "print(\"java\" in mlb_filtered.classes_)    # True ou False\n",
    "print(\"python\" in mlb_filtered.classes_)  # True ou False\n",
    "tags_name = mlb.classes_\n",
    "for tag in [\"java\", \"python\"]:\n",
    "    idx = list(tags_name).index(tag)\n",
    "    print(f\"{tag} count: {tag_counts[idx]}\")\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "tag_freq = Counter({tag: tag_counts[i] for i, tag in enumerate(tags_name)})\n",
    "print(tag_freq[\"asp.net-core-mvc\"])  # ‚Üí combien d‚Äôoccurrences ?\n",
    "\n",
    "# Top 10 tags les plus fr√©quents\n",
    "top_tags = tag_freq.most_common(10)\n",
    "print(\"üîù Tags dominants :\", top_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0433e4e",
   "metadata": {},
   "source": [
    "### 2.2. DEFINITION DES VARIABLES VECTEURS A ENTRAINER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbd81fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES DE MODELISATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import src.tags_suggester.modeling.modeling as mdl\n",
    "importlib.reload(mdl)\n",
    "# ----------------------------------------------\n",
    "# --- 2. S√âPARATION DES FEATURES ET DE LA CIBLE\n",
    "# ----------------------------------------------\n",
    "Y = Y_full_filtered\n",
    "# X_text = full_df[\"clean_title_body\"]\n",
    "# --- DEBUT IMPACT DIFFERENCIATION PRE TRAITEMENT SELON TYPE TRANSFORMATIONS\n",
    "import pandas as pd\n",
    "# --- POUR TFIDF/SVD/BOW\n",
    "X_text = pd.read_csv(\n",
    "    \"data/processed/full_explo_wo.csv.gz\",\n",
    "    usecols=[\"clean_title_body\"],\n",
    "    dtype={\"clean_title_body\": \"str\"}\n",
    ")[\"clean_title_body\"]\n",
    "# --- POUR SBERT/USE\n",
    "X_text_embed = pd.read_csv(\n",
    "    \"data/processed/full_explo_wo.csv.gz\",\n",
    "    usecols=[\"clean_title_body_embed\"],\n",
    "    dtype={\"clean_title_body_embed\": \"str\"}\n",
    ")[\"clean_title_body_embed\"]\n",
    "# --- POUR WORD2VEC\n",
    "X_text_w2v = pd.read_csv(\n",
    "    \"data/processed/full_explo_wo.csv.gz\",\n",
    "    usecols=[\"clean_title_body_w2v\"],\n",
    "    dtype={\"clean_title_body_w2v\": \"str\"}\n",
    ")[\"clean_title_body_w2v\"]\n",
    "\n",
    "X_text_dict = {\n",
    "    \"bow\": X_text,\n",
    "    \"tfidf\": X_text,\n",
    "    \"svd\": X_text,\n",
    "    \"w2v\": X_text_w2v,\n",
    "    \"use\": X_text_embed,\n",
    "    \"sbert\": X_text_embed\n",
    "}\n",
    "\n",
    "# --- FIN IMPACT DIFFERENCIATION PRE TRAITEMENT SELON TYPE TRANSFORMATIONS\n",
    "print(f\"# --- Matrice multilabel (Y) : {Y.shape}\")\n",
    "print(f\"# --- Colonne textuelle (X) : {X_text.shape}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# --- 3. CR√âATION DES VECTEURS + DICTS DE SUPPORT\n",
    "# ---------------------------------------------------------\n",
    "X_dict = {\n",
    "    \"bow\": X_bow,\n",
    "    \"tfidf\": X_tfidf,\n",
    "    \"svd\": X_svd,\n",
    "    \"w2v\": X_w2v,\n",
    "    \"use\": X_use,\n",
    "    \"sbert\": X_sbert\n",
    "}\n",
    "\n",
    "preproc_dict = {\n",
    "    \"bow\": None,\n",
    "    \"tfidf\": None,\n",
    "    \"svd\": None,\n",
    "    \"w2v\": \"scale\",\n",
    "    \"use\": \"scale\",\n",
    "    \"sbert\": \"scale\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ee6ad5",
   "metadata": {},
   "source": [
    "### 2.3. DIVISION TRAIN/ TEST POUR CHAQUE VECTEUR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63e6bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------\n",
    "# --- OBSOLETE\n",
    "# --------------\n",
    "# ---------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES DE MODELISATION\n",
    "# ---------------------------------------------\n",
    "import importlib\n",
    "import src.tags_suggester.modeling.modeling as mdl\n",
    "importlib.reload(mdl)\n",
    "# ---------------------------------------------------------\n",
    "# --- 4. DIVISION EN TRAIN / TEST SUR CHAQUE VECTEUR\n",
    "# ---------------------------------------------------------\n",
    "indices = np.arange(Y.shape[0])\n",
    "train_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=42)\n",
    "\n",
    "splits_dict = {}\n",
    "for name, X in X_dict.items():\n",
    "    X_train, X_test, y_train, y_test = mdl.split_on_indices(X, Y, train_idx, test_idx)\n",
    "    splits_dict[name] = (X_train, X_test)\n",
    "\n",
    "print(f\"# --- Splits pr√™ts pour vecteurs : {list(splits_dict.keys())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a6bd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES DE MODELISATION\n",
    "# ---------------------------------------------\n",
    "import importlib\n",
    "import src.tags_suggester.modeling.modeling as mdl\n",
    "importlib.reload(mdl)\n",
    "\n",
    "indices = np.arange(Y.shape[0])\n",
    "train_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=42)\n",
    "splits_dict = {}\n",
    "\n",
    "for name, X in X_dict.items():\n",
    "    # R√©cup√©ration du texte associ√©\n",
    "    X_text_current = X_text_dict[name]\n",
    "    \n",
    "    # Split des vecteurs et des labels\n",
    "    X_train, X_test, y_train, y_test = mdl.split_on_indices_custom(X, Y, train_idx, test_idx)\n",
    "    \n",
    "    # Split du texte\n",
    "    X_text_train = X_text_current.iloc[train_idx].reset_index(drop=True)\n",
    "    X_text_test  = X_text_current.iloc[test_idx].reset_index(drop=True)\n",
    "    \n",
    "    # Stockage dans le dictionnaire\n",
    "    splits_dict[name] = {\n",
    "        \"X_train\": X_train,\n",
    "        \"X_test\": X_test,\n",
    "        \"y_train\": y_train,\n",
    "        \"y_test\": y_test,\n",
    "        \"X_text_train\": X_text_train,\n",
    "        \"X_text_test\": X_text_test\n",
    "    }\n",
    "\n",
    "print(f\"# --- Splits enrichis pour vecteurs : {list(splits_dict.keys())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612c2165",
   "metadata": {},
   "source": [
    "### 2.4. ENTRAINEMENT DU MODELE POUR CHAQUE VECTEURS - TRACKING - SAUVEGARDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ba5fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- NOUVELLE SOLUTION --- DEVENUE OBSOLETE\n",
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES DE MODELISATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import src.tags_suggester.modeling.modeling as mdl\n",
    "importlib.reload(mdl)\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "# Ignorer seulement le warning sp√©cifique sur les labels absents\n",
    "warnings.filterwarnings(\"ignore\", message=\"Label not .* is present in all training examples\")\n",
    "\n",
    "df_results = []\n",
    "trained_models_dict = {}  # üß† Dictionnaire des mod√®les entra√Æn√©s\n",
    "uri = \"file:///D:/machine_learning_training/openclassrooms_projects/05_categorisez_automatiquement_question/mlruns\"\n",
    "mlflow.set_tracking_uri(uri)\n",
    "mlflow.set_experiment(\"logreg_stackoverflow\")\n",
    "notebook_path = Path().resolve()\n",
    "base_path = notebook_path.parents[0]\n",
    "\n",
    "for name, (X_train, X_test) in splits_dict.items():\n",
    "    preproc = preproc_dict.get(name)\n",
    "\n",
    "    with mlflow.start_run(run_name=f\"{model_type}_{name}\"):\n",
    "        mlflow.log_param(\"model_type\", model_type)\n",
    "        mlflow.log_param(\"vecteur\", name)\n",
    "        mlflow.log_param(\"preprocessing\", preproc)\n",
    "\n",
    "        # üß™ Entra√Ænement + pr√©diction\n",
    "        scores = mdl.train_and_score_vector_full_metrics(\n",
    "            name=name,\n",
    "            X_train=X_train,\n",
    "            X_test=X_test,\n",
    "            y_train=y_train,\n",
    "            y_test=y_test,\n",
    "            model_class=model_class,\n",
    "            model_wrapper=model_wrapper,\n",
    "            preprocess=preproc\n",
    "        )\n",
    "\n",
    "        # üìä Logging des m√©triques\n",
    "        mlflow.log_metric(\"f1_micro\", scores[\"f1_micro\"])\n",
    "        mlflow.log_metric(\"hamming_loss\", scores[\"hamming_loss\"])\n",
    "        mlflow.log_metric(\"coverage_score\", scores[\"coverage_tags\"])\n",
    "        mlflow.log_metric(\"f1_macro\", scores[\"f1_macro\"])\n",
    "        mlflow.log_metric(\"precision_micro\", scores[\"precision_micro\"])\n",
    "        mlflow.log_metric(\"recall_micro\", scores[\"recall_micro\"])\n",
    "\n",
    "        # --- DEFINITION EMPLACEMENT SOUHAITE DE SAUVEGARDE LOCALE DU MODELE\n",
    "        path_model = f\"models/logreg/logreg_{name}.joblib\"\n",
    "        os.makedirs(os.path.dirname(path_model), exist_ok=True)\n",
    "\n",
    "        # üíæ SAUVEGARDE DU MODELE EN LOCAL\n",
    "        joblib.dump(scores[\"model\"], path_model)\n",
    "        mlflow.log_artifact(path_model)\n",
    "        # --- SAUVEGARDE DANS UN EMPLACEMENT ACCESSIBLE A L'API\n",
    "        api_model_path = base_path / \"src\" / \"tags_suggester\" / \"api\" / \"models\" / name\n",
    "        # api_model_path.mkdir(parents=True, exist_ok=True)\n",
    "        # üì• Stockage dans le tableau de r√©sultats\n",
    "        df_results.append({\n",
    "            \"vecteur\": name,\n",
    "            \"f1_micro\": round(scores[\"f1_micro\"], 3),\n",
    "            \"f1_macro\": round(scores[\"f1_macro\"], 3),\n",
    "            \"precision_micro\": round(scores[\"precision_micro\"], 3),\n",
    "            \"recall_micro\": round(scores[\"recall_micro\"], 3),\n",
    "            \"hamming_loss\": round(scores[\"hamming_loss\"], 4),\n",
    "            \"coverage_tags\": round(scores[\"coverage_tags\"], 4)\n",
    "        })\n",
    "\n",
    "        # üîÑ Remplissage du dictionnaire des mod√®les entra√Æn√©s\n",
    "        trained_models_dict[name] = scores[\"model\"]\n",
    "        print(f\"üì¶ Mod√®le '{name}' stock√© dans le dict : {type(scores['model'])}\")\n",
    "\n",
    "\n",
    "print(f\"üß† Dictionnaire final : {list(trained_models_dict.keys())}\")\n",
    "\n",
    "# üßæ Cr√©ation du dataframe final\n",
    "df_results = pd.DataFrame(df_results)\n",
    "display(df_results.sort_values(\"f1_micro\", ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a706609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES DE MODELISATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import src.tags_suggester.modeling.modeling as mdl\n",
    "importlib.reload(mdl)\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "# Ignorer seulement le warning sp√©cifique sur les labels absents\n",
    "warnings.filterwarnings(\"ignore\", message=\"Label not .* is present in all training examples\")\n",
    "\n",
    "df_results = []\n",
    "trained_models_dict = {}  # üß† Dictionnaire des mod√®les entra√Æn√©s\n",
    "uri = \"file:///D:/machine_learning_training/openclassrooms_projects/05_categorisez_automatiquement_question/mlruns\"\n",
    "mlflow.set_tracking_uri(uri)\n",
    "mlflow.set_experiment(\"logreg_stackoverflow\")\n",
    "notebook_path = Path().resolve()\n",
    "base_path = notebook_path.parents[0]\n",
    "\n",
    "for name, split in splits_dict.items():\n",
    "    preproc = preproc_dict.get(name)\n",
    "\n",
    "    with mlflow.start_run(run_name=f\"{model_type}_{name}\"):\n",
    "        mlflow.log_param(\"model_type\", model_type)\n",
    "        mlflow.log_param(\"vecteur\", name)\n",
    "        mlflow.log_param(\"preprocessing\", preproc)\n",
    "\n",
    "        # üß™ Entra√Ænement + pr√©diction avec les bons textes\n",
    "        scores = mdl.train_and_score_vector_full_metrics_custom(\n",
    "            name=name,\n",
    "            X_train=split[\"X_train\"],\n",
    "            X_test=split[\"X_test\"],\n",
    "            y_train=split[\"y_train\"],\n",
    "            y_test=split[\"y_test\"],\n",
    "            model_class=model_class,\n",
    "            model_wrapper=model_wrapper,\n",
    "            preprocess=preproc,\n",
    "            X_text_train=split[\"X_text_train\"],\n",
    "            X_text_test=split[\"X_text_test\"]\n",
    "        )\n",
    "\n",
    "        # üìä Logging des m√©triques\n",
    "        mlflow.log_metric(\"f1_micro\", scores[\"f1_micro\"])\n",
    "        mlflow.log_metric(\"hamming_loss\", scores[\"hamming_loss\"])\n",
    "        mlflow.log_metric(\"coverage_score\", scores[\"coverage_tags\"])\n",
    "        mlflow.log_metric(\"f1_macro\", scores[\"f1_macro\"])\n",
    "        mlflow.log_metric(\"precision_micro\", scores[\"precision_micro\"])\n",
    "        mlflow.log_metric(\"recall_micro\", scores[\"recall_micro\"])\n",
    "\n",
    "        # üíæ Sauvegarde du mod√®le\n",
    "        path_model = f\"models/logreg/logreg_{name}.joblib\"\n",
    "        os.makedirs(os.path.dirname(path_model), exist_ok=True)\n",
    "        joblib.dump(scores[\"model\"], path_model)\n",
    "        mlflow.log_artifact(path_model)\n",
    "\n",
    "        # üì• Stockage des r√©sultats\n",
    "        df_results.append({\n",
    "            \"vecteur\": name,\n",
    "            \"f1_micro\": round(scores[\"f1_micro\"], 3),\n",
    "            \"f1_macro\": round(scores[\"f1_macro\"], 3),\n",
    "            \"precision_micro\": round(scores[\"precision_micro\"], 3),\n",
    "            \"recall_micro\": round(scores[\"recall_micro\"], 3),\n",
    "            \"hamming_loss\": round(scores[\"hamming_loss\"], 4),\n",
    "            \"coverage_tags\": round(scores[\"coverage_tags\"], 4)\n",
    "        })\n",
    "\n",
    "        trained_models_dict[name] = scores[\"model\"]\n",
    "        print(f\"üì¶ Mod√®le '{name}' stock√© dans le dict : {type(scores['model'])}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"üß† Dictionnaire final : {list(trained_models_dict.keys())}\")\n",
    "\n",
    "# üßæ Cr√©ation du dataframe final\n",
    "df_results = pd.DataFrame(df_results)\n",
    "display(df_results.sort_values(\"f1_micro\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae76243",
   "metadata": {},
   "source": [
    "### 2.5. SAUVEGARDE DU MEILLLEUR MODELE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a3794d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- NOUVELLE SOLUTION 2\n",
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES DE MODELISATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import src.tags_suggester.modeling.modeling as mdl\n",
    "importlib.reload(mdl)\n",
    "import json\n",
    "print(df_results.columns)\n",
    "import json\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# --- IDENTIFICATION DU MEILLEUR MODELE SELON LE SCORE F1 MICRO\n",
    "# ----------------------------------------------------------------\n",
    "best_row = df_results.sort_values(\"f1_micro\", ascending=False).iloc[0]\n",
    "print(f\"# --- LA LIGNE DE MEILLEURE f1 micro EST : {best_row}\")\n",
    "best_vect = best_row[\"vecteur\"]\n",
    "print(f\"# --- LE VECTEUR PRESENTANT LE MEILLEUT f1 micro EST : {best_vect}\")\n",
    "best_model = trained_models_dict[best_vect]\n",
    "print(f\"# --- LE MODELE DE REGRESSION LOGISTIQUE ASSOCIE A CE MEILLEUR VECTEUR EST : {best_model}\")\n",
    "\n",
    "# üìÇ D√©finir le chemin du dossier API correspondant\n",
    "api_model_dir = base_path / \"src\" / \"tags_suggester\" / \"api\" / \"models\" / best_vect\n",
    "# TODO JE VEUX COPIER  DANS api_model_dir LE FICHIER logreg_{best_vect}.joblib QUI SE TROUVE DANS models/best_vect\n",
    "import shutil\n",
    "# üìÇ D√©finir le chemin source et destination avec Path\n",
    "source_path = base_path / \"notebooks\" / \"models\" / \"logreg\" / f\"logreg_{best_vect}.joblib\"\n",
    "destination_path = api_model_dir / f\"logreg_{best_vect}.joblib\"\n",
    "\n",
    "# üìÅ Cr√©er le dossier destination s‚Äôil n‚Äôexiste pas\n",
    "api_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# üîÑ Copier le fichier\n",
    "if source_path.exists():\n",
    "    shutil.copy2(source_path, destination_path)\n",
    "    print(f\"‚úÖ Mod√®le copi√© de {source_path} vers {destination_path}\")\n",
    "else:\n",
    "    print(f\"üö´ Fichier source introuvable : {source_path}\")\n",
    "\n",
    "# FIN TODO \n",
    "\n",
    "# üìå MultilabelBinarizer \n",
    "mlb_path = base_path / \"notebooks\" /  \"models\" / \"tags\" / \"multilabel_binarizer_full.pkl\"\n",
    "mlb_exists = mlb_path.exists()\n",
    "\n",
    "\n",
    "api_model_dir = base_path / \"src\" / \"tags_suggester\" / \"api\" / \"models\" / best_vect\n",
    "model_path = api_model_dir / f\"logreg_{best_vect}.joblib\"\n",
    "print(f\"# --- LE CHEMIN DU MEILLEUR MODELE DE REGRESSION LOGISTIQUE EST : {model_path}\")\n",
    "# üìå Chemin vers le transformateur\n",
    "if best_vect == \"sbert\":\n",
    "    vectorizer_path_api = api_model_dir / \"sbert_model\"\n",
    "    vectorizer_path = base_path / \"notebooks\" / \"models\" / \"sbert\" / \"sbert_model\"\n",
    "elif best_vect == \"use\":\n",
    "    vectorizer_path_api = api_model_dir / \"use_path.json\"\n",
    "    vectorizer_path = base_path / \"notebooks\" / \"models\" / \"use\" / \"use_path.json\"\n",
    "elif best_vect in [\"word2vec\", \"w2v\"]:\n",
    "    vectorizer_path_api = api_model_dir / \"w2v_titlebody_full.bin\"\n",
    "    vectorizer_path = base_path / \"notebooks\" / \"models\" / \"w2v\" / \"w2v_titlebody_full.bin\"\n",
    "elif best_vect == \"bow\":\n",
    "    vectorizer_path_api = api_model_dir / \"vectorizer_bow_full.pkl\"\n",
    "    vectorizer_path = base_path / \"notebooks\" / \"models\" / \"bow\" / \"vectorizer_bow_full.pkl\"\n",
    "elif best_vect == \"tfidf\":\n",
    "    vectorizer_path_api = api_model_dir / \"tfidf_vectorizer_titlebody.joblib\"\n",
    "    vectorizer_path = base_path / \"notebooks\" / \"models\" / \"tfidf\" / \"tfidf_vectorizer_titlebody.joblib\"\n",
    "elif best_vect == \"svd\":\n",
    "    vectorizer_path_api = api_model_dir / \"tfidf_vectorizer_titlebody.joblib\"\n",
    "    vectorizer_path = base_path / \"notebooks\" / \"models\" / \"tfidf\" / \"tfidf_vectorizer_titlebody.joblib\"\n",
    "    svd_path_api = api_model_dir / \"svd_model_titlebody.joblib\"\n",
    "    svd_path = base_path / \"notebooks\" / \"models\" / \"svd\" / \"svd_model_titlebody.joblib\"\n",
    "    shutil.copy2(svd_path, svd_path_api)\n",
    "    print(f\"‚úÖ Mod√®le SVD copi√© de {svd_path} vers {svd_path_api}\")\n",
    "else:\n",
    "    raise ValueError(f\"üö´ Type de vecteur inconnu : {best_vect}\")\n",
    "\n",
    "# --- TODO SAUVEGARDE DANS API DU VECTORIZER - ET DU MultilabelBinarizer\n",
    "# üì¶ Copie du vectorizer\n",
    "if best_vect == \"sbert\":\n",
    "    if vectorizer_path.exists():\n",
    "        if vectorizer_path_api.exists():\n",
    "            shutil.rmtree(vectorizer_path_api)  # Supprime l'ancien dossier s'il existe\n",
    "        shutil.copytree(vectorizer_path, vectorizer_path_api)\n",
    "        print(f\"‚úÖ Dossier SBERT copi√© de {vectorizer_path} vers {vectorizer_path_api}\")\n",
    "    else:\n",
    "        print(f\"üö´ Dossier SBERT introuvable : {vectorizer_path}\")\n",
    "else:\n",
    "    # üì¶ Copie des autres vectorizers (fichiers)\n",
    "    if vectorizer_path.exists():\n",
    "        shutil.copy2(vectorizer_path, vectorizer_path_api)\n",
    "        print(f\"‚úÖ Vectorizer copi√© de {vectorizer_path} vers {vectorizer_path_api}\")\n",
    "    else:\n",
    "        print(f\"üö´ Fichier vectorizer introuvable : {vectorizer_path}\")\n",
    "\n",
    "\n",
    "\n",
    "api_model_dir_config = base_path / \"src\" / \"tags_suggester\" / \"api\" / \"models\"\n",
    "# üì¶ Copie du MultilabelBinarizer\n",
    "mlb_path_api = api_model_dir_config / \"tags\" / \"multilabel_binarizer_full.pkl\"\n",
    "mlb_path_api.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if mlb_path.exists():\n",
    "    shutil.copy2(mlb_path, mlb_path_api)\n",
    "    print(f\"‚úÖ MultilabelBinarizer copi√© de {mlb_path} vers {mlb_path_api}\")\n",
    "else:\n",
    "    print(f\"üö´ Fichier MultilabelBinarizer introuvable : {mlb_path}\")\n",
    "\n",
    "# --- FIN TODO\n",
    "\n",
    "# # üìÑ Construire le dictionnaire de config\n",
    "config = {\n",
    "    \"vectorizer\": best_vect,\n",
    "    \"model_path\": (Path(\"models\") / best_vect / f\"logreg_{best_vect}.joblib\").as_posix(),\n",
    "    \"vectorizer_path\": (Path(\"models\") / best_vect / vectorizer_path_api.name).as_posix(),\n",
    "    \"mlb_path\": (Path(\"models\") / \"tags\" / \"multilabel_binarizer_full.pkl\").as_posix() if mlb_exists else None\n",
    "}\n",
    "if best_vect == \"svd\":\n",
    "    config[\"svd_path\"] = (Path(\"models\") / best_vect / svd_path_api.name).as_posix()\n",
    "\n",
    "\n",
    "# üíæ Sauvegarde du fichier config : FONCTIONNEMENT A CONSERVER\n",
    "config_file = api_model_dir_config / \"config_best_model.json\"\n",
    "with open(config_file, \"w\") as f:\n",
    "    json.dump(config, f, indent=4)\n",
    "\n",
    "print(f\"‚úÖ Fichier config_best_model.json sauvegard√© dans : {config_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399f6e9f",
   "metadata": {},
   "source": [
    "### 2.6. LOG MLFLOW DES M√âTRIQUES DANS UN TABLEAU COMPARATIF FINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866f6704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES DE MODELISATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import src.tags_suggester.modeling.modeling as mdl\n",
    "importlib.reload(mdl)\n",
    "\n",
    "# -----------------------------------------\n",
    "# --- ARRONDI A 3 CHIFFRES POUR LISIBILITE\n",
    "# -----------------------------------------\n",
    "df_results[\n",
    "    [\"f1_micro\", \"f1_macro\", \"precision_micro\", \"recall_micro\", \"hamming_loss\", \"coverage_tags\"]\n",
    "    ] = df_results[\n",
    "    [\"f1_micro\", \"f1_macro\", \"precision_micro\", \"recall_micro\", \"hamming_loss\", \"coverage_tags\"]\n",
    "].round(3)\n",
    "\n",
    "# --------------------------------------\n",
    "# --- LOG MLFLOW DU TABLEAU COMPARATIF \n",
    "# --------------------------------------\n",
    "model_directory = \"models/logreg\"\n",
    "os.makedirs(model_directory, exist_ok=True)\n",
    "mlflow.end_run()\n",
    "with mlflow.start_run(run_name=\"comparatif_vecteurs_final\"):\n",
    "    csv_path = f\"{model_directory}/comparatif_vecteurs.csv\"\n",
    "    df_results.to_csv(csv_path, index=False)\n",
    "    mlflow.log_artifact(csv_path)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# --- AFFICHAGE DE CE TABLEAU DANS LE NOTEBOOK\n",
    "# ----------------------------------------------\n",
    "display(df_results.sort_values(\"f1_micro\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b567324",
   "metadata": {},
   "source": [
    "### 2.7 VISUALISATION DES METRIQUES DANS MLFLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e22b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES DE MODELISATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import src.tags_suggester.modeling.modeling as mdl\n",
    "importlib.reload(mdl)\n",
    "# ---------------------------------------------------------------------\n",
    "# --- POUR SAUVEGARDES LOCALES DES BARPLOTS SEABORN LOGUES DANS MLFLOW\n",
    "# ---------------------------------------------------------------------\n",
    "img_model_directory = f\"{model_directory}/mlflow_images\"\n",
    "os.makedirs(img_model_directory, exist_ok=True)\n",
    "# --------------------------------------------------\n",
    "# --- BARPLOT COUVERTURE DES TAGS LOGUE DANS MLFLOW\n",
    "# --------------------------------------------------\n",
    "mlflow.end_run()\n",
    "with mlflow.start_run(run_name=\"comparatif_coverage_barplot\", nested=True):\n",
    "    mdl.plot_and_log_barplot(\n",
    "        df_scores=df_results,\n",
    "        metric=\"coverage_tags\",\n",
    "        title=\"Couverture des tags par vecteur\",\n",
    "        save_path=f\"{img_model_directory}/barplot_coverage_tags.png\"\n",
    "    )\n",
    "# --------------------------------------------------\n",
    "# --- BARPLOT SCORE F1 LOGUE DANS MLFLOW\n",
    "# --------------------------------------------------\n",
    "mlflow.end_run()\n",
    "with mlflow.start_run(run_name=\"comparatif_f1_micro_barplot\", nested=True):\n",
    "    mdl.plot_and_log_barplot(\n",
    "        df_scores=df_results,\n",
    "        metric=\"f1_micro\",\n",
    "        title=\"Score F1 (entra√Ænement classique) par vecteur\",\n",
    "        save_path=f\"{img_model_directory}/barplot_f1_micro.png\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "StackOverFlowTags (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
