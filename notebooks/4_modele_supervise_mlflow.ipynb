{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c32993ea",
   "metadata": {},
   "source": [
    "## INTRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7aedb4f",
   "metadata": {},
   "source": [
    "Apr√®s avoir nettoy√©, explor√© et vectoris√© les donn√©es textuelles, cette section vise √† appliquer des techniques de mod√©lisation pour extraire de l'information ou pr√©dire les tags associ√©s aux questions Stack Overflow.\n",
    "\n",
    "Nous distinguons deux approches compl√©mentaires :\n",
    "\n",
    "- **Mod√©lisation non supervis√©e** : pour explorer les th√©matiques latentes du corpus sans utiliser les tags\n",
    "- **Mod√©lisation supervis√©e** : pour entra√Æner un mod√®le capable de pr√©dire automatiquement les tags √† partir du texte\n",
    "\n",
    "Chaque approche sera test√©e sur les repr√©sentations vectorielles construites pr√©c√©demment (TF-IDF, embeddings, CouterVectorizer), et √©valu√©e selon des crit√®res adapt√©s √† la t√¢che.\n",
    "\n",
    "Les mod√©lisations seront r√©alis√©es dans deux notebooks s√©par√©s :\n",
    "- `2_modelisation_non_supervisee.ipynb`\n",
    "- `3_modelisation_supervisee.ipynb`\n",
    "\n",
    "\n",
    "\n",
    "La mod√©lisation supervis√©e vise √† pr√©dire automatiquement les tags associ√©s √† une question √† partir de son texte.  \n",
    "Il s‚Äôagit d‚Äôun probl√®me de **classification multi-label**, o√π chaque observation (question) peut √™tre associ√©e √† plusieurs classes (tags).\n",
    "\n",
    "Dans cette sous-section, nous testons plusieurs mod√®les adapt√©s √† cette t√¢che, tels que :\n",
    "\n",
    "- **Logistic Regression** avec strat√©gie One-vs-Rest\n",
    "- **Random Forest**, **SVM**, ou **XGBoost**\n",
    "- **R√©seaux de neurones** (optionnel)\n",
    "\n",
    "Les performances seront √©valu√©es √† l‚Äôaide de m√©triques sp√©cifiques au multi-label (F1-score, pr√©cision, rappel, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4f4de5",
   "metadata": {},
   "source": [
    "## 1. IMPORTS  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9921b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Standard\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# üìä Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# üî¨ Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import f1_score, hamming_loss, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# üß™ Traitement conditionnel\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# üíæ Persistance / outils externes\n",
    "import joblib\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# üóÉÔ∏è Utilitaires\n",
    "from scipy.sparse import load_npz\n",
    "from tqdm.notebook import tqdm  # ou simplement tqdm si console\n",
    "\n",
    "# üí° Tracking exp√©rimental (optionnel)\n",
    "import mlflow\n",
    "import mlflow.sklearn  # Pour les mod√®les scikit-learn\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Ajoute le dossier parent du notebook aux chemins Python\n",
    "project_root = os.path.abspath(\"..\")  # ou \"../..\" selon ton niveau\n",
    "sys.path.append(project_root)\n",
    "# üìÅ Modules personnalis√©s\n",
    "import importlib\n",
    "import src.modeling.modeling as mdl\n",
    "importlib.reload(mdl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d084e2",
   "metadata": {},
   "source": [
    "## 2. MODELISATION : LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5366b0ab",
   "metadata": {},
   "source": [
    "### 2.1. CHARGEMENT DES FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021eed5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES DE MODELISATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import src.modeling.modeling as mdl\n",
    "importlib.reload(mdl)\n",
    "\n",
    "# -----------------------------------------\n",
    "# --- 0. CONFIGURATION DU MOD√àLE √Ä TESTER\n",
    "# -----------------------------------------\n",
    "# ‚úÖ Tu peux changer ces lignes pour benchmarker un autre mod√®le\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "model_type = \"logreg\"\n",
    "model_class = LogisticRegression\n",
    "model_wrapper = OneVsRestClassifier  # ou ClassifierChain ou None\n",
    "\n",
    "# ------------------------------\n",
    "# --- 1. SELECTION DES FEATURES\n",
    "# ------------------------------\n",
    "full_df = pd.read_parquet(\"data/processed/full_explo_wo.parquet\")\n",
    "print(f\"Dimensions du dataframe full_df : {full_df.shape}\")\n",
    "print(f\" Colonnes du dataframe full_df : {full_df.columns.tolist()}\")\n",
    "print(full_df[[\"clean_title_body\"]].head(3))\n",
    "\n",
    "# --- A. CHARGEMENT DES VECTEURS\n",
    "X_bow   = load_npz(\"models/bow/X_bow_full.npz\")\n",
    "X_tfidf = load_npz(\"models/tfidf/X_tfidf_full.npz\")\n",
    "X_svd   = np.load(\"models/svd/X_titlebody_svd10k.npy\")\n",
    "X_w2v   = np.load(\"models/word2vec/X_w2v_full.npy\")\n",
    "X_use   = np.load(\"models/use_model/embeddings_use_full.npy\")\n",
    "X_sbert = np.load(\"models/sbert/embeddings_sbert_full.npy\")\n",
    "\n",
    "print(\"# --- DIMENSIONS DES VECTEURS :\")\n",
    "for name, mat in [(\"BoW\", X_bow), (\"TF-IDF\", X_tfidf), (\"SVD\", X_svd),\n",
    "                  (\"Word2Vec\", X_w2v), (\"USE\", X_use), (\"SBERT\", X_sbert)]:\n",
    "    print(f\"# --- {name:<10}: {mat.shape}\")\n",
    "\n",
    "\n",
    "# --- B. CHARGEMENT DES LABELS MULTILABEL\n",
    "mlb = joblib.load(\"models/tags/multilabel_binarizer_full.pkl\")\n",
    "Y_full = np.load(\"models/tags/y_tags_full.npy\")\n",
    "\n",
    "print(f\"# --- Labels multilabel charg√©s : {Y_full.shape}\")\n",
    "print(f\"# --- Nombre de tags avant filtrage : {len(mlb.classes_)}\")\n",
    "\n",
    "tag_counts = Y_full.sum(axis=0)\n",
    "tag_mask = tag_counts >= 1  # seuil de raret√©\n",
    "Y_full_filtered = Y_full[:, tag_mask]\n",
    "\n",
    "mlb_filtered = mlb\n",
    "mlb_filtered.classes_ = np.array(mlb.classes_)[tag_mask]\n",
    "print(f\"# --- TAGS conserv√©s apr√®s filtrage : {len(mlb_filtered.classes_)}\")\n",
    "print(mlb_filtered.classes_.tolist())\n",
    "print(\"java\" in mlb_filtered.classes_)    # True ou False\n",
    "print(\"python\" in mlb_filtered.classes_)  # True ou False\n",
    "tags_name = mlb.classes_\n",
    "for tag in [\"java\", \"python\"]:\n",
    "    idx = list(tags_name).index(tag)\n",
    "    print(f\"{tag} count: {tag_counts[idx]}\")\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "tag_freq = Counter({tag: tag_counts[i] for i, tag in enumerate(tags_name)})\n",
    "print(tag_freq[\"asp.net-core-mvc\"])  # ‚Üí combien d‚Äôoccurrences ?\n",
    "\n",
    "# Top 10 tags les plus fr√©quents\n",
    "top_tags = tag_freq.most_common(10)\n",
    "print(\"üîù Tags dominants :\", top_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0433e4e",
   "metadata": {},
   "source": [
    "### 2.2. DEFINITION DES VARIABLES VECTEURS A ENTRAINER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbd81fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES DE MODELISATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import src.modeling.modeling as mdl\n",
    "importlib.reload(mdl)\n",
    "# ----------------------------------------------\n",
    "# --- 2. S√âPARATION DES FEATURES ET DE LA CIBLE\n",
    "# ----------------------------------------------\n",
    "Y = Y_full_filtered\n",
    "X_text = full_df[\"clean_title_body\"]\n",
    "print(f\"# --- Matrice multilabel (Y) : {Y.shape}\")\n",
    "print(f\"# --- Colonne textuelle (X) : {X_text.shape}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# --- 3. CR√âATION DES VECTEURS + DICTS DE SUPPORT\n",
    "# ---------------------------------------------------------\n",
    "X_dict = {\n",
    "    \"bow\": X_bow,\n",
    "    \"tfidf\": X_tfidf,\n",
    "    \"svd\": X_svd,\n",
    "    \"w2v\": X_w2v,\n",
    "    \"use\": X_use,\n",
    "    \"sbert\": X_sbert\n",
    "}\n",
    "\n",
    "preproc_dict = {\n",
    "    \"bow\": None,\n",
    "    \"tfidf\": None,\n",
    "    \"svd\": None,\n",
    "    \"w2v\": \"scale\",\n",
    "    \"use\": \"scale\",\n",
    "    \"sbert\": \"scale\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ee6ad5",
   "metadata": {},
   "source": [
    "### 2.3. DIVISION TRAIN/ TEST POUR CHAQUE VECTEUR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63e6bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES DE MODELISATION\n",
    "# ---------------------------------------------\n",
    "import importlib\n",
    "import src.modeling.modeling as mdl\n",
    "importlib.reload(mdl)\n",
    "# ---------------------------------------------------------\n",
    "# --- 4. DIVISION EN TRAIN / TEST SUR CHAQUE VECTEUR\n",
    "# ---------------------------------------------------------\n",
    "indices = np.arange(Y.shape[0])\n",
    "train_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=42)\n",
    "\n",
    "splits_dict = {}\n",
    "for name, X in X_dict.items():\n",
    "    X_train, X_test, y_train, y_test = mdl.split_on_indices(X, Y, train_idx, test_idx)\n",
    "    splits_dict[name] = (X_train, X_test)\n",
    "\n",
    "print(f\"# --- Splits pr√™ts pour vecteurs : {list(splits_dict.keys())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612c2165",
   "metadata": {},
   "source": [
    "### 2.4. ENTRAINEMENT DU MODELE POUR CHAQUE VECTEURS - RESULTATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23f202d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES DE MODELISATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import src.modeling.modeling as mdl\n",
    "importlib.reload(mdl)\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "df_results = []\n",
    "uri = \"file:///D:/machine_learning_training/openclassrooms_projects/05_categorisez_automatiquement_question/mlruns\"\n",
    "mlflow.set_tracking_uri(uri)\n",
    "mlflow.set_experiment(\"logreg_stackoverflow\")\n",
    "\n",
    "\n",
    "for name, (X_train, X_test) in splits_dict.items():\n",
    "    preproc = preproc_dict.get(name)\n",
    "\n",
    "    with mlflow.start_run(run_name=f\"{model_type}_{name}\"):\n",
    "        mlflow.log_param(\"model_type\", model_type)\n",
    "        mlflow.log_param(\"vecteur\", name)\n",
    "        mlflow.log_param(\"preprocessing\", preproc)\n",
    "\n",
    "        # üß™ Entra√Ænement + pr√©diction\n",
    "        scores = mdl.train_and_score_vector_full_metrics(\n",
    "            name=name,\n",
    "            X_train=X_train,\n",
    "            X_test=X_test,\n",
    "            y_train=y_train,\n",
    "            y_test=y_test,\n",
    "            model_class=model_class,\n",
    "            model_wrapper=model_wrapper,\n",
    "            preprocess=preproc\n",
    "        )\n",
    "\n",
    "        # üìä Logging des m√©triques\n",
    "        mlflow.log_metric(\"f1_micro\", scores[\"f1_micro\"])\n",
    "        mlflow.log_metric(\"hamming_loss\", scores[\"hamming_loss\"])\n",
    "        mlflow.log_metric(\"coverage_score\", scores[\"coverage_tags\"])\n",
    "        mlflow.log_metric(\"f1_macro\", scores[\"f1_macro\"])\n",
    "        mlflow.log_metric(\"precision_micro\", scores[\"precision_micro\"])\n",
    "        mlflow.log_metric(\"recall_micro\", scores[\"recall_micro\"])\n",
    "\n",
    "\n",
    "        # --- DEFINITION EMPLACEMENT SOUHAITE DE SAUVEGARDE LOCALE DU MODELE\n",
    "        path_model = f\"models/logreg/logreg_{name}.joblib\"\n",
    "        # --- CREATION DU DOSSIER COMPLET S'IL N'EXISTE PAS ENCORE\n",
    "        os.makedirs(os.path.dirname(path_model), exist_ok=True)\n",
    "        # --- SAUVEGARDE DU MODELE EN LOCAL\n",
    "        joblib.dump(scores[\"model\"], path_model)\n",
    "        # --- COPIE DANS mlruns/.../artifacts/\n",
    "        mlflow.log_artifact(path_model)\n",
    "\n",
    "\n",
    "        # üì• Stockage dans le tableau de r√©sultats\n",
    "        df_results.append({\n",
    "            \"vecteur\": name,\n",
    "            \"f1_micro\": round(scores[\"f1_micro\"], 3),\n",
    "            \"f1_macro\": round(scores[\"f1_macro\"], 3),\n",
    "            \"precision_micro\": round(scores[\"precision_micro\"], 3),\n",
    "            \"recall_micro\": round(scores[\"recall_micro\"], 3),\n",
    "            \"hamming_loss\": round(scores[\"hamming_loss\"], 4),\n",
    "            \"coverage_tags\": round(scores[\"coverage_tags\"], 4)\n",
    "})\n",
    "\n",
    "\n",
    "df_results = pd.DataFrame(df_results)\n",
    "display(df_results.sort_values(\"f1_micro\", ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f608974",
   "metadata": {},
   "source": [
    "### 2.5. VALIDATION CROISEE SUR CHAQUE VECTEUR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9356056d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES DE MODELISATION\n",
    "# ---------------------------------------------\n",
    "import importlib\n",
    "import src.modeling.modeling as mdl\n",
    "importlib.reload(mdl)\n",
    "# ---------------------------------------------------------\n",
    "# --- 6. VALIDATION CROIS√âE SUR CHAQUE VECTEUR\n",
    "# ---------------------------------------------------------\n",
    "cv_results = []\n",
    "# --- VALIDATION CROISEE POUR CHAQUE VECTEUR (AVEC PREPROCESSING SPECIFIQUE)\n",
    "for name, X in X_dict.items():\n",
    "    preprocess = preproc_dict.get(name)\n",
    "\n",
    "    with mlflow.start_run(run_name=f\"{model_type}_{name}_cv\"):\n",
    "        mlflow.log_param(\"model_type\", model_type)\n",
    "        mlflow.log_param(\"vecteur\", name)\n",
    "        mlflow.log_param(\"preprocessing\", preprocess)\n",
    "        mlflow.log_param(\"validation\", \"cross_val\")\n",
    "\n",
    "        f1_cv = mdl.cross_validate_vector(\n",
    "            name=name,\n",
    "            X=X,\n",
    "            Y=Y,\n",
    "            model_class=model_class,\n",
    "            model_wrapper=model_wrapper,\n",
    "            preprocess=preprocess\n",
    "        )\n",
    "\n",
    "        mlflow.log_metric(\"f1_cv\", f1_cv)\n",
    "\n",
    "        cv_results.append({\n",
    "            \"vecteur\": name,\n",
    "            \"f1_cv\": f1_cv\n",
    "        })\n",
    "\n",
    "# --- AFFICHAGE DES RESULTATS\n",
    "df_cv_results = pd.DataFrame(cv_results).sort_values(\"f1_cv\", ascending=False)\n",
    "display(df_cv_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d773bf3",
   "metadata": {},
   "source": [
    "### 2.6. COMPARAISON RESULTATS SPLIT VERSUS CROSS VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56513bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES DE MODELISATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import src.modeling.modeling as mdl\n",
    "importlib.reload(mdl)\n",
    "# ---------------------------------------------------------\n",
    "# --- 7. M√âTRIQUES COMPARATIVES + DELTA CV/SPLIT\n",
    "# ---------------------------------------------------------\n",
    "model_metrics = []\n",
    "# --- POUR CHAQUE VECTEUR ON RECUPERE LES RESULTATS D'ENTRAINEMENT SPLIT ET CV\n",
    "for name in X_dict.keys():\n",
    "    f1_split = df_results.query(\"vecteur == @name\")[\"f1_micro\"].values[0]\n",
    "    f1_cv = df_cv_results.query(\"vecteur == @name\")[\"f1_cv\"].values[0]\n",
    "    delta = round(f1_split - f1_cv, 3)\n",
    "    hamming = df_results.query(\"vecteur == @name\")[\"hamming_loss\"].values[0]\n",
    "    \n",
    "    model_metrics.append({\n",
    "        \"vecteur\": name,\n",
    "        \"F1_split\": f1_split,\n",
    "        \"F1_cv\": f1_cv,\n",
    "        \"delta_split_cv\": delta,\n",
    "        \"hamming_loss\": hamming\n",
    "    })\n",
    "# --- AFFICHAGE DES METRIQUES POUR CHAQUE VECTEUR/COMPARAISON RESULTATS SPLIT ET CROSS VALIDATION\n",
    "df_model_metrics = pd.DataFrame(model_metrics).sort_values(\"F1_cv\", ascending=False)\n",
    "display(df_model_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad1d12",
   "metadata": {},
   "source": [
    "### 2.7. SAUVEGARDE DU MEILLLEUR MODELE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e76a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES DE MODELISATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import src.modeling.modeling as mdl\n",
    "importlib.reload(mdl)\n",
    "# ---------------------------------------------------------\n",
    "# --- 8. SAUVEGARDE DU MEILLEUR MOD√àLE\n",
    "# ---------------------------------------------------------\n",
    "best_vector_name = mdl.select_best_vecteur(\n",
    "    df_results,\n",
    "     min_f1=0.4,\n",
    "     min_coverage=0.3,\n",
    "     ranking_col=\"f1_micro\"\n",
    "     )\n",
    "print(f\"best_vector_name : {best_vector_name}\")\n",
    "# --- SELECTION DU MEILLEUR MODELE D'APRES LES METRIQUES PUIS SAUVEGARDE\n",
    "# üìÅ Cr√©er le dossier complet s‚Äôil n‚Äôexiste pas\n",
    "model_directory = \"models/logreg\"\n",
    "os.makedirs(model_directory, exist_ok=True)\n",
    "final_model, best_vect_name, path = mdl.save_best_model(\n",
    "    # df_model_metrics,\n",
    "    df_results,\n",
    "    splits_dict,\n",
    "    y_train,\n",
    "    model_class=model_class,\n",
    "    model_wrapper=model_wrapper,\n",
    "    model_type=model_type,\n",
    "    save_dir=model_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399f6e9f",
   "metadata": {},
   "source": [
    "### 2.8. FUSION DES M√âTRIQUES DANS UN TABLEAU COMPARATIF FINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866f6704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES DE MODELISATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import src.modeling.modeling as mdl\n",
    "importlib.reload(mdl)\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# --- FUSION DES M√âTRIQUES DANS UN TABLEAU COMPARATIF FINAL\n",
    "# ---------------------------------------------------------\n",
    "# --- Fusionner les DataFrames sur la colonne 'vecteur'\n",
    "df_merged = df_results.merge(df_cv_results, on=\"vecteur\")\n",
    "\n",
    "# ‚ú® Arrondi pour lisibilit√©\n",
    "df_merged[[\"f1_micro\", \"hamming_loss\", \"coverage_tags\", \"f1_cv\"]] = df_merged[\n",
    "    [\"f1_micro\", \"hamming_loss\", \"coverage_tags\", \"f1_cv\"]\n",
    "].round(3)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# --- LOG DU TABLEAU COMPARATIF DANS MLFLOW\n",
    "# ---------------------------------------------------------\n",
    "os.makedirs(model_directory, exist_ok=True)\n",
    "mlflow.end_run()\n",
    "with mlflow.start_run(run_name=\"comparatif_vecteurs_final\"):\n",
    "    csv_path = f\"{model_directory}/comparatif_vecteurs.csv\"\n",
    "    df_merged.to_csv(csv_path, index=False)\n",
    "    mlflow.log_artifact(csv_path)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# üëÄ AFFICHAGE FINAL DANS LE NOTEBOOK\n",
    "# ---------------------------------------------------------\n",
    "display(df_merged.sort_values(\"f1_micro\", ascending=False))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b567324",
   "metadata": {},
   "source": [
    "### 2.9 VISUALISATION DES SCORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e22b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES DE MODELISATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import src.modeling.modeling as mdl\n",
    "importlib.reload(mdl)\n",
    "# ---------------------------------------------------------\n",
    "# üìä Barplot Seaborn logu√© dans MLflow\n",
    "# ---------------------------------------------------------\n",
    "img_model_directory = f\"{model_directory}/mlflow_images\"\n",
    "os.makedirs(img_model_directory, exist_ok=True)\n",
    "\n",
    "mlflow.end_run()\n",
    "with mlflow.start_run(run_name=\"comparatif_f1_cv_barplot\", nested=True):\n",
    "    mdl.plot_and_log_barplot(\n",
    "        df_scores=df_merged,\n",
    "        metric=\"f1_cv\",\n",
    "        title=\"Score F1 moyen par vecteur\",\n",
    "        save_path=f\"{img_model_directory}/barplot_f1_cv.png\"\n",
    "    )\n",
    "\n",
    "mlflow.end_run()\n",
    "with mlflow.start_run(run_name=\"comparatif_coverage_barplot\", nested=True):\n",
    "    mdl.plot_and_log_barplot(\n",
    "        df_scores=df_merged,\n",
    "        metric=\"coverage_tags\",\n",
    "        title=\"Couverture des tags par vecteur\",\n",
    "        save_path=f\"{img_model_directory}/barplot_coverage_tags.png\"\n",
    "    )\n",
    "mlflow.end_run()\n",
    "with mlflow.start_run(run_name=\"comparatif_f1_micro_barplot\", nested=True):\n",
    "    mdl.plot_and_log_barplot(\n",
    "        df_scores=df_merged,\n",
    "        metric=\"f1_micro\",\n",
    "        title=\"Score F1 (entra√Ænement classique) par vecteur\",\n",
    "        save_path=f\"{img_model_directory}/barplot_f1_micro.png\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8b190c",
   "metadata": {},
   "source": [
    "## ANNEXES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc59a54",
   "metadata": {},
   "source": [
    "## 3. MODELE XGBOOST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e825b6",
   "metadata": {},
   "source": [
    "### 3.1. CHARGEMENT DES FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed9a25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES DE MODELISATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import src.modeling.modeling as mdl\n",
    "importlib.reload(mdl)\n",
    "\n",
    "# -----------------------------------------\n",
    "# --- 0. CONFIGURATION DU MOD√àLE √Ä TESTER\n",
    "# -----------------------------------------\n",
    "# ‚úÖ Tu peux changer ces lignes pour benchmarker un autre mod√®le\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "model_class = XGBClassifier\n",
    "model_wrapper = OneVsRestClassifier\n",
    "model_type = \"xgboost\"\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# --- 1. SELECTION DES FEATURES\n",
    "# ------------------------------\n",
    "full_df = pd.read_parquet(\"data/processed/full_explo_wo.parquet\")\n",
    "print(f\"Dimensions du dataframe full_df : {full_df.shape}\")\n",
    "print(f\" Colonnes du dataframe full_df : {full_df.columns.tolist()}\")\n",
    "print(full_df[[\"clean_title_body\"]].head(3))\n",
    "\n",
    "# --- A. CHARGEMENT DES VECTEURS\n",
    "X_bow   = load_npz(\"models/bow/X_bow_full.npz\")\n",
    "X_tfidf = load_npz(\"models/tfidf/X_tfidf_full.npz\")\n",
    "X_svd   = np.load(\"models/svd/X_titlebody_svd10k.npy\")\n",
    "X_w2v   = np.load(\"models/word2vec/X_w2v_full.npy\")\n",
    "X_use   = np.load(\"models/use_model/embeddings_use_full.npy\")\n",
    "X_sbert = np.load(\"models/sbert/embeddings_sbert_full.npy\")\n",
    "\n",
    "print(\"# --- DIMENSIONS DES VECTEURS :\")\n",
    "for name, mat in [(\"BoW\", X_bow), (\"TF-IDF\", X_tfidf), (\"SVD\", X_svd),\n",
    "                  (\"Word2Vec\", X_w2v), (\"USE\", X_use), (\"SBERT\", X_sbert)]:\n",
    "    print(f\"# --- {name:<10}: {mat.shape}\")\n",
    "\n",
    "\n",
    "# --- B. CHARGEMENT DES LABELS MULTILABEL\n",
    "mlb = joblib.load(\"models/tags/multilabel_binarizer_full.pkl\")\n",
    "Y_full = np.load(\"models/tags/y_tags_full.npy\")\n",
    "\n",
    "print(f\"# --- Labels multilabel charg√©s : {Y_full.shape}\")\n",
    "print(f\"# --- Nombre de tags avant filtrage : {len(mlb.classes_)}\")\n",
    "\n",
    "tag_counts = Y_full.sum(axis=0)\n",
    "tag_mask = tag_counts >= 10  # seuil de raret√©\n",
    "Y_full_filtered = Y_full[:, tag_mask]\n",
    "\n",
    "mlb_filtered = mlb\n",
    "mlb_filtered.classes_ = np.array(mlb.classes_)[tag_mask]\n",
    "print(f\"# --- TAGS conserv√©s apr√®s filtrage : {len(mlb_filtered.classes_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d1a591",
   "metadata": {},
   "source": [
    "### 3.2. DEFINITION DES VARIABLES VECTEURS A ENTRAINER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2bf467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES DE MODELISATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import src.modeling.modeling as mdl\n",
    "importlib.reload(mdl)\n",
    "# ----------------------------------------------\n",
    "# --- 2. S√âPARATION DES FEATURES ET DE LA CIBLE\n",
    "# ----------------------------------------------\n",
    "Y = Y_full_filtered\n",
    "X_text = full_df[\"clean_title_body\"]\n",
    "print(f\"# --- Matrice multilabel (Y) : {Y.shape}\")\n",
    "print(f\"# --- Colonne textuelle (X) : {X_text.shape}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# --- 3. CR√âATION DES VECTEURS + DICTS DE SUPPORT\n",
    "# ---------------------------------------------------------\n",
    "X_dict = {\n",
    "    \"bow\": X_bow,\n",
    "    \"tfidf\": X_tfidf,\n",
    "    \"svd\": X_svd,\n",
    "    \"w2v\": X_w2v,\n",
    "    \"use\": X_use,\n",
    "    \"sbert\": X_sbert\n",
    "}\n",
    "\n",
    "preproc_dict = {\n",
    "    \"bow\": None,\n",
    "    \"tfidf\": None,\n",
    "    \"svd\": None,\n",
    "    \"w2v\": \"scale\",\n",
    "    \"use\": \"scale\",\n",
    "    \"sbert\": \"scale\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084206c1",
   "metadata": {},
   "source": [
    "### 3.3. DIVISION TRAIN / TEST POUR CHAQUE VECTEUR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d20edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES DE MODELISATION\n",
    "# ---------------------------------------------\n",
    "import importlib\n",
    "import src.modeling.modeling as mdl\n",
    "importlib.reload(mdl)\n",
    "# ---------------------------------------------------------\n",
    "# --- 4. DIVISION EN TRAIN / TEST SUR CHAQUE VECTEUR\n",
    "# ---------------------------------------------------------\n",
    "indices = np.arange(Y.shape[0])\n",
    "train_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=42)\n",
    "\n",
    "splits_dict = {}\n",
    "for name, X in X_dict.items():\n",
    "    X_train, X_test, y_train, y_test = mdl.split_on_indices(X, Y, train_idx, test_idx)\n",
    "    splits_dict[name] = (X_train, X_test)\n",
    "\n",
    "print(f\"# --- Splits pr√™ts pour vecteurs : {list(splits_dict.keys())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb7fe12",
   "metadata": {},
   "source": [
    "### 3.4. ENTRAINEMENT DU MODELE POUR CHAQUE VECTEURS - RESULTATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dc87f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES DE MODELISATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import src.modeling.modeling as mdl\n",
    "importlib.reload(mdl)\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "df_results = []\n",
    "uri = \"file:///D:/machine_learning_training/openclassrooms_projects/05_categorisez_automatiquement_question/mlruns\"\n",
    "mlflow.set_tracking_uri(uri)\n",
    "\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()\n",
    "mlflow.set_experiment(\"xgboost_stackoverflow\")\n",
    "\n",
    "\n",
    "\n",
    "for name, (X_train, X_test) in splits_dict.items():\n",
    "    preproc = preproc_dict.get(name)\n",
    "\n",
    "    with mlflow.start_run(run_name=f\"{model_type}_{name}\"):\n",
    "        mlflow.log_param(\"model_type\", model_type)\n",
    "        mlflow.log_param(\"vecteur\", name)\n",
    "        mlflow.log_param(\"preprocessing\", preproc)\n",
    "\n",
    "        # üß™ Entra√Ænement + pr√©diction\n",
    "        scores = mdl.train_and_score_vector_full_metrics(\n",
    "            name=name,\n",
    "            X_train=X_train,\n",
    "            X_test=X_test,\n",
    "            y_train=y_train,\n",
    "            y_test=y_test,\n",
    "            model_class=model_class,\n",
    "            model_wrapper=model_wrapper,\n",
    "            preprocess=preproc\n",
    "        )\n",
    "\n",
    "        # üìä Logging des m√©triques\n",
    "        mlflow.log_metric(\"f1_micro\", scores[\"f1_micro\"])\n",
    "        mlflow.log_metric(\"hamming_loss\", scores[\"hamming_loss\"])\n",
    "        mlflow.log_metric(\"coverage_score\", scores[\"coverage_tags\"])\n",
    "        mlflow.log_metric(\"f1_macro\", scores[\"f1_macro\"])\n",
    "        mlflow.log_metric(\"precision_micro\", scores[\"precision_micro\"])\n",
    "        mlflow.log_metric(\"recall_micro\", scores[\"recall_micro\"])\n",
    "\n",
    "\n",
    "        # üíæ D√©finir le chemin complet du mod√®le\n",
    "        path_model = f\"models/xgboost/xgboost_{name}.joblib\"\n",
    "        # üìÅ Cr√©er le dossier complet s‚Äôil n‚Äôexiste pas\n",
    "        os.makedirs(os.path.dirname(path_model), exist_ok=True)\n",
    "        joblib.dump(scores[\"model\"], path_model)\n",
    "        mlflow.log_artifact(path_model)\n",
    "\n",
    "\n",
    "        # üì• Stockage dans le tableau de r√©sultats\n",
    "        df_results.append({\n",
    "            \"vecteur\": name,\n",
    "            \"f1_micro\": round(scores[\"f1_micro\"], 3),\n",
    "            \"f1_macro\": round(scores[\"f1_macro\"], 3),\n",
    "            \"precision_micro\": round(scores[\"precision_micro\"], 3),\n",
    "            \"recall_micro\": round(scores[\"recall_micro\"], 3),\n",
    "            \"hamming_loss\": round(scores[\"hamming_loss\"], 4),\n",
    "            \"coverage_tags\": round(scores[\"coverage_tags\"], 4)\n",
    "})\n",
    "\n",
    "\n",
    "df_results = pd.DataFrame(df_results)\n",
    "display(df_results.sort_values(\"f1_micro\", ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9cdf41",
   "metadata": {},
   "source": [
    "### 3.5. VALIDATION CROISEE SUR CHAQUE VECTEUR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8822db9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES DE MODELISATION\n",
    "# ---------------------------------------------\n",
    "import importlib\n",
    "import src.modeling.modeling as mdl\n",
    "importlib.reload(mdl)\n",
    "# ---------------------------------------------------------\n",
    "# --- 6. VALIDATION CROIS√âE SUR CHAQUE VECTEUR\n",
    "# ---------------------------------------------------------\n",
    "cv_results = []\n",
    "# --- VALIDATION CROISEE POUR CHAQUE VECTEUR (AVEC PREPROCESSING SPECIFIQUE)\n",
    "for name, X in X_dict.items():\n",
    "    preprocess = preproc_dict.get(name)\n",
    "\n",
    "    with mlflow.start_run(run_name=f\"{model_type}_{name}_cv\"):\n",
    "        mlflow.log_param(\"model_type\", model_type)\n",
    "        mlflow.log_param(\"vecteur\", name)\n",
    "        mlflow.log_param(\"preprocessing\", preprocess)\n",
    "        mlflow.log_param(\"validation\", \"cross_val\")\n",
    "\n",
    "        f1_cv = mdl.cross_validate_vector(\n",
    "            name=name,\n",
    "            X=X,\n",
    "            Y=Y,\n",
    "            model_class=model_class,\n",
    "            model_wrapper=model_wrapper,\n",
    "            preprocess=preprocess\n",
    "        )\n",
    "\n",
    "        mlflow.log_metric(\"f1_cv\", f1_cv)\n",
    "\n",
    "        cv_results.append({\n",
    "            \"vecteur\": name,\n",
    "            \"f1_cv\": f1_cv\n",
    "        })\n",
    "\n",
    "# --- AFFICHAGE DES RESULTATS\n",
    "df_cv_results = pd.DataFrame(cv_results).sort_values(\"f1_cv\", ascending=False)\n",
    "display(df_cv_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f47f67",
   "metadata": {},
   "source": [
    "### 3.6. COMPARAISON RESULTATS SPLIT VERSUS CROSS VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ff7349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES DE MODELISATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import src.modeling.modeling as mdl\n",
    "importlib.reload(mdl)\n",
    "# ---------------------------------------------------------\n",
    "# --- 7. M√âTRIQUES COMPARATIVES + DELTA CV/SPLIT\n",
    "# ---------------------------------------------------------\n",
    "model_metrics = []\n",
    "# --- POUR CHAQUE VECTEUR ON RECUPERE LES RESULTATS D'ENTRAINEMENT SPLIT ET CV\n",
    "for name in X_dict.keys():\n",
    "    f1_split = df_results.query(\"vecteur == @name\")[\"f1_micro\"].values[0]\n",
    "    f1_cv = df_cv_results.query(\"vecteur == @name\")[\"f1_cv\"].values[0]\n",
    "    delta = round(f1_split - f1_cv, 3)\n",
    "    hamming = df_results.query(\"vecteur == @name\")[\"hamming_loss\"].values[0]\n",
    "    \n",
    "    model_metrics.append({\n",
    "        \"vecteur\": name,\n",
    "        \"F1_split\": f1_split,\n",
    "        \"F1_cv\": f1_cv,\n",
    "        \"delta_split_cv\": delta,\n",
    "        \"hamming_loss\": hamming\n",
    "    })\n",
    "# --- AFFICHAGE DES METRIQUES POUR CHAQUE VECTEUR/COMPARAISON RESULTATS SPLIT ET CROSS VALIDATION\n",
    "df_model_metrics = pd.DataFrame(model_metrics).sort_values(\"F1_cv\", ascending=False)\n",
    "display(df_model_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548707ce",
   "metadata": {},
   "source": [
    "### 3.7. SAUVEGARDE DU MEILLLEUR MODELE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c312943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES DE MODELISATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import src.modeling.modeling as mdl\n",
    "importlib.reload(mdl)\n",
    "# ---------------------------------------------------------\n",
    "# --- 8. SAUVEGARDE DU MEILLEUR MOD√àLE\n",
    "# ---------------------------------------------------------\n",
    "# --- SELECTION DU MEILLEUR MODELE D'APRES LES METRIQUES PUIS SAUVEGARDE\n",
    "# üìÅ Cr√©er le dossier complet s‚Äôil n‚Äôexiste pas\n",
    "model_directory = \"models/xgboost\"\n",
    "os.makedirs(model_directory, exist_ok=True)\n",
    "final_model, best_vect_name, path = mdl.save_best_model(\n",
    "    df_model_metrics,\n",
    "    splits_dict,\n",
    "    y_train,\n",
    "    model_class=model_class,\n",
    "    model_wrapper=model_wrapper,\n",
    "    model_type=model_type,\n",
    "    save_dir=model_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74e7d14",
   "metadata": {},
   "source": [
    "### 3.8. FUSION DES M√âTRIQUES DANS UN TABLEAU COMPARATIF FINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7537db0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES DE MODELISATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import src.modeling.modeling as mdl\n",
    "importlib.reload(mdl)\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# --- FUSION DES M√âTRIQUES DANS UN TABLEAU COMPARATIF FINAL\n",
    "# ---------------------------------------------------------\n",
    "# --- Fusionner les DataFrames sur la colonne 'vecteur'\n",
    "df_merged = df_results.merge(df_cv_results, on=\"vecteur\")\n",
    "\n",
    "# ‚ú® Arrondi pour lisibilit√©\n",
    "df_merged[[\"f1_micro\", \"hamming_loss\", \"coverage_tags\", \"f1_cv\"]] = df_merged[\n",
    "    [\"f1_micro\", \"hamming_loss\", \"coverage_tags\", \"f1_cv\"]\n",
    "].round(3)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# üìä LOG DU TABLEAU COMPARATIF DANS MLFLOW\n",
    "# ---------------------------------------------------------\n",
    "os.makedirs(\"models/xgboost/\", exist_ok=True)\n",
    "mlflow.end_run()\n",
    "with mlflow.start_run(run_name=\"comparatif_vecteurs_final\"):\n",
    "    csv_path = \"models/comparatif_vecteurs.csv\"\n",
    "    df_merged.to_csv(csv_path, index=False)\n",
    "    mlflow.log_artifact(csv_path)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# üëÄ AFFICHAGE FINAL DANS LE NOTEBOOK\n",
    "# ---------------------------------------------------------\n",
    "display(df_merged.sort_values(\"f1_micro\", ascending=False))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcae67d",
   "metadata": {},
   "source": [
    "### 3.9 VISUALISATION DES SCORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d703e925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# --- RECHARGEMENT DES MODULES DE MODELISATION\n",
    "# -------------------------------------------\n",
    "import importlib\n",
    "import src.modeling.modeling as mdl\n",
    "importlib.reload(mdl)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# -- BARPLOT SEABORN LOGUE DANS MLFLOW\n",
    "# ---------------------------------------------------------\n",
    "img_model_directory = f\"{model_directory}/mlflow_images\"\n",
    "os.makedirs(img_model_directory, exist_ok=True)\n",
    "\n",
    "mlflow.end_run()\n",
    "with mlflow.start_run(run_name=\"comparatif_f1_cv_barplot\", nested=True):\n",
    "    mdl.plot_and_log_barplot(\n",
    "        df_scores=df_merged,\n",
    "        metric=\"f1_cv\",\n",
    "        title=\"Score F1 moyen par vecteur\",\n",
    "        save_path=f\"{img_model_directory}/barplot_f1_cv.png\"\n",
    "    )\n",
    "\n",
    "mlflow.end_run()\n",
    "with mlflow.start_run(run_name=\"comparatif_coverage_barplot\", nested=True):\n",
    "    mdl.plot_and_log_barplot(\n",
    "        df_scores=df_merged,\n",
    "        metric=\"coverage_tags\",\n",
    "        title=\"Couverture des tags par vecteur\",\n",
    "        save_path=f\"{img_model_directory}/barplot_coverage_tags.png\"\n",
    "    )\n",
    "\n",
    "mlflow.end_run()\n",
    "with mlflow.start_run(run_name=\"comparatif_f1_micro_barplot\", nested=True):\n",
    "    mdl.plot_and_log_barplot(\n",
    "        df_scores=df_merged,\n",
    "        metric=\"f1_micro\",\n",
    "        title=\"Score F1 (entra√Ænement classique) par vecteur\",\n",
    "        save_path=f\"{img_model_directory}/barplot_f1_micro.png\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "StackOverFlowTags (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
